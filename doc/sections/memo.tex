% \section{Memoization for HMMs}

In this section, I propose a different method of likelihood maximization that, unlike the method from section \label{ss:indep_wo_labels}, avoids the need to impute a subset of the hidden states $\calX_\calT$. It is similar to the incremental EM algorithm described in \citet{Gotoh:1998}, but it does not require that sequences of observations be divided into sub-sequences.

For this section, suppose that no labels are known and an observation sequence $\{y_t\}_{t=1}^T$ is generated from a standard HMM. To set up notation, I define the probability density of the observations between times $s$ and $t$ as $p(y_{s:t};\theta,\Gamma)$. One way to write the HMM likelihood in Equation (\ref{eqn:HMM_like}) is in terms of forward probabilities $\alpha^{(i)}_t = p(y_{1:t},X_t = i;\theta,\Gamma)$ (for $i = 1,\ldots,N$ and $t = 1,\ldots,T$) and backward probabilities $\beta^{(i)}_t = p(y_{t+1:T}|X_t = i;\theta,\Gamma)$ (for $i = 1,\ldots,N$ and $t = 1,\ldots,T-1$). By convention, $\beta^{(i)}_T = 1$ for $i = 1,\ldots,N$. If both $\alpha_t$ and $\beta_t$ are known for \textit{any} time $t$, the HMM likelihood can be written as a function of $\alpha_t$ and $\beta_t$ rather than $\theta$ and $\Gamma$:
%
\begin{equation*}
    \mathcal{L}(~\alpha_t,~\beta_t~) = \sum_{i=1}^N \alpha^{(i)}_t \beta^{(i)}_t.
\end{equation*}
%
Both $\alpha_t$ and $\beta_t$ can be computed recursively using the forward-backward algorithm as follows:
%
\begin{align}
    \text{Set} \quad \alpha_1^{(i)}(\theta,\Gamma) &= \delta_i f^{(i)}(y_1;\theta). & \text{Then,} \quad & \alpha_t^{(i)}(\theta,\Gamma,\alpha_{t-1}) = \sum_{j=1}^N \alpha_{t-1}^{(j)} \Gamma^{(j,i)} f^{(i)}(y_t;\theta), \quad t = 2,\ldots,T. \label{eqn:alpha}\\
    %
    \text{Set} \quad \beta_T^{(i)}(\theta,\Gamma) &= 1. & \text{Then,} \quad & \beta_t^{(i)}(\theta,\Gamma,\beta_{t+1}) = \sum_{j=1}^N \Gamma^{(i,j)} f^{(j)}(y_{t+1};\theta) \beta^{(j)}_{t+1}, \quad t = 1,\ldots,T-1.
    \label{eqn:beta}
\end{align}
%
While both $\alpha_t$ and $\beta_t$ can be written as functions of only $\theta$ and $\Gamma$, I purposefully write $\alpha_t$ as a function of $\alpha_{t-1}$ and $\beta_t$ as a function of $\beta_{t+1}$ to emphasize the recursive nature of this calculation.

The gradient of the forward and backward probability vectors can also be computed recursively using the chain rule. All gradients are taken with respect to $\theta$ and $\Gamma$:
%
\begin{align}
    \nabla \alpha_1^{(i)}\big(~\theta,~\Gamma~\big) &= \Big(\nabla \delta^{(i)}\Big)~ f^{(i)}(y_1;\theta) + \delta^{(i)} ~ \Big(\nabla f^{(i)}(y_1;\theta)\Big), \nonumber \\
    %
    \nabla \alpha_{t}^{(i)}\big(~\theta,~\Gamma, ~\alpha_{t-1} ~\big) &= \sum_{j=1}^N \Big(\nabla \alpha_{t-1}^{(j)}\Big) ~ \Gamma^{(j,i)} ~ f^{(i)}(y_{t};\theta) \nonumber \\
    & \hspace{1cm} + \alpha_{t-1}^{(j)} ~ \Big(\nabla \Gamma^{(j,i)} \Big) ~ f^{(i)}(y_{t};\theta) \nonumber \\
    & \hspace{1cm} + \alpha_{t-1}^{(j)} ~ \Gamma^{(j,i)} ~ \Big(\nabla f^{(i)}(y_{t};\theta)\Big) \label{eqn:grad_alpha}, \\\nonumber\\
    %
    \nabla \beta_T^{(i)}\big(~\theta,~\Gamma~\big) &= 0, \nonumber \\
    %
    \nabla \beta_{t}^{(i)}\big(~\theta,~\Gamma, ~\beta_{t+1}~\big) &= \sum_{j=1}^N \Big(\nabla \Gamma^{(i,j)}\Big) ~ f^{(j)}(y_{t+1};\theta) ~ \beta_{t+1}^{(j)} \nonumber \\
    & \hspace{1cm} + \Gamma^{(i,j)} ~ \Big(\nabla f^{(j)}(y_{t+1};\theta)\Big) ~ \beta_{t+1}^{(j)} \nonumber \\
    & \hspace{1cm} + \Gamma^{(i,j)} ~ f^{(j)}(y_{t+1};\theta) ~ \Big(\nabla \beta_{t+1}^{(j)}\Big). \label{eqn:grad_beta}
\end{align}
%
The gradient of the full likelihood can be written as follows:
%
\begin{equation}
    \nabla \calL\Big(~ \alpha_t , ~ \beta_t ~ \Big) = \sum_{i=1}^N \Big(\nabla \alpha_t^{(i)} \Big) \beta_t^{(i)} + \alpha_t^{(i)} \Big( \nabla \beta_t^{(i)} \Big).
    \label{eqn:grad_L}
\end{equation}

Now, suppose that some arbitrary parameter estimates, $\hat \theta$ and $\hat \Gamma$, are given. For direct likelihood maximization, updating $\hat \theta$ and $\hat \Gamma$ once involves recursively calculating $\nabla \alpha_t$ for all $t = 1,\ldots,T$ and using $\nabla \calL = \sum_{i=1}^N \nabla \alpha_T$ to update the parameters. 

For the EM algorithm, one iteration of the ``expectation" step depends upon the probabilities $\gamma_{t}^{(i)}(\hat \theta_t, \hat \Gamma_t) := p(X_t = i \mid Y = y ~ ; ~ \hat \theta, ~ \hat \Gamma)$ and $\xi_{t}^{(i,j)}(\hat \theta, \hat \Gamma) := p(X_t = i, X_{t+1} = j \mid Y = y ~ ; ~ \hat \theta, ~ \hat \Gamma)$ for all $t = 1,\ldots,T$ (where applicable). One can show that
%
\begin{gather}
    \gamma_{t}^{(i)}\big(~\alpha_t, ~\beta_t, ~\hat \theta, ~ \hat \Gamma ~ \big) = \frac{\alpha_{t}^{(i)} ~ \beta_{t}^{(i)}}{\sum_{i'} \alpha_{t}^{(i')} ~ \beta_{t}^{(i')}}, \quad \text{and} \label{eqn:gamma} \\
    %
    \xi_{t}^{(i,j)}\big(~\alpha_t, ~\beta_t, ~\hat \theta, ~ \hat \Gamma ~ \big) = \frac{\alpha_{t}^{(i)} ~ \hat \Gamma^{(i,j)} ~ f^{(j)}(y_{t+1} ~ ; ~ \hat \theta) ~ \beta_{t+1}^{(j)}}{\sum_{i',j'} ~ \alpha_{t}^{(i')} ~ \hat \Gamma^{(i',j')} ~ f^{(j')}(y_{t+1} ~ ; ~\hat \theta) ~ \beta_{t+1}^{(j')}} \label{eqn:xi},
\end{gather}
%
Where $\alpha_t$ and $\beta_t$ are calculated using the parameter estimates $\{\hat \theta, \hat \Gamma\}$, Equation (\ref{eqn:alpha}), and Equation (\ref{eqn:beta}). While $\gamma_t$ and $\xi_{t}$ can be written as function of only $\hat \theta$ and $\hat \Gamma$, I include $\alpha_t$ and $\beta_t$ as function arguments to emphasize their close relationship.

Updating $\hat \theta$ and $\hat \Gamma$ using traditional EM algorithm involves recursively calculating $\alpha_t(\hat \theta, \hat \Gamma)$ and $\beta_t(\hat \theta, \hat \Gamma)$ to compute both $\gamma_{t}$ and $\xi_{t}$ for all $t = 1,\ldots,T$. These values are then used to update $\hat \theta$ and $\hat \Gamma$ using the well-known Baum-Welch update equations \citep{Baum:1970}.

Both the EM algorithm and direct likelihood maximization recalculate the probability vectors $\alpha_t$ and/or $\beta_t$ for \textit{all} $t = 1,\ldots,T$ each time the parameter estimates are updated. Therefore, both the EM algorithm and direct likelihood maximization have time-complexities of $\calO(N^2T)$ \citep{Khreich:2012}. Linear time complexity in $T$ can be prohibitively expensive for very large data sets.

It is difficult to exactly calculate $\alpha_{T}$ and $\beta_1$ after each parameter update without iterating through the entire data set because $\alpha_{t+1}$ depends upon $\alpha_{t}$ and $\beta_{t-1}$ depends upon $\beta_{t}$ for all $t$. However, it is possible to update an \textit{estimate} $\hat \alpha_{t}$, $\widehat{\nabla \alpha}_{t}$, $\hat \beta_{t}$, or $\widehat{\nabla \beta}_{t}$ by storing these estimates for all $t = 1,\ldots,T$. Then, it is simple to update the \textit{estimate} $\hat \alpha_{t+1}$ by using a previously-stored estimate at the previous time index, $\hat \alpha_{t}$. A similar argument holds to update $\hat \beta_{t-1}$ using $\hat \beta_t$.

Inspired by this insight, I propose an algorithm for likelihood maximization that is similar to the standard Baum-Welch algorithm or gradient descent. However, standard inference algorithms alternate between updating the parameter estimates $\{\hat \theta, \hat \Gamma\}$ and updating $\hat \alpha_t$, $\widehat{\nabla \alpha}_t$, $\hat \beta_t$, and $\widehat{\nabla \beta}_t$ for \textit{all} $t \in \{1,\ldots,T\}$. Instead, I alternate between updating the parameter estimates $\{\hat \theta, \hat \Gamma\}$ and updating $\hat \alpha_t$, $\widehat{\nabla \alpha}_t$, $\hat \beta_t$, and $\widehat{\nabla \beta}_t$ for all $t$ within \textit{some sub-sequence of} $\{1,\ldots,T\}$. This subsequence should change each time the parameter estimates $\{\hat \theta, \hat \Gamma\}$ are updated. In particular, I update $\hat \alpha_t$, $\widehat{\nabla \alpha}_t$, $\hat \beta_t$, and $\widehat{\nabla \beta}_t$ within a moving window of width $h$ and stride $h$ that moves after each update of $\{\hat \theta, \hat \Gamma\}$. Pseudo-code for my algorithm is given below.
%
\begin{enumerate}
    \item Initialize parameter estimates $\{\hat \theta,\hat \Gamma\}$ and a spacing $h$.
    \item Initialize $\hat \alpha_{t}$, $\widehat{\nabla \alpha}_{t}$, $\hat \beta_{t}$, and $\widehat{\nabla \beta}_{t}$, $\hat \gamma_t$, and $\hat \xi_t$ for $t=1,\ldots,T$ by iterating once through Equations (\ref{eqn:alpha}), (\ref{eqn:beta}), (\ref{eqn:grad_alpha}), and (\ref{eqn:grad_beta}), followed by Equations (\ref{eqn:gamma}) and (\ref{eqn:xi}). Note that $\hat \gamma_t$ and $\hat \xi_t$ are only used for the EM algorithm.
    %
    \item For $t = 1,h+1,2h+1,\ldots,T-h$:
    \begin{enumerate}
        \item For $s = 1,2,\ldots,h$:
        \begin{enumerate}
            \item Calculate $\hat \alpha_{t+s}$ using $\hat \alpha_{t+s-1}$, $\hat \theta$, $\hat \Gamma$, and Equation (\ref{eqn:alpha}).
            \item Calculate $\hat \beta_{t+h-s}$ using $\hat \beta_{t+h-s+1}$, $\hat \theta$, $\hat \Gamma$, and Equation (\ref{eqn:beta}).
            \item Calculate $\widehat{\nabla \alpha}_{t+s}$ using $\hat \alpha_{t+s-1}$, $\widehat{\nabla \alpha}_{t+s-1}$, $\hat \theta$, $\hat \Gamma$, and Equation (\ref{eqn:grad_alpha}).
            \item Calculate $\widehat{\nabla \beta}_{t+h-s}$ using $\hat \beta_{t+h-s+1}$, $\widehat{\nabla \beta}_{t+h-s+1}$, $\hat \theta$, $\hat \Gamma$, and Equation (\ref{eqn:grad_beta}).
        \end{enumerate}
        \item For $s = 0,1,\ldots,h-1$:
        \begin{enumerate}
            \item Update $\hat \gamma_{t+s}$ by using $\hat \alpha_{t+s}$, $\hat \beta_{t+s}$, and Equation (\ref{eqn:gamma}). 
            \item Update $\hat \xi_{t+s}$ using $\hat \alpha_{t+s}$, $\hat \beta_{t+s}$, $\hat \theta$, $\hat \Gamma$, and Equation (\ref{eqn:xi})
        \end{enumerate}
        \item If using the EM algorithm, update $\hat \Gamma$ and $\hat \theta$ according to the standard Baum-Welch updates. If using direct likelihood maximization, update $\hat \Gamma$ and $\hat \theta$ using the gradient estimate $$\widehat{\nabla \calL}_{t} = \sum_{i=1}^N \widehat{\nabla \alpha}_{t}^{(i)} \hat \beta_{t}^{(i)} + \hat \alpha_{t}^{(i)} \widehat{\nabla \beta}_{t}^{(i)}.$$
    \end{enumerate}
    \item Return to step 3 until convergence.
\end{enumerate}

Although the initialization step of this algorithm has time-complexity $\calO(N^2T)$, updating the parameters of the HMM takes no longer than $\calO(N^2h)$ time. The time complexity is independent of $T$ since the algorithm simply moves forward (or backwards) $h$ time-steps each time it updates the parameters. One drawback of the memoization algorithm is that it has a space complexity of $\calO(NT)$ for the Baum-Welch algorithm and $\calO(N^2T)$ for direct likelihood maximization. This is because a total of $T$ forward-backward vectors (each of length $N$) must be stored for the memoized Baum-Welch algorithm and a total of $NT$ gradient estimates (each with approximate length $N^2$) must be stored for memo-ized direct likelihood maximization. However, it is possible reduce the space complexity of both algorithms by a factor of $h$ by deleting unnecessary values -- see \citet{Florez:2005} for details. The spacing $h$ must therefore be selected in a way that balances space and time complexity. One option is to pick $h = \calO(\sqrt{T})$, which results in both a time complexity and space complexity of $\calO(\sqrt{T})$ in $T$. This is in line with the check-pointing algorithm of \citet{Grice:1997}.