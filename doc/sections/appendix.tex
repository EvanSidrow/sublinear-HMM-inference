\section{Version 2 of \texttt{EM-VRSO}}

For the purposes of readability, we abuse notation and use the shorthand $F(\bfphi \mid \bfgamma(\bfphi'), \bfxi(\bfphi')) \equiv F(\bfphi \mid \bfphi')$ and $F_{t}(\bfphi \mid \bfgamma_{t}(\bfphi'), \bfxi_{t}(\bfphi')) \equiv F_{t}(\bfphi \mid \bfphi')$ in this appendix. Further, we denote $F^*(\bfphi') \equiv \inf_{\bfphi \in \bfPhi} F(\bfphi \mid \bfphi')$.

\begin{algorithm}
\caption{\texttt{EM-VRSO}$(\bfphi_0, \lambda, A, M, K)$ (Version 2)}\label{alg:EM-VRSO-v2}
\begin{algorithmic}[1]
\Require Initial parameters ($\bfphi_{0}$), step size ($\lambda$), algorithm $A \in \{\text{SVRG, SAGA}\}$, whether to do a partial E step $P \in \{\texttt{True,False}\}$, number of iterations per update ($M$), and number of updates ($K$). Assume known Lipschitz constant $L$ and known strong convexity constant $C$.
%
\vspace{5pt}
%
\State $\zeta = (C \lambda(1-2L\lambda)M)^{-1} + (2L\lambda) / (1-2L\lambda) < 1$ 
%
\Comment{$\zeta < 1$}
%
\For{$k = 0,\ldots,K-1$}
% 
\State $\{\bfalpha_{k,t},\bfbeta_{k,t},\bfgamma_{k,t},\bfxi_{k,t}\}_{t=1}^{T} = \texttt{E-step}(\bfphi_{k})$ \Comment{E step}
%
\State $K(\bfphi_{k}) = F^{*}(\bfphi_{k}) + \left[(1 + \zeta) / 2\right]\left(F(\bfphi_{k} \mid \bfphi_{k}) - F^{*}(\bfphi_{k})\right)$
%
\State $\ell \gets 0$ \Comment{M step}
%
\While{$\ell = 0$ or $F(\bfphi_{k,\ell} \mid \bfphi_{k}) > K(\bfphi_{k})$}
\State $\ell \gets \ell+1$
\State $\bfphi_{k,\ell} = \texttt{VRSO-PE}(\{\bfalpha_t,\bfbeta_t,\bfgamma_t,\bfxi_t\}_{t=1}^{T},\bfphi_{k},\lambda,A,P,M)$
%
\EndWhile
\State $\bfphi_{k+1} = \bfphi_{k,\ell}$
\EndFor
\State \Return $\bfphi_K$
\end{algorithmic}
\end{algorithm}

One concern with Algorithm (\ref{alg:EM-VRSO-v2}) is that it requires quantities that are not known in practice, namely $\zeta$ and $F^{*}(\bfphi_{k})$. However, these quantities are only used in the while loop to ensure a strict increase in the likelihood for every iteration of \texttt{VRSO-PE}. In addition, version 1 of the algorithm is intuitively preferable since it will accept new parameters that increase the likelihood by any amount. We only use version 2 above to prove Theorem 1.

Another concern with Algorithm (\ref{alg:EM-VRSO-v2}) is that $F^{*}(\bfphi_{k})$ is defined using an infimum, which can be problematic in some settings. However, we prove in Lemma 1 that if the conditions of Theorem 1 hold, then $F^*(\bfphi')$ is finite, has a unique minimizer, and is uniformly continuous.

\begin{lemma}
    If the conditions of Theorem 1 hold, then for all $\bfphi_k \in \bfPhi$, $F^{*}(\bfphi_k) = \inf_{\bfphi \in \bfPhi} F(\bfphi \mid \bfphi_k)$ is finite and has a unique minimizer, so $F^*(\bfphi_k) = \min_{\bfphi \in \bfPhi} F(\bfphi \mid \bfphi_k) = F(\bfphi_{k+1}^* \mid \bfphi_k)$ for a unique $\bfphi_{k+1}^* \in \bfPhi$. Further, $F^*(\bfphi_k)$ is uniformly continuous for all $\bfphi_k \in \bfPhi$.
\end{lemma}

\begin{proof}
    For any fixed $\bfphi_k$, if the conditions of Theorem 1 hold, then there exists a unique minimizer of $F(\cdot \mid \bfphi_k)$ over $\bfPhi_{\bfphi_{k}}$ because $\bfPhi_{\bfphi_{k}}$ is compact and $F$ is strongly convex and continuous. Denote this minimizer as $\bfphi_{k+1}^* = \argmin_{\bfphi \in \bfPhi_{\bfphi_{k}}} F(\bfphi \mid \bfphi_{k})$. We show that $\bfphi_{k+1}^*$ minimizes $F(\cdot \mid \bfphi_{k})$ over the \textit{entirety} of $\bfPhi$ using two cases:
    
    \begin{enumerate}
        \item For any $\bfphi \in \bfPhi_{\bfphi_{k}}$ with $\bfphi \neq \bfphi_{k+1}^*$, we have that $F(\bfphi \mid \bfphi_{k}) > F(\bfphi_{k+1}^* \mid \bfphi_{k})$ because $\bfphi_{k+1}^*$ is the unique minimizer over $\bfPhi_{\bfphi_{k}}$. 
        %
        \item For any $\bfphi \notin \bfPhi_{\bfphi_{k}}$, $\log p(\bfy;\bfphi) < \log p(\bfy;\bfphi_k) \leq \log p(\bfy;\bfphi^*_{k+1})$ by the definition of $\bfPhi_{\bfphi_{k}}$. This implies that $F(\bfphi \mid \bfphi_{k}) > F(\bfphi_{k} \mid \bfphi_{k}) \geq F(\bfphi^*_{k+1} \mid \bfphi_{k})$ by basic properties of the EM algorithm \citep{Dempster:1977}.
    \end{enumerate} 

    Therefore, $\bfphi_{k+1}^*$ is a global minimizer of $F(\cdot \mid \bfphi_k)$, and $F^{*}(\bfphi_{k}) = F(\bfphi_{k+1}^* \mid \bfphi_k)$.

    We now prove that $F^*(\bfphi_k)$ is uniformly continuous. Let $\epsilon > 0$ be fixed. Then, because $F(y \mid x)$ is uniformly continuous in $(x,y) \in \bfPhi \times \bfPhi$, there exists some $\delta$ such that for all $(y,x)$ and $(y,x_0)$ with $|x-x_0| < \delta$ we have that $|F(y \mid x) - F(y \mid x_0)| < \epsilon$. Now, we fix arbitrary $x$ and $x_0$ with $|x-x_0| < \delta$ and show that $|F^*(x) - F^*(x_0)| < \epsilon$. To this end, pick $y^*$ such that $F(y^* \mid x) = \min_{y \in \bfPhi} F(y \mid x)= F^*(x)$ and pick $y_0^*$ such that $F(y_0^* \mid x_0) = \min_{y \in \bfPhi} F(y \mid x_0) = F^*(x_0)$. Then: 
    
    \begin{align*}
        F^*(x_0) &= F(y_0^* \mid x_0) \leq F(y^* \mid x_0) < F(y^* \mid x) + \epsilon = F^*(x) + \epsilon, \\ 
        %
        F^*(x_0) &= F(y_0^* \mid x_0) > F(y_0^* \mid x) - \epsilon \geq F(y^* \mid x) - \epsilon = F^*(x) - \epsilon,
    \end{align*}
    %
    %The inequalities above rely on both the continuity of $F$ and the fact that $y^*$ and $y_0^*$ are minimizers of $F$. Therefore, we have found a $\delta$ such that for any arbitrary $x$ and $x_0$ with $|x-x_0| < \delta$, $F^*(x) - \epsilon \leq F^*(x_0) \leq F^*(x) + \epsilon$. But this is precisely the definition of uniform continuity.
    so $|F^*(x) - F^*(x_0)| < \epsilon$.
\end{proof}

\section{Proof of Theorem 1}

We begin by introducing notation needed for the proof. Let $I \in \{1,\ldots,T\}^M$ be an $M$-dimensional vector of indices. Let $\texttt{VRSO-PE}^*(I,\lambda,\bfphi')$ correspond to the mapping that results from performing $M$ steps of SVRG on objective function $F(\cdot \mid \bfphi')$ starting at $\bfphi'$ with step size $\lambda$ and random realization $I$. That is, $\texttt{VRSO-PE}^*(I,\lambda,\bfphi')$ equals $\texttt{VRSO-PE}(\{\bfalpha_t(\bfphi'),\bfbeta_t(\bfphi'),\bfgamma_t(\bfphi'),\bfxi_t(\bfphi')\}_{t=1}^{T}, \bfphi', \lambda, \text{SVRG}, \texttt{False}, M)$ when the random vector of indices $\begin{pmatrix} t_0 & \cdots & t_{M-1} \end{pmatrix}$ is equal to $I$. We prove that $\texttt{VRSO-PE}^*(I,\lambda,\bfphi')$ is uniformly continuous in $\bfphi' \in \bfPhi$ for fixed $I$ and $\lambda$ in Lemma 2 below.

\begin{lemma}
    Suppose all conditions from Theorem 1 hold. Then $\texttt{VRSO-PE}^*(I,\lambda,\bfphi')$ is uniformly continuous in $\bfphi' \in \bfPhi$ for fixed $I$ and $\lambda$.
\end{lemma}

\begin{proof}
    Define $\bfphi'^{(0)} = \bfphi'$. The function $\texttt{VRSO-PE}^*(I,\lambda,\bfphi')$ involves calculating $\bfphi'^{(0)}, \bfphi'^{(1)}, \ldots, \bfphi'^{(m)}, \ldots, \bfphi'^{(M-1)}$ by iteratively applying the following mapping (with $t_m = I[m+1]$) and returning $\bfphi'^{(M)} \equiv \bfphi$:
    %
    \begin{equation*}
        \bfphi'^{(m+1)}(\bfphi'^{(m)},\bfphi'^{(0)}) = \lambda \left[\nabla F_{t_m}(\bfphi'^{(m)} \mid \bfphi'^{(0)}) - \nabla F_{t_m}(\bfphi'^{(0)} \mid \bfphi'^{(0)}) + \frac{1}{T}\sum_{t=1}^T\nabla F_{t}(\bfphi'^{(0)} \mid \bfphi'^{(0)}) \right].
    \end{equation*}
    %
    Note that $\nabla F_t(\bfphi \mid \bfphi')$ is uniformly continuous in $(\bfphi,\bfphi')$ for each $t$ by condition (7) of Theorem 1. Therefore, the mapping above is also uniformly continuous in $(\bfphi'^{(m)},\bfphi'^{(0)})$, and so $\texttt{VRSO-PE}^*(I,\lambda,\bfphi')$ is uniformly continuous in $\bfphi'$ for fixed $I$ and $\lambda$.
\end{proof}

If the conditions of Theorem 1 are met, iteration $k$ of Algorithm (\ref{alg:EM-VRSO-v2}) is equivalent to drawing a sequence of index vectors $I_{k,0},I_{k,1},\ldots$ and calculating $\bfphi_{k,\ell} = \texttt{VRSO-PE}^*(I_{k,\ell},\lambda,\bfphi_{k})$ until the condition to exit the while loop is satisfied, which happens in finite time by Lemma 3 below. Lemma 3 depends upon Theorem 1 of \citet{Johnson:2013}, so Table (\ref{tbl:notation}) identifies how our notation corresponds to theirs.
%
\begin{table}[]
\centering
\begin{tabular}{c|c}
\citet{Johnson:2013}                  & Our Notation                          \\ \hline
$\alpha$                              & $\zeta$                               \\
$\eta$                                & $\lambda$                             \\
$L$                                   & $L$                                   \\
$\gamma$                              & $C$                                   \\
$m$                                   & $M$                                   \\
$\tilde{w}_0$                         & $\bfphi_{k}$                          \\
$\tilde{w}_1$                         & $\bfphi_{k,\ell}$                     \\
$w_{*}$                               & $\bfphi^*_{k+1}$                      \\
$P$                                   & $F(\cdot \mid \bfphi_k)$              \\
$\psi_i$                              & $F_t(\cdot \mid \bfphi_k)$                         
\end{tabular}
\caption{Legend connecting this paper's notation to that of \citet{Johnson:2013}.}
\label{tbl:notation}
\end{table}

\begin{lemma}
    Suppose all conditions from Theorem 1 hold. Then, the while loop within Algorithm (\ref{alg:EM-VRSO-v2}) almost surely terminates in finite time, i.e. $\bbP(\ell^*(k) < \infty) = 1$ for all $k \geq 0$. Furthermore, $\bbP\{\ell^*(k) < \infty \forall k\} = 1$.
\end{lemma}

\begin{proof}

By Lemma 1, $F^*(\bfphi_k) = \inf_{\bfphi \in \bfPhi} F(\bfphi \mid \bfphi_k)$ is finite and uniformly continuous in $\bfphi_k$. Further, Theorem 1 of \citet{Johnson:2013} applies for one iteration through $M$ steps of SVRG. In particular, for all $\ell \geq 0$:

\begin{align}
    \bbE & \left[F(\bfphi_{k,\ell} \mid \bfphi_{k}) - F^*(\bfphi_k) ~\Big\vert~ \bfphi_{k} \right] \leq \zeta \Big(F(\bfphi_{k} \mid \bfphi_{k}) - F^*(\bfphi_k) \Big), \label{eqn:SVRG_T1}
\end{align}
%
where $\zeta$ is defined in condition (6) of Theorem 1. Using (\ref{eqn:SVRG_T1}) in Markov's inequality gives

\begin{align}
    \bbP \Big[F(\bfphi_{k,\ell} \mid \bfphi_{k}) - F^*(\bfphi_{k}) \geq \frac{1 + \zeta}{2} \left(F(\bfphi_{k} \mid \bfphi_{k}) - F^*(\bfphi_k) \right) ~\Big\vert~ \bfphi_{k} \Big] &\leq \frac{\zeta \left( F(\bfphi_{k} \mid \bfphi_{k}) - F^*(\bfphi_k) \right)}{\frac{1+\zeta}{2} \Big( F(\bfphi_{k} \mid \bfphi_{k}) - F^*(\bfphi_k) \Big)} \\
    &= \frac{2}{1 + 1/\zeta} < 1.
\end{align}

Taking the complement of the above expression and rearranging terms gives

\begin{equation}
    \bbP \Big[F(\bfphi_{k,\ell} \mid \bfphi_{k}) \leq F^*(\bfphi_{k}) + \frac{1 + \zeta}{2} \Big(F(\bfphi_{k} \mid \bfphi_{k}) - F^*(\bfphi_{k}) \Big) ~\Big\vert~ \bfphi_{k} \Big] \geq \frac{1-\zeta}{1 + \zeta} > 0 \label{eqn:markov_ineq},
\end{equation}

so with some probability greater than or equal to $(1-\zeta)/(1+\zeta) > 0$, the condition to exit the while loop at iteration $\ell$ of Algorithm (\ref{alg:EM-VRSO-v2}) is satisfied.

Further, repeated iteration through Algorithm (\ref{alg:EM-VRSO-v2}) corresponds to drawing \textit{independent} samples of $\bfphi_{k,\ell}$ conditioned on $\bfphi_k$. As a result, $\ell^*(k)$ follows a geometric distribution with some positive success probability $p_k \geq (1-\zeta)/(1+\zeta)$. Therefore, $\bbP\Big\{\ell^*(k) < \infty \Big\} = \sum_{\ell \geq 1} p_k(1-p_k)^{\ell-1} = 1$, and 

\begin{equation*}
    \bbP\Big\{\forall k : \ell^*(k) < \infty \Big\} = 1 - \bbP\Big\{\exists k : \ell^*(k) = \infty \Big\} = 1.
\end{equation*}
\end{proof}

Finally, we introduce $R_{M,\lambda}$ as a point-to-set map corresponding to one iteration of Algorithm (\ref{alg:EM-VRSO-v2}) with $M$ steps of step size $\lambda$. First, recall that

\begin{equation}
    K(\bfphi') = F^*(\bfphi') + [(1 + \zeta)/2] \Big(F(\bfphi' \mid \bfphi') - F^*(\bfphi') \Big). 
\end{equation}

Then, we define $R_{M,\lambda}(\bfphi')$ as

\begin{equation}
    R_{M,\lambda}(\bfphi') = \Big\{\bfphi ~ : ~ \bfphi = \texttt{VRSO-PE}^*(I,\lambda,\bfphi') ~ \text{for some} ~ I \in \{1,\ldots,T\}^M ~\text{and}~ F(\bfphi \mid \bfphi') \leq K(\bfphi')\Big\}.
\end{equation}

Iteration $k$ of Algorithm (\ref{alg:EM-VRSO-v2}) corresponds to randomly sampling $\bfphi_{k+1}$ from $R_{M,\lambda}(\bfphi_k)$. If the conditions of Theorem 1 are met, then $\bbP(\ell^*(k) < \infty) = 1$, so $R_{M,\lambda}(\bfphi_k)$ must not be the empty set. We prove two more useful Lemmas regarding $R_{M,\lambda}$ before finally proving Theorem 1.

\begin{lemma}
    Suppose all conditions from Theorem 1 hold. Then $R_{M,\lambda}$ is a closed point-to-set map for all non-stationary points $\bfphi' \in \bfPhi$. Note that $R_{M,\lambda}$ is closed at a point $\bfphi'$ if $\bfphi'_{n} \to \bfphi'$ and $\bfphi_{n} \to \bfphi$ with $\bfphi_{n} \in R_{M,\lambda}(\bfphi'_{n})$, implies that $\bfphi \in R_{M,\lambda}(\bfphi')$. 
\end{lemma}

\begin{proof}
     Given a point $\bfphi'$, suppose there exists some sequence $\bfphi'_{n} \to \bfphi'$ as well as another sequence $\bfphi_{n} \to \bfphi$ with $\bfphi_{n} \in R_{M,\lambda}(\bfphi'_{n})$. To prove the Lemma, we prove that $\bfphi \in R_{M,\lambda}(\bfphi')$, i.e. that
    \begin{enumerate}
        \item $F(\bfphi \mid \bfphi') \leq K(\bfphi')$, and
        \item $\bfphi = \texttt{VRSO-PE}^*(I,\lambda,\bfphi')$ for some $I \in \{1,\ldots,T\}^M$
    \end{enumerate}
    %
    We start with the first condition. Since $\bfphi_{n} \to \bfphi$ and $\bfphi'_{n} \to \bfphi'$,  \ we have that $F(\bfphi_{n} \mid \bfphi'_{n}) \to F(\bfphi \mid \bfphi')$ because $F$ is continuous by condition (7) of Theorem 1. Likewise, $K(\bfphi'_{n}) \to K(\bfphi')$ because $K$ is continuous by Lemma 1. Finally, $F(\bfphi \mid \bfphi') \leq K(\bfphi')$ because for all $n$, $\bfphi_n \in R_{M,\lambda}(\bfphi_n')$, which implies that $F(\bfphi_{n} \mid \bfphi'_{n}) \leq K(\bfphi'_{n})$. Taking the limit of both sides as $n \to \infty$ gives the result.
    
    We now show the second condition. By way of contradiction, assume that $\bfphi \neq \texttt{VRSO-PE}^*(I,\lambda,\bfphi')$ for any value of $I \in \{1,\ldots,T\}^M$. Then, $\min_I ||\bfphi - \texttt{VRSO-PE}^*(I,\lambda,\bfphi')||$ is strictly positive. 
    %Because the sequence $\{\bfphi_n\}_{n>1}$ converges to $\bfphi$, there must exist some $N_1$ such that for all $n \geq N_1$, $||\bfphi_{n} - \bfphi|| < \epsilon/2$. Further, since $\{\bfphi'_{n}\}_{n>1}$ converges to $\bfphi'$ and $\texttt{VRSO-PE}^*(I,\lambda,\bfphi')$ is continuous in $\bfphi'$, there must exist some $N_2$ such that for all $n \geq N_2$ and $I \in \{1,\ldots,T\}^M$, $||\texttt{VRSO-PE}^*(I,\lambda,\bfphi'_{n}) - \texttt{VRSO-PE}^*(I,\lambda,\bfphi')|| < \epsilon/2$. Now pick an arbitrary $n > \max\{N_1,N_2\}$. 
    By assumption, for each $n$, $\bfphi_{n} \in R_{M,\lambda}(\bfphi'_{n})$, so $\bfphi_{n} = \texttt{VRSO-PE}^*(I_n,\lambda,\bfphi'_{n})$ for some $I_n \in \{1,\ldots,T\}^M$. %Therefore, $||\bfphi_{n} - \texttt{VRSO-PE}^*(J,\lambda,\bfphi')|| = ||\texttt{VRSO-PE}^*(J,\lambda,\bfphi'_{n}) - \texttt{VRSO-PE}^*(J,\lambda,\bfphi')|| < \epsilon/2$ because $n > N_2$. 
    Using the triangle inequality, we have:

    \begin{align}
        0 < \min_{I}||\bfphi - \texttt{VRSO-PE}^*(I,\lambda,\bfphi')|| &\leq ||\bfphi_{n} - \bfphi|| + ||\bfphi_{n} - \texttt{VRSO-PE}^*(I_n,\lambda,\bfphi')|| \\
        &= ||\bfphi_{n} - \bfphi|| + ||\texttt{VRSO-PE}^*(I_n,\lambda,\bfphi'_{n}) - \texttt{VRSO-PE}^*(I_n,\lambda,\bfphi')|| \\
        &\leq ||\bfphi_{n} - \bfphi|| + \max_{I} ||\texttt{VRSO-PE}^*(I,\lambda,\bfphi'_{n}) - \texttt{VRSO-PE}^*(I,\lambda,\bfphi')||.
    \end{align}
    
    However, the right-hand side of the equation converges to zero since $\bfphi_n \to \bfphi$, $\bfphi'_n \to \bfphi'$, and $\texttt{VRSO-PE}^*(I,\lambda,\bfphi')$ is continuous for fixed $I$ and $\lambda$. This is a contradiction, so it must be that $\bfphi = \texttt{VRSO-PE}^*(I,\lambda,\bfphi')$ for some $I$.
\end{proof}

\begin{lemma}
    Suppose all conditions from Theorem 1 hold, and consider the sequence $\{\bfphi_k\}_{k=0}^{K}$ generated from running Algorithm (\ref{alg:EM-VRSO-v2}) with $\bfphi_0 \in \bfPhi$. For $k = 0,\ldots,K-1$, we have that $\log p(\bfy;\bfphi_{k+1}) \geq \log p(\bfy;\bfphi_k)$. Further, if $\bfphi_k$ is not a stationary point of $\log p$, then $\log p(\bfy;\bfphi_{k+1}) > \log p(\bfy;\bfphi_k)$.
\end{lemma}

\begin{proof}

First, note that $\bfphi_k$ is a stationary point of $\log p$ if and only if it is a stationary point of $F(\cdot \mid \bfphi_k)$ because $\nabla \log p(\bfphi_k) = \nabla Q(\bfphi_k \mid \bfphi_k) = -\nabla F(\bfphi_k \mid \bfphi_k) / T$.

We begin with the case where $\bfphi_k$ is a stationary point of $\log p$ (and therefore a stationary point of $F(\cdot \mid \bfphi_k)$). Then, $F(\bfphi_k \mid \bfphi_k) = F^{*}(\bfphi_{k})$ since $F$ is continuously differentiable and strongly convex. To exit the while loop at iteration $k$ of Algorithm (\ref{alg:EM-VRSO-v2}), it must be the case that $F(\bfphi_{k+1} \mid \bfphi_{k}) \leq F^*(\bfphi_{k}) + [(1+\zeta)/2] \Big(F(\bfphi_{k} \mid \bfphi_{k}) - F^*(\bfphi_{k}) \Big) = F(\bfphi_k \mid \bfphi_k)$. Finally, we know from \citet{Dempster:1977} that $\log p(\bfy ; \bfphi_{k+1}) - \log p(\bfy ; \bfphi_{k}) \geq F(\bfphi_{k} \mid \bfphi_{k}) - F(\bfphi_{k+1} \mid \bfphi_{k}) \geq 0$. This implies that $\log p(\bfy ; \bfphi_{k+1}) \geq \log p(\bfy ; \bfphi_{k})$.

Next, we focus on the case where $\bfphi_k$ is \textit{not} a stationary point of $\log p$ (and therefore not a stationary point $F(\cdot \mid \bfphi_k)$). Then, $F(\bfphi_k \mid \bfphi_k) > F^{*}(\bfphi_{k})$ since $F$ is continuously differentiable and strongly convex. To exit the while loop at iteration $k$ of Algorithm (\ref{alg:EM-VRSO-v2}), it must be the case that $F(\bfphi_{k+1} \mid \bfphi_{k}) \leq F^*(\bfphi_{k}) + [(1+\zeta)/2] \Big(F(\bfphi_{k} \mid \bfphi_{k}) - F^*(\bfphi_{k}) \Big) < F(\bfphi_k \mid \bfphi_k)$. Finally, we know from \citet{Dempster:1977} that $\log p(\bfy ; \bfphi_{k+1}) - \log p(\bfy ; \bfphi_{k}) \geq F(\bfphi_{k} \mid \bfphi_{k}) - F(\bfphi_{k+1} \mid \bfphi_{k}) > 0$. This implies that $\log p(\bfy ; \bfphi_{k+1}) > \log p(\bfy ; \bfphi_{k})$.
\end{proof}

We now prove Theorem 1.

\begin{proof}

The first part of Theorem 1 states that $\bbP(\ell^*(k) < \infty) = 1$ for all $k \geq 0$ and is true by Lemma 3. The second part of Theorem 1 states that all limit points of $\{\bfphi_{k}\}_{k=0}^\infty$ are stationary points of $\log p(\bfy;\bfphi)$, and $\log p(\bfy;\bfphi_{k})$ converges monotonically to $\log p^* = \log p(\bfy;\bfphi^*)$ for some stationary point of $\log p$, $\bfphi^*$. This is a direct application of Theorem 1 of \citet{Wu:1983}, which requires the following conditions:

\begin{enumerate}[label=(\alph*)]
    \item $R_{M,\lambda}$ is a closed point-to-set map for all non-stationary points.
    %
    \item $\log p(\bfy;\bfphi_{k+1}) \geq \log p(\bfy;\bfphi_k)$ for all $\bfphi_k  \in \bfPhi$, and $\log p(\bfy;\bfphi_{k+1}) > \log p(\bfy;\bfphi_k)$ for all $\bfphi_k \in \bfPhi$ that are not stationary points of $\log p$.
\end{enumerate}

Condition (a) is satisfied by Lemma 4, and condition (b) is satisfied by Lemma 5. 
%
\end{proof}

\section{Proof of Theorem 2}

We first give a basic outline of the proof. If $\nabla \log p(\bfy;\bfphi_0) = 0$, then $\nabla F(\bfphi_0 \mid \bfphi_0) = 0$, so every gradient step of SAGA and SVRG within $\texttt{VRSO-PE}$ does not change $\bfphi_0^{(m)}$, so $ = \bfphi_0^{(m+1)} =  \bfphi_0^{(m)} = \bfphi^{(0)}_0$. As a result, the gradient approximations do not change either, so $\widehat F^{(m+1)} = \widehat F^{(m)}$ and $\widehat F^{(m+1)}_t = \widehat F^{(m)}_t$ for $t = 1,\ldots,T$. Finally, since the parameter estimates do not change, the conditional probability approximations $\left\{\widehat \bfalpha^{(m)}_t, \widehat \bfbeta^{(m)}_t, \widehat \bfgamma^{(m)}_t, \widehat \bfxi^{(m)}_t\right\}_{t=1}^T = \left\{\bfalpha_t(\bfphi_0), \bfbeta_t(\bfphi_0), \bfgamma_t(\bfphi_0), \bfxi_t(\bfphi_0) \right\}_{t=1}^T$ do not change either. Because $\texttt{VRSO-PE}$ returns $\bfphi_1 = \bfphi_0^{(M)} = \bfphi_0$, each iteration of $\texttt{EM-VRSO}$ must do the same, so by induction $\bfphi_K = \bfphi_0$.

To prove the Theorem more formally, we first prove a useful Lemma.

\begin{lemma}
    If:
    \begin{enumerate}
        \item $\widehat \bfalpha^{(0)}_t = \bfalpha_t(\bfphi^{(0)})$ and $\widehat \bfbeta^{(0)}_{t} = \bfbeta_t(\bfphi^{(0)})$ for all $t = 1,\ldots,T$,
        %
        \item $\widehat \bfgamma^{(0)}_{t} = \bfgamma_t(\bfphi^{(0)})$ and $\widehat \bfxi^{(0)}_{t} = \bfxi_t(\bfphi^{(0)})$ for all $t = 1,\ldots,T$,
        %
        \item $\nabla F(\bfphi^{(0)} \mid \bfgamma(\bfphi^{(0)}),\bfxi(\bfphi^{(0)})) = 0$,
    \end{enumerate}
    then with probability 1, for algorithm (\ref{alg:VRSO-PE}):
    \begin{equation}
        \texttt{VRSO-PE}\Big(\left\{\widehat \bfalpha_t^{(0)},\widehat \bfbeta_t^{(0)},\widehat \bfgamma_t^{(0)},\widehat \bfxi_t^{(0)}\right\}_{t=1}^T, \bfphi^{(0)}, \lambda, A, P, M \Big) = \bfphi^{(0)}
    \end{equation}
    for all $\lambda \in \bbR$, $A \in \{\text{SAGA}, \text{SVRG}\}$, $P \in \{\texttt{True},\texttt{False}\}$, and $M \in \bbN$.
\end{lemma}

\begin{proof}

    We use induction to show that the following conditions hold for all $m \geq 0$:

    \begin{enumerate}
        \item $\widehat \bfalpha^{(m)}_t = \bfalpha_t(\bfphi^{(0)})$ and $\widehat \bfbeta^{(m)}_{t} = \bfbeta_t(\bfphi^{(0)})$ for all $t = 1,\ldots,T$.
        %
        \item $\widehat \bfgamma^{(m)}_{t} = \bfgamma_t(\bfphi^{(0)})$ and $\widehat \bfxi^{(m)}_{t} = \bfxi_t(\bfphi^{(0)})$ for all $t = 1,\ldots,T$.
        %
        \item $\bfphi^{(m)} = \bfphi^{(0)}$
        %
        \item $\widehat \nabla F^{(m)}_{t} = \nabla F_t\left(\bfphi^{(0)} \mid \bfgamma_t(\bfphi^{(0)}),\bfxi_t(\bfphi^{(0)})\right)$ for all $t = 1,\ldots,T$.
        %
        \item $\widehat \nabla F^{(m)} = 0$
    \end{enumerate}

    Algorithm (\ref{alg:VRSO-PE}) returns $\bfphi^{(M)}$ for some finite $M$, so proving the above will prove the lemma. 
    %
    The base case ($m=0$) is trivial either from the assumptions of the Lemma or from lines 1--4 of Algorithm (\ref{alg:VRSO-PE}), so we focus on the inductive case. We assume that all of the conditions (1--5) above hold for $m$, and prove each condition holds for $m+1$ sequentially.

    \begin{enumerate}
        \item First note that if $P = \texttt{False}$ or if $P = \texttt{True}$ and $t \neq t_m$, then the conditional probability approximations are not updated, so $\widehat \bfalpha^{(m+1)}_t = \widehat \bfalpha^{(m)}_t  = \bfalpha_t(\bfphi^{(0)})$ and $\widehat \bfbeta^{(m+1)}_t = \widehat \bfbeta^{(m)}_t = \bfbeta_t(\bfphi^{(0)})$ by inductive hypothesis (1).
        
        If $P = \texttt{True}$ and $t = t_m$, then both $\widehat \bfalpha^{(m+1)}_{t_m}$ and $\widehat \bfbeta^{(m+1)}_{t_m}$ are updated in line 9 of Algorithm (\ref{alg:EM-VRSO}). The approximation $\widehat \bfalpha^{(m+1)}_{t_m}$ is updated using the mapping 
        %
        \begin{align}
            \widehat \bfalpha^{(m+1)}_{t_m} &= \widetilde \bfalpha_{t_m}(\widehat \bfalpha^{(m)}_{t_m-1},\bfphi^{(m)}) \nonumber \\
            %
            &= \widetilde \bfalpha_{t_m}(\bfalpha_{t_m-1}(\bfphi^{(0)}),\bfphi^{(0)}) \nonumber \\
            %
            &= 
            \begin{cases}
                \bfdelta(\bfeta^{(0)}) ~ P(y_1;\bftheta^{(0)}), & \text{for } t_m = 1 \\
                \bfalpha_{t_m-1}(\bfphi^{(0)}) ~ \bfGamma(\bfeta^{(0)}) ~P(y_{t_m};\bftheta^{(0)}), & \text{for } t_m = 2,\ldots,T
            \end{cases} \nonumber \\
            %
            &= \bfalpha_{t_m}(\bfphi^{(0)}) \label{eqn:a_inductive_step}
        \end{align}
        The second line is true by inductive hypotheses (1) and (3), the third line is the definition of $\widetilde \bfalpha_{t_m}$, and the final line is the definition of $\bfalpha_{t_m}(\bfphi^{(0)})$. Similar logic can be used to show that 
        
        \begin{equation}
            \widehat \bfbeta^{(m+1)}_{t_m} = \bfbeta_{t_m}(\bfphi^{(0)}). \label{eqn:b_inductive_step}
        \end{equation}
        %
        \item First note that if $P = \texttt{False}$ or if $P = \texttt{True}$ and $t \neq t_m$, then the conditional probability approximations are not updated, so $\widehat \bfgamma^{(m+1)}_t = \widehat \bfgamma^{(m)}_t = \bfgamma_t(\bfphi^{(0)})$ and $\widehat \bfxi^{(m+1)}_t = \widehat \bfxi^{(m)}_t = \bfxi_t(\bfphi^{(0)})$ by inductive hypothesis (2).
        
        If $P = \texttt{True}$ and $t = t_m$, both $\widehat \bfgamma^{(m+1)}_{t_m}$ and $\widehat \bfxi^{(m+1)}_{t_m}$ are calculated in line 10 of Algorithm (\ref{alg:EM-VRSO}). $\widehat \bfxi^{(m+1)}_{t_m}$ is updated using the mapping 
        
        \begin{align}
            \widehat \bfxi^{(m+1)}_{t_m} &= \widetilde \bfxi_{t_m}(\widehat \bfalpha^{(m+1)}_{t_m-1},\widehat \bfbeta^{(m+1)}_{t_m}, \bfphi^{(m)}) \nonumber \\
            %
            &= \widetilde \bfxi_{t_m}(\bfalpha_{t_m-1}(\bfphi^{(0)}),\bfbeta_{t_m}(\bfphi^{(0)}), \bfphi^{(0)}) \nonumber  \\
            %
            &= \frac{\text{diag}(\bfalpha_{t-1}(\bfphi^{(0)})) ~~ \bfGamma(\bfeta^{(0)}) ~~ P(y_t;\bftheta^{(0)}) ~~ \text{diag}(\bfbeta_t(\bfphi^{(0)}))}{\bfalpha_{t-1}(\bfphi^{(0)}) ~~ \bfGamma(\bfeta^{(0)}) ~~ P(y_{t};\bftheta^{(0)}) ~~ \bfbeta_{t}(\bfphi^{(0)})^\top} \nonumber  \\
            %
            &= \bfxi_{t_m}(\bfphi^{(0)}). \label{eqn:g_inductive_step}
        \end{align}
        The second line is true by Equations (\ref{eqn:a_inductive_step} -- \ref{eqn:b_inductive_step}) and inductive hypothesis (3), the third line is the definition of $\widetilde \bfxi_{t_m}$, and the final line is the definition of $\bfxi_{t_m}(\bfphi^{(0)})$. Similar logic can be used to show that 
        
        \begin{equation}
            \widehat \bfgamma^{(m+1)}_{t_m} = \bfgamma_{t_m}(\bfphi^{(0)}). \label{eqn:x_inductive_step}
        \end{equation}
        %
        \item The parameter $\bfphi^{(m+1)}$ is calculated in line 12 of \texttt{VRSO-PE} as follows: 
        
        \begin{align*}
            \bfphi^{(m+1)} &= \bfphi^{(m)} - \lambda \left[\nabla F_{t_m}\left(\bfphi^{(m)} \mid \widehat \bfgamma^{(m+1)}_{t_m}, \widehat \bfxi^{(m+1)}_{t_m}\right) - \widehat \nabla F^{(m)}_{t_m} + \widehat \nabla F^{(m)} \right] \\
            %
            &= \bfphi^{(0)} - \lambda \left[\nabla F_{t_m}\left(\bfphi^{(0)} \mid \widehat \bfgamma^{(m+1)}_{t_m}, \widehat \bfxi^{(m+1)}_{t_m}\right) - \nabla F_{t_m}\left(\bfphi^{(0)} \mid \bfgamma_{t_m}(\bfphi^{(0)}), \bfxi_{t_m}(\bfphi^{(0)})\right) + 0 \right] \\
            %
            &= \bfphi^{(0)} - \lambda \left[ \nabla F_{t_m}\left(\bfphi^{(0)} \mid \bfgamma_{t_m}(\bfphi^{(0)}), \bfxi_{t_m}(\bfphi^{(0)})\right) - \nabla F_{t_m}\left(\bfphi^{(0)} \mid \bfgamma_{t_m}(\bfphi^{(0)}), \bfxi_{t_m}(\bfphi^{(0)})\right) \right] \\
            %
            &= \bfphi^{(0)}.
        \end{align*}
        The second line is true by the inductive hypotheses (3), (4), and (5), and the third line is true by Equations (\ref{eqn:g_inductive_step} -- \ref{eqn:x_inductive_step}).
        %
        \item Note that if $A = \text{SVRG}$ or if $A = \text{SAGA}$ and $t \neq t_m$, then the gradient approximation at index $t$ is not updated, so $\widehat \nabla F^{(m+1)}_t = \widehat \nabla F^{(m)}_t = \nabla F_t\left(\bfphi^{(0)} \mid \bfgamma_t(\bfphi^{(0)}),\bfxi_t(\bfphi^{(0)})\right)$ by inductive hypothesis (4).
        
        If $A = \text{SAGA}$ and $t = t_m$, then $\widehat \nabla F^{(m+1)}_{t_m}$ is calculated in line 14 of \texttt{VRSO-PE} as follows:
        
        \begin{align}
            \widehat \nabla F^{(m+1)}_{t_m} &= \nabla F_{t_m}\left(\bfphi^{(m)} \mid \widehat \bfgamma^{(m+1)}_{t_m}, \widehat \bfxi^{(m+1)}_{t_m}\right) \nonumber \\
            %
            &= \nabla F_{t_m}\left(\bfphi^{(0)} \mid \widehat \bfgamma^{(m+1)}_{t_m}, \widehat \bfxi^{(m+1)}_{t_m}\right) \nonumber \\
            %
            &= \nabla F_{t_m}\left(\bfphi^{(0)} \mid \bfgamma_{t_m}(\bfphi^{(0)}), \bfxi_{t_m}(\bfphi^{(0)})\right). \label{eqn:nabla_hat_inductive_step}
        \end{align}
        The second line is true by inductive hypothesis (3), and the third line is true by Equations (\ref{eqn:g_inductive_step} -- \ref{eqn:x_inductive_step}).
        %
        \item If $A = \text{SVRG}$, then the full gradient approximation is not updated, so $\widehat \nabla F^{(m+1)} = \widehat \nabla F^{(m)} = 0$ by inductive hypothesis (5).
        
        If $A = \text{SAGA}$, then $\widehat \nabla F^{(m+1)}$ is calculated in line 14 of \texttt{VRSO-PE} as follows:
        
        \begin{align*}
            \widehat \nabla F^{(m+1)} &= \widehat \nabla F^{(m)} + \frac{1}{T} \left(\widehat \nabla F^{(m+1)}_{t_m} - \widehat \nabla F^{(m)}_{t_m}\right) \\
            %
            &= 0 + \frac{1}{T} \left[\widehat \nabla F^{(m+1)}_{t_m} - \nabla F_{t_m}\left(\bfphi^{(0)} \mid \bfgamma_{t_m}(\bfphi^{(0)}), \bfxi_{t_m}(\bfphi^{(0)})\right)\right] \\
            %
            &= \frac{1}{T} \left[\nabla F_{t_m}\left(\bfphi^{(0)} \mid \bfgamma_{t_m}(\bfphi^{(0)}), \bfxi_{t_m}(\bfphi^{(0)})\right) - \nabla F_{t_m}\left(\bfphi^{(0)} \mid \bfgamma_{t_m}(\bfphi^{(0)}), \bfxi_{t_m}(\bfphi^{(0)})\right)\right] \\
            %
            &= 0.
        \end{align*}
        The second line is true by inductive hypotheses (4) and (5), and the third line is true by Equation (\ref{eqn:nabla_hat_inductive_step}).
        %
    \end{enumerate}
    This completes the inductive step, and proves the lemma.
\end{proof}

We now prove Theorem 2.

\begin{proof}

We use induction and show that if $\nabla \log p(\bfy;\bfphi_0) = 0$, then $\bfphi_{k} = \bfphi_0$ for all iterations $k$ of Algorithm (\ref{alg:EM-VRSO}). The base case, $k = 0$, is trivial. For the inductive case, we assume that $\bfphi_k = \bfphi_0$ and prove that $\bfphi_{k+1} = \bfphi_k$. First, note that 

\begin{align}
    \nabla F(\bfphi_{0} \mid \bfgamma(\bfphi_{0}), \bfxi(\bfphi_{0})) &= \frac{1}{T} \sum_{t=1}^T \nabla F_t(\bfphi_0 \mid \bfgamma_t(\bfphi_0), \bfxi_t(\bfphi_0)) \nonumber \\
    &= -\frac{1}{T} \nabla Q(\bfphi_0 \mid \bfphi_0) \nonumber \\
    &= -\frac{1}{T} \nabla \log p(\bfy ~;~ \bfphi_0) \nonumber \\
    &= 0 \label{eqn:nabla_zero}.
\end{align}
%
By Equation (\ref{eqn:nabla_zero}), the conditions within Lemma 6 are satisfied with $\bfphi^{(0)} = \bfphi_0$. Further, step 2 of Algorithm (\ref{alg:EM-VRSO}) performs the E step of the EM algorithm so that $\{\bfalpha_{k,t}, \bfbeta_{k,t}, \bfgamma_{k,t}, \bfxi_{k,t}\}_{t=1}^T = \left\{\bfalpha_t(\bfphi_k), \bfbeta_t(\bfphi_k), \bfgamma_t(\bfphi_k), \bfxi_t(\bfphi_k) \right\}_{t=1}^T$. So:

\begin{align*}
    \bfphi_{k,1} &= \texttt{VRSO-PE}\Big(\left\{\bfalpha_{k,t}, \bfbeta_{k,t}, \bfgamma_{k,t}, \bfxi_{k,t}\right\}_{t=1}^T, \bfphi_k, \lambda, A, P, M \Big) \\
    %
    &= \texttt{VRSO-PE}\Big(\left\{\bfalpha_t(\bfphi_k),\bfbeta_t(\bfphi_k),\bfgamma_t(\bfphi_k),\bfxi_t(\bfphi_k)\right\}_{t=1}^T, \bfphi_k, \lambda, A, P, M \Big)\\
    %
    &= \texttt{VRSO-PE}\Big(\left\{\bfalpha_t(\bfphi_0),\bfbeta_t(\bfphi_0),\bfgamma_t(\bfphi_0),\bfxi_t(\bfphi_0)\right\}_{t=1}^T, \bfphi_0, \lambda, A, P, M \Big)\\
    %
    &= \bfphi_0.
\end{align*}

The second line is true by the definition of the E step in the EM algorithm, the third line is true by the inductive hypothesis, and the final line is true by Lemma 6. Since $\log p(\bfy; \bfphi_{k,1}) = \log p(\bfy;\bfphi_{k}) = \log p(\bfy; \bfphi_{0})$, $\bfphi_{k,1}$ satisfies the condition to exit the while loop of Algorithm (\ref{alg:EM-VRSO}), so $\bfphi_{k+1} = \bfphi_{k,1} = \bfphi_{0}$. 

This completes the inductive step, so $\bfphi_k = \bfphi_0$ for all $k \geq 0$. To complete the proof, note that Algorithm (\ref{alg:EM-VRSO}) returns $\bfphi_K$, so $\texttt{EM-VRSO}(\bfphi_0, \lambda, A, M, K) = \bfphi_K = \bfphi_0$.
\end{proof}