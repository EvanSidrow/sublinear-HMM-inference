% \section{Induced Independence in the PHMM}

\subsection{Inducing Independence with Labels}

\label{ss:indep_w_labels}

Suppose that a data set $\Ya := \{\Ya_t\}_{t=1}^T$ is generated from a PHMM as described in Chapter 3. In this subsection, I will describe how to partition $\Ya$ such that it is comprised of independent sub-sequences. Then, I will describe how these independent sub-sequences can be used within a stochastic gradient descent algorithm to obtain maximum likelihood estimates for the parameters $\bar \theta$ and $\Gamma$. %Using these independent sub-sequences, it is straightforward to evaluate the log-likelihood of each sub-sequence separately to define an unbiased estimator of the log-likelihood from Equation (\ref{eqn:PHMM_like}). By taking gradients of these unbiased likelihood estimates, the likelihood can be maximized using stochastic gradient descent \citep{Robbins:1951}.

Intuitively, sequences of observations before and after a label ($\ell_t = 1$) should be independent of another since $\Ya_{t:T}$ depends on $\Ya_{1:t-1}$ only through the hidden state $X_t$. To make this idea formal, note that if $\ell_t = 1$, then $x_t$ is known and $\fa^{(i)}(\ya_t) = 0$ for all $i \neq x_t$. Thus, the matrix of emission densities $\Pa(\ya_t;\thetaa)$ has only one non-zero element in position $(x_t,x_t)$. Now define $\onevec_{N,x_t}$ as a row vector with a one in index $x_t$ and zeros everywhere else. Then, $\onevec^T_{N,x_t} \onevec_{N,x_t}$ is a matrix of all zeros except for a one in position $(x_t,x_t)$, so:
%
\begin{equation}
    \onevec^T_{N,x_t} \onevec_{N,x_t} \Pa(\ya_t;\thetaa) = \Pa(\ya_t;\thetaa).
    \label{eqn:new_P}
\end{equation}
%
Assume labels are known for a subset of $M$ times, which I denote as $\calT := \{t_m\}_{m=1}^M$. Further, define $t_0 = 1$, $t_{M+1}=T$, and $\onevec_{N,0} = \onevec_N$. Then, plugging Equation (\ref{eqn:new_P}) into Equation (\ref{eqn:PHMM_like}) gives

\begin{align}
    \calL(\thetaa,\Gamma;\ya) &= \left(\delta \Pa(\ya_1,\thetaa) \left(\prod_{s=2}^{t_1-1} \Gamma \Pa(\ya_s;\thetaa)\right) \Gamma \onevec^T_{N,x_{t_1}}\right) \left(\onevec_{N,x_{t_1}} \Pa(\ya_{t_1},\thetaa) \left(\prod_{s=t_1+1}^{t_2-1} \Gamma \Pa(\ya_s;\thetaa)\right) \Gamma \onevec^T_{N,x_{t_2}}\right) \ldots \nonumber \\
    %
    &= \left(\delta \Pa(\ya_1,\thetaa) \left(\prod_{s=2}^{t_1-1} \Gamma \Pa(\ya_s;\thetaa)\right) \Gamma \onevec_{N,x_{t_1}}^T\right) \prod_{m=1}^{M} \left( \onevec_{N,x_{t_m}} \Pa(\ya_{t_m},\thetaa) \left(\prod_{s=t_m+1}^{t_{m+1}-1} \Gamma \Pa(\ya_s;\thetaa)\right) \Gamma \onevec_{N,x_{t_{m+1}}}^T\right)  \nonumber \\
    %
    &:= \prod_{m=0}^{M} \calL_m(\thetaa,\Gamma;\ya_{t_m:t_{m+1}}).
    \label{eqn:HMM_label_like}
\end{align}

This form of the likelihood is advantageous because it is written as the product of separate likelihood terms $\calL_m(\thetaa,\Gamma;\ya_{t_m:t_{m+1}})$, where each term is the likelihood of the sub-sequence $\ya_{t_m:t_{m+1}}$. Denote the logarithm of $\calL(\thetaa,\Gamma;\ya)$ as $U(\thetaa,\Gamma;\ya)$, and the logarithm of $\calL_m(\thetaa,\Gamma;\ya_{t_m:t_{m+1}})$ as $U_m(\thetaa,\Gamma;\ya_{t_m:t_{m+1}})$. Then, $U$ can be written as

\begin{align}
    U(\thetaa,\Gamma;\ya) &:= \log\left(\calL(\thetaa,\Gamma;\ya)\right) \nonumber \\
    %
    &= \sum_{m=0}^M \log \left(\calL_m(\thetaa,\Gamma;\ya_{t_m:t_{m+1}})\right) \nonumber \\
    %
    & := \sum_{m=0}^M U_m(\thetaa,\Gamma;\ya_{t_m:t_{m+1}}).
    \label{eqn:HMM_label_loglike}
\end{align}
%
The gradient of Equation (\ref{eqn:HMM_label_loglike}) then takes the following form:
%
\begin{equation}
    \nabla_{\thetaa,\Gamma} U(\thetaa,\Gamma;\ya) = \sum_{m=0}^M \nabla_{\thetaa,\Gamma} U_m(\thetaa,\Gamma;\ya_{t_m:t_{m+1}}).
    \label{eqn:grad_HMM_label_loglike}
\end{equation}
%
Using the method of stochastic gradients, an unbiased, but noisy, estimator of $\nabla_{\thetaa,\Gamma} U(\thetaa,\Gamma;\ya)$ is simply given by a scaled gradient of the log-likelihood of one randomly selected subset \citep{Robbins:1951}:
%
\begin{gather}
    \widehat{\nabla_{\thetaa,\Gamma} U}(\thetaa,\Gamma,\ya) := \frac{1}{w_k} \nabla_{\thetaa,\Gamma} U_k(\thetaa,\Gamma;\ya_{t_k:t_{k+1}}) \label{eqn:SGD_estimate}, \text{ where} \\
    %
    k \sim \text{Categorical}\left(w_0,\ldots,w_M\right), \enspace w_m > 0, \enspace \sum_{m=0}^M w_m = 1. \nonumber
\end{gather}
%
I scale the gradient of the log-likelihood associated with subset $k$ by the inverse of its weight, $1/w_k$, to ensure that $\mathbb{E}\left[\widehat{\nabla_{\thetaa,\Gamma} U}(\thetaa,\Gamma,\ya)\right] = \nabla_{\thetaa,\Gamma} U(\thetaa,\Gamma;\ya)$ for all $k = 1,\ldots,M$. 

Stochastic gradient descent repeatedly updates some parameter estimates $\{\hat \thetaa, \hat \Gamma\}$ by calculating the gradient estimate from Equation (\ref{eqn:SGD_estimate}) at those parameter estimates, scaling the gradient by some learning rate $\eta$, and adding the scaled gradient estimate to the current parameter estimates $\{\hat \thetaa, \hat \Gamma\}$. In addition, the learning rate $\eta$ is usually defined to decay over time according to some schedule. This decay is necessary for the SGD algorithm to converge \citep{Robbins:1951}. The pseudo-code for SGD remarkably simple and is given below.

\begin{enumerate}
    \item Initialize parameter estimates $\hat \thetaa$ and $\hat \Gamma$, weights $\{w_m\}_{m=1}^M$, the number of iterations $I$, and a learning rate schedule $\{\eta_i\}_{i=1}^I$
    \item For $i = 1,\ldots,I$:
    \begin{enumerate}
        \item Randomly draw a subset of data: $k \sim \text{Categorical}\left(w_0,\ldots,w_M\right)$
        \item Calculate a gradient estimate: $\widehat{\nabla_{\thetaa,\Gamma}} \leftarrow \frac{1}{w_k} \nabla_{\thetaa,\Gamma} U_k(\hat \thetaa_{i-1},\hat \Gamma_{i-1};\ya_{t_k:t_{k+1}})$ 
        \item Update the parameter estimates: $(\hat \thetaa,\hat \Gamma) \leftarrow (\hat \thetaa,\hat \Gamma) + \eta_i \widehat{\nabla_{\thetaa,\Gamma}}$
    \end{enumerate}
    \item Return $(\hat \thetaa,\hat \Gamma)$
\end{enumerate}
%
While $\widehat{\nabla_{\thetaa,\Gamma} U}(\thetaa,\Gamma,\ya)$ is unbiased, it is also desirable to keep the variance of $\widehat{\nabla_{\thetaa,\Gamma} U}(\thetaa,\Gamma,\ya)$ as small as possible, which is difficult. Because the set $\{w_m\}_{m=0}^M$ is user-defined, I will look into methods which can reduce the variance of $\widehat{\nabla_{\thetaa,\Gamma} U}(\thetaa,\Gamma,\ya)$ with respect to $\{w_m\}_{m=0}^{M}$. I will begin with an intuitive formulation where the weight of each segment is proportional to its length $\left(w_m \propto t_{m+1}-t_m\right)$, and experiment with alternatives such as $w_m = 1/(M+1)$.
%but this formulation relies on the unrealistic assumption that $ \nabla_{\thetaa,\Gamma}U_m$ is a fixed scalar is proportional to $t_{k+1}-t_k$.
In addition, variance reduction for gradient estimators is a rich sub-field within optimization, and I plan on implementing various variance reduction techniques such as SAGA \citep{Defazio:2014} and stochastic variance reduced gradient (SVRG) \citep{Johnson:2013} to improve the quality of the gradient estimates from Equation (\ref{eqn:SGD_estimate}).

\subsection{Inducing Independence without Labels}

Although the stochastic gradient descent method described above can only estimate the parameters of a PHMM (i.e., when labels are observed), it is possible to extend it so it may be used on data observed from a standard HMM. In this subsection I will focus on maximizing the likelihood of a standard HMM as defined in Equation (\ref{eqn:HMM_like}).

First, select a subset of times $\calT = \{t_m\}_{m=1}^M$ and define the random set $\calX_\calT = \{X_t : t \in \calT\}$. The key insight for this procedure is that we can treat $\calX_\calT$ as missing data in the HMM and \textit{impute} the hidden states $X_t$ for all $t \in \calT$. %Because all values of $\calX_\calT$ are missing, its missingness is independent of the hidden state process $X$ and the observations $Y$. 
Then, conditioned on the estimated hidden states, all sub-sequences between imputed hidden states are independent and enjoy the advantages of the previous subsection. 

Recall that the likelihood of an HMM can take two different forms: the full data likelihood from Equation (\ref{eqn:HMM_like_full}) is defined over the hidden states $x$ and the observations $y$, while the marginal likelihood from Equation (\ref{eqn:HMM_like}) is only defined over the observations $y$. I now define the \textit{partial data likelihood}, which is defined over the observations $Y$ and the imputed hidden states, which I denote as  I denote the set of imputed states as $\hat \calX_\calT = \{\hat X_t : t \in \calT\}$. The partial data likelihood is simply the full data likelihood after marginalizing over the non-imputed hidden states $\{X_t: t \notin \calT\}$:
%
\begin{align}
    \calL(\theta,\Gamma;y,\hat \calX_\calT) &= \left(\delta P(y_1,\theta) \left(\prod_{s=2}^{t_1-1} \Gamma P(y_s;\theta)\right) \Gamma \onevec^T_{N,\hat X_{t_1}}\right) * \nonumber \\
    & \qquad \left(\onevec_{N,\hat X_{t_1}} P(y_{t_1},\theta) \left(\prod_{s=t_1+1}^{t_2-1} \Gamma P(y_s;\theta)\right) \Gamma \onevec^T_{N,\hat X_{t_2}}\right) * \ldots \nonumber \\
    %
    &= \left(\delta P(y_1,\theta) \left(\prod_{s=1}^{t_1-1} \Gamma P(y_s;\theta)\right) \Gamma \onevec_{N,\hat X_{t_1}}^T\right) * \nonumber \\
    & \qquad \prod_{m=1}^{M} \left( \onevec_{N,\hat X_{t_m}} P(y_{t_m},\theta) \left(\prod_{s=t_m+1}^{t_{m+1}-1} \Gamma P(y_s;\theta)\right) \Gamma \onevec_{N,\hat X_{t_{m+1}}}^T\right)  \nonumber \\
    %
    &:= \prod_{m=0}^{M} \calL_m(\theta,\Gamma;y_{t_m:t_{m+1}},\hat X_{t_m},\hat X_{t_{m+1}}).
    \label{eqn:HMM_like_partial}
\end{align}
%
\begin{align}
    \log\left(\calL(\theta,\Gamma;y,\hat \calX_\calT)\right) &:= U(\theta,\Gamma;y,\hat \calX_\calT) \nonumber \\
    &= \sum_{m=0}^{M} \log\left(\calL_m(\theta,\Gamma;y_{t_m:t_{m+1}},\hat X_{t_m},\hat X_{t_{m+1}})\right) \nonumber \\
    &:= \sum_{m=0}^{M} U_m(\theta,\Gamma;y_{t_m:t_{m+1}},\hat X_{t_m},\hat X_{t_{m+1}})
    \label{eqn:HMM_log_like_partial}
\end{align}
%
Then, armed with imputed states $\hat \calX_\calT$ and the log-likelihood from Equation (\ref{eqn:HMM_log_like_partial}), it is straightforward to perform stochastic gradient descent using the same algorithm described in section \ref{ss:indep_w_labels} above. Several methods are viable to impute the hidden states. One intuitive approach is to simply maximize the likelihood (\ref{eqn:HMM_log_like_partial}) with respect to $\hat \calX_\calT$ in addition to $\theta$ and $\Gamma$. Another is to draw $\hat \calX_\calT$ from $\Pr(\hat \calX_\calT | y_{1:T}) \propto \calL(\theta,\Gamma;\hat \calX_\calT,y_{1:T})$ and then maximize the likelihood conditioned on the imputed data. I will investigate both of these methods.

One alternative to the maximum likelihood approach above is to take a Bayesian approach and include priors over $\Gamma$ and $\theta$. Define the prior density over $\theta$ and $\Gamma$ as $p$. Then, the posterior density $\pi$ is equal to the following:

\begin{align}
    \pi(\theta,\Gamma,\hat \calX_\calT|y) &\propto p( \theta, \Gamma) \calL(\theta,\Gamma;y,\hat \calX_\calT) \nonumber \\
    %
    %&= \Pa(\theta, \Gamma)\Pa(\calX_\calT, y | \theta, \Gamma) \nonumber \\
    %
    &= p(\theta, \Gamma) \prod_{m=0}^{M} \calL_m(\theta,\Gamma;y_{t_m:t_{m+1}},\hat X_{t_m},\hat X_{t_{m+1}}), 
    \label{eqn:bayes_no_label_like} \\ \nonumber \\
    %
    \log(\pi(\theta,\Gamma,\hat \calX_\calT|y)) &= \log(p(\theta,\Gamma)) + \sum_{m=0}^{M} U_m(\theta,\Gamma;y_{t_m:t_{m+1}},\hat X_{t_m}, \hat X_{t_{m+1}}) - Z(y),
\end{align}
%
where $Z(y)$ is a normalizing constant. 

I propose a numerical procedure to maximize either the partial data likelihood $\calL$ or posterior distribution $\pi$ with respect to $\theta$, $\Gamma$, and $\hat \calX_\calT$. It is very similar to standard stochastic gradient descent, but it takes turns between maximizing the likelihood with respect to the imputed states $\hat \calX_\calT$ and with respect to the parameters $\{\theta,\Gamma\}$. The pseudocode for this procedure is as follows:

\begin{enumerate}
    \item Initialize parameter estimates $\hat \theta$ and $\hat \Gamma$, imputed hidden states $\hat \calX_\calT := \{\hat X_{t} : t \in \calT\}$, weights $\{w_m\}_{m=1}^M$, the number of iterations $I$, a learning rate schedule $\{\eta_i\}_{i=1}^I$.
    \item For $i = 1,\ldots,I$:
    \begin{enumerate}
        \item Draw $k \sim \text{Categorical}\left(w_0,\ldots,w_M\right)$
        %
        \item Update the values of $(\hat X_{t_k}, \hat X_{t_{k+1}})$ by maximizing the likelihood (or posterior) while holding all other values fixed:
        \begin{align*}
            (\hat X_{t_k},\hat X_{t_{k+1}}) \leftarrow \argmax_{(x_{t_k},x_{t_{k+1}})} & U_{k-1} \left(\hat \theta,\hat \Gamma ; y_{t_{k-1}:t_k},\hat X_{t_{k-1}}, x_{t_k}\right) + \\
            & U_{k} \left(\hat \theta,\hat \Gamma;y_{t_{k}:t_{k+1}},x_{t_{k}}, x_{t_{k+1}}\right) + \\
            & U_{k+1} \left(\hat \theta, \hat \Gamma;y_{t_{k+1}:t_{k+2}}, x_{t_{k+1}}, \hat X_{t_{k+2}}\right).
        \end{align*}
        %
        \item Calculate a gradient estimate:
        %
        \begin{itemize}
            \item When maximizing the likelihood $\calL$:
            %
            \begin{equation*}
                \widehat{\nabla_{\theta,\Gamma}} \leftarrow \frac{1}{w_k} \nabla_{\theta,\Gamma} U_{k} \left(\theta,\Gamma;y_{t_{k}:t_{k+1}},\hat X_{t_{k}}, \hat X_{t_{k+1}}\right), 
            \end{equation*}
            %
            \item When maximizing $\pi$:
            \begin{equation*}
                \widehat{\nabla_{\theta,\Gamma}} \leftarrow \nabla_{\theta,\Gamma} \log(p(\hat \theta,\hat \Gamma)) +  \frac{1}{w_k} \nabla_{\theta,\Gamma} U_{k} \left(\hat \theta,\hat \Gamma;y_{t_{k}:t_{k+1}}, \hat X_{t_{k}}, \hat X_{t_{k+1}}\right)
            \end{equation*}
            %
        \end{itemize}
        %
        \item Update $(\hat \theta,\hat \Gamma) \leftarrow (\hat \theta,\hat \Gamma) + \eta_i \widehat{\nabla_{\theta,\Gamma}}$
    \end{enumerate}
    \item Return $(\hat \theta,\hat \Gamma)$
\end{enumerate}