% !TeX root = ../ms.tex

The advent of high-frequency sensing technology has allowed researchers to model exceptionally long, high-frequency stochastic processes with increasingly complex HMMs \citep{Patterson:2017}. However, these complex models can be computationally expensive to fit \citep{Glennie:2023}.
%($\approx 50$ samples per second for our case study). The complicated, multi-scale dependence structure within these data sets has led to the advent of increasingly complex statistical models. As such, optimization algorithms for parameter estimation within complex HMMs over large data sets can be expensive.
We introduce an inference algorithm that speeds up maximum likelihood estimation for HMMs compared to existing batch-gradient methods. We do so without approximating the likelihood, which is required for many existing stochastic inference methods \citep{Gotoh:1998,Ye:2017}. %Importantly, our method does not require a closed-form solution to the M step, and thus allows fast inference on a wide variety of HMM models. %Models without closed-form solutions to the M step include HMMs with covariates in the transition probability matrix and/or state-dependent distributions, HMMs with autoregression in the state-dependent distributions, and non-parametric HMMs.
%Add a paragraph or two to help biologists know what to do with your method?  How should biologists proceed with the way they are currently describing animal behavior from HMM models?  How can they incorporate what you have done into their analyses?  Why should what you have done matter to biologists who are collecting biologging data?

Our method does not require a closed-form solution for the M step, which enables quick inference for a diverse range of HMM models. This is useful in practice because many hidden Markov models lack closed-form solutions for the M step. In ecology, \citet{Pirotta:2018} used covariates in the transition probability matrix of an HMM to determine the effect of fishing boats on the behavior of Northern Fulmars (\textit{Fulmarus glacialis}). Likewise, \citet{Lawler:2019} used autoregression in the state-dependent distributions of an HMM to model the movement of grey seals (\textit{Halichoerus grypus}). In finance, \citet{Langrock:2018} modelled the relationship between the price of energy and price of the oil in Spain using non-parametric HMMs. The parameters of these models can not be estimated using a naive Baum-Welch algorithm, so our algorithm is a good candidate to speed up inference for complicated HMMs of this type. This will be especially relevant in the future as advances in biologging and tracking technology allow practitioners to collect increasingly large and high-frequency data sets \citep{Patterson:2017}.

Our new algorithm was particularly effective when performing inference over large data sets. For example, when applied to simulations with $T=10^{5}$ observations, our algorithm converged in approximately half as many epochs and tended to converge to regions of higher likelihood compared to existing baselines. Our case study of killer whale kinematic data showed similar improvements, demonstrating how our optimization procedure makes complex hierarchical HMMs less computationally expensive to fit on large biologging data sets.

%Our inference method allows practitioners to use variance-reduced stochastic optimization techniques within the M step of an EM algorithm. We implement a partial E step in addition to the stochastic M step, and empirical results suggest that t
The partial E step variant of our algorithm (i.e. with $P = \texttt{True}$) outperforms baselines particularly well early in the optimization procedure (in the first $\sim$ 5 epochs). As such, using a partial E step may be particularly advantageous when researchers are modeling large data sets with relatively low convergence thresholds.

One method that is particularly aligned with our algorithm is that of \citet{Zhu:2017}, who implement variance-reduced stochastic optimization to perform the M step of the EM algorithm on high-dimensional latent-variable models. Their method obtains a sub-linear computational complexity in the length of the observation sequence as well as a linear convergence rate. However, they focus primarily on mixture models rather than HMMs, and they do not combine the variance-reduced stochastic M step with a partial E step, which is an extension that we implement here. Further, their theoretical results assume independence between observations, which we do not rely on here.

%Our stochastic optimization algorithms perform better within the simulation study compared to the case study. We believe this is due to model misspecification in the case study. Intuitively, optimization procedures require fewer examples (i.e. a shorter observations sequence) to learn a reasonable model when the data comes from a well-specified model (compared to a mis-specified model). Stochastic optimization is well-suited to problems in which there is a large degree of redundancy in the data, so our stochastic optimization algorithms shine in the case study, where the well-specified model produces more redundant data. 
%However, for real-world applications, the model is mis-specified, so there is likely less redundancy in the data. Therefore, more of the data set is needed to find a reasonable model.

%We found that SVRG tended to perform better than SAGA for all simulation and case studies. While we are not sure why this is the case, it could have to do with fact that we sampled without replacement, and summing equation (\ref{eqn:SAGA_update}) over all $t_m$ will cancel out all control variate terms so long as $M$ is a multiple of $T$. In particular:
%
%\begin{equation}
%    \theta_{k,M} = \theta_{k,0} - \lambda_F \sum_{m=1}^{M} \left[\nabla_\theta F^{(k)}_{t_m}(\theta_{k,m})\right]
%\end{equation}
%
%This cancellation of control variate terms is not the case for SAGA. A more formal analysis of why SVRG performs better than SAGA is left as future work.

%While we use SVRG and SAGA in our analysis as examples of variance-reduced stochastic optimization algorithms,
While we use SVRG and SAGA in our analysis, there are other variance-reduced stochastic optimization algorithms that could be applied within our framework. For example, SARAH recursively updates the control variate in the inner loop of the optimization algorithm \citep{Nguyen:2017}, SPIDER uses a path-integrated differential estimator of the gradient \citep{Fang:2018}, and LiSSA is a second-order variance-reduced stochastic optimization scheme \citep{Agarwal:2017}. Future work can integrate these algorithms within our framework and evaluate the resulting performance. 
%In addition, future work can be done to determine the rate of convergence for Algorithm (\ref{alg:EM-VRSO}) with $P = \texttt{False}$, as well as provide theoretical guarantees for Algorithm (\ref{alg:EM-VRSO}) with $P = \texttt{True}$.
%\citet{Chen:2018} integrate variance-reduction into the partial E step of the EM algorithm, which may also be possible within the partial E step of Algorithm (\ref{alg:EM-VRSO}). 
While we focus on an ecological case study here, the inference procedures we developed can unlock more complicated HMMs with larger latent spaces and bigger data sets for practitioners across a variety of disciplines.