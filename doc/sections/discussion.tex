% !TeX root = ../ms.tex

The advent of high-frequency sensing technology has allowed researchers to model exceptionally long, high-frequency stochastic processes with increasingly complex HMMs \citep{Patterson:2017}. However, these complex models can be computationally expensive to fit \citep{Glennie:2023}.
%($\approx 50$ samples per second for our case study). The complicated, multi-scale dependence structure within these data sets has led to the advent of increasingly complex statistical models. As such, optimization algorithms for parameter estimation within complex HMMs over large data sets can be expensive.
We introduce an inference algorithm that speeds up maximum likelihood estimation for HMMs compared to existing batch-gradient methods. We do so without approximating the likelihood, which is required for many existing stochastic inference methods \citep{Gotoh:1998,Ye:2017}. Importantly, our method does not require a closed-form solution to the M step, and thus allows fast inference on a wide variety of HMM models. Hidden Markov models without closed-form solutions to the M step are common in practice, and include HMMs with covariates in the transition probability matrix and/or emission distributions (e.g. \citet{Pirotta:2018}), HMMs with autoregression in the emissions distributions \citep{Lawler:2019}, and non-parametric HMMs \citep{Langrock:2018}.

Our new algorithm was particularly useful when performing inference over large data sets. For example, when applied to simulations with $T=10^{5}$ observations, our algorithm converged in approximately half as many epochs and tended to converge to regions of higher likelihood compared to existing baselines. Our case study of killer whale kinematic data showed similar improvements, demonstrating how our optimization procedure makes complex hierarchical HMMs less computationally expensive to fit on large biologging data sets.

%Our inference method allows practitioners to use variance-reduced stochastic optimization techniques within the M step of an EM algorithm. We implement a partial E step in addition to the stochastic M step, and empirical results suggest that t
The partial E step variant of our algorithm (i.e. with $P = \texttt{True}$) outperforms baselines particularly well early in the optimization procedure (in the first $\sim$ 5 epochs). As such, using a partial E step may be particularly advantageous when researchers are modelling large data sets with relatively low convergence thresholds.

%Our stochastic optimization algorithms perform better within the simulation study compared to the case study. We believe this is due to model misspecification in the case study. Intuitively, optimization procedures require fewer examples (i.e. a shorter observations sequence) to learn a reasonable model when the data comes from a well-specified model (compared to a mis-specified model). Stochastic optimization is well-suited to problems in which there is a large degree of redundancy in the data, so our stochastic optimization algorithms shine in the case study, where the well-specified model produces more redundant data. 
%However, for real-world applications, the model is mis-specified, so there is likely less redundancy in the data. Therefore, more of the data set is needed to find a reasonable model.

%We found that SVRG tended to perform better than SAGA for all simulation and case studies. While we are not sure why this is the case, it could have to do with fact that we sampled without replacement, and summing equation (\ref{eqn:SAGA_update}) over all $t_m$ will cancel out all control variate terms so long as $M$ is a multiple of $T$. In particular:
%
%\begin{equation}
%    \theta_{k,M} = \theta_{k,0} - \lambda_F \sum_{m=1}^{M} \left[\nabla_\theta F^{(k)}_{t_m}(\theta_{k,m})\right]
%\end{equation}
%
%This cancellation of control variate terms is not the case for SAGA. A more formal analysis of why SVRG performs better than SAGA is left as future work.

%While we use SVRG and SAGA in our analysis as examples of variance-reduced stochastic optimization algorithms,
While we use SVRG and SAGA in our analysis, there are other variance-reduced stochastic optimization algorithms that could be applied within our framework. For example, SARAH recursively updates the control variate in the inner loop of the optimization algorithm \citep{Nguyen:2017}, SPIDER uses a path-integrated differential estimator of the gradient \citep{Fang:2018}, and LiSSA is a second-order variance-reduced stochastic optimization scheme \citep{Agarwal:2017}. Future work can integrate these algorithms within our framework and evaluate the resulting performance. 
%In addition, future work can be done to determine the rate of convergence for Algorithm (\ref{alg:EM-VRSO}) with $P = \texttt{False}$, as well as provide theoretical guarantees for Algorithm (\ref{alg:EM-VRSO}) with $P = \texttt{True}$.
%\citet{Chen:2018} integrate variance-reduction into the partial E step of the EM algorithm, which may also be possible within the partial E step of Algorithm (\ref{alg:EM-VRSO}). 
While we focus on an ecological case study here, the inference procedures we developed can unlock more complicated HMMs with larger latent spaces and bigger data sets for practitioners across a variety of disciplines.