% !TeX root = ../main.tex

The advent of high-frequency sensing technology has allowed researchers to model exceptionally long, high-frequency stochastic processes with increasingly complex HMMs that can be computationally expensive to fit \citep{Patterson:2017}.
%($\approx 50$ samples per second for our case study). The complicated, multi-scale dependence structure within these data sets has led to the advent of increasingly complex statistical models. As such, optimization algorithms for parameter estimation within complex HMMs over large data sets can be expensive.
%
We introduce inference algorithms that speed up the maximum likelihood estimation for hidden Markov models compared to existing batch-gradient methods. We do so without approximating the likelihood, which is required for many existing sub-linear inference methods \citep{Gotoh:1998,Ye:2017}. Although it is related to the Baum-Welch algorithm, our method does not require a closed-form solution to the M-step, which allows fast inference on a larger set of HMM models. Such models are common in practice and include HMMs which use covariates in the transition probability matrix and/or emission distributions (e.g. \citet{Pirotta:2018}), HMMs with mixed effects (e.g. \citet{McClintock:2021}), HMMs with autoregression in the emissions distributions \citep{Lawler:2019}, and non-parametric HMMs \citep{Langrock:2018}. 

In our simulation study, the SVRG version of Algorithms (\ref{alg:EM-SO}) and (\ref{alg:P-EM-SO}) converged in approximately half as many epochs compared to BFGS on data sets with $T=10^{5}$ observations. They also tended to converge to regions of higher likelihood. 

For our case study of $T = 89462$ observations of killer whale kinematic data, our method also converged in fewer epochs and to regions of higher likelihood compared to BFGS. This inference technique lets us implement a model that classifies dive types and phases that would be difficult to use otherwise due to computational issues EXPAND A BIT HERE. Biologing data is growing in scale, and our optimization technique allows practitioners to implement models that are more computationally expensive. FIX LATER

Our inference method allows practitioners to use variance-reduced stochastic optimization techniques within the M-step of an EM algorithm. Importantly, our algorithms add minimal computational burden to the EM algorithm when no closed-form solution of the M step exists. We implement a partial E step in addition to the stochastic M step, and empirical results suggest that the partial E step may help the algorithm move to areas of high likelihood for large data sets and complicated models early in the optimization procedure.

%Our stochastic optimization algorithms perform better within the simulation study compared to the case study. We believe this is due to model misspecification in the case study. Intuitively, optimization procedures require fewer examples (i.e. a shorter observations sequence) to learn a reasonable model when the data comes from a well-specified model (compared to a mis-specified model). Stochastic optimization is well-suited to problems in which there is a large degree of redundancy in the data, so our stochastic optimization algorithms shine in the case study, where the well-specified model produces more redundant data. 
%However, for real-world applications, the model is mis-specified, so there is likely less redundancy in the data. Therefore, more of the data set is needed to find a reasonable model.

%We found that SVRG tended to perform better than SAGA for all simulation and case studies. While we are not sure why this is the case, it could have to do with fact that we sampled without replacement, and summing equation (\ref{eqn:SAGA_update}) over all $t_m$ will cancel out all control variate terms so long as $M$ is a multiple of $T$. In particular:
%
%\begin{equation}
%    \theta_{k,M} = \theta_{k,0} - \lambda_F \sum_{m=1}^{M} \left[\nabla_\theta F^{(k)}_{t_m}(\theta_{k,m})\right]
%\end{equation}
%
%This cancellation of control variate terms is not the case for SAGA. A more formal analysis of why SVRG performs better than SAGA is left as future work.

%While we use SVRG and SAGA in our analysis as examples of variance-reduced stochastic optimization algorithms,
While we use SVRG and SAGA in our analysis, there are other variance-reduced stochastic optimization algorithms that could be applied within this framework. For example, SARAH recursively updates the control variate in the inner loop of the optimization algorithm \citep{Nguyen:2017}, SPIDER uses a path-integrated differential estimator of the gradient \citep{Fang:2018}, and LiSSA is a second-order variance-reduced stochastic optimization scheme \citep{Agarwal:2017}. Future work can integrate these algorithms within our framework and evaluate the resulting performance. In addition, future work can be done to determine the rate of convergence for Algorithm (\ref{alg:EM-SO}), as well as provide theoretical guarantees for Algorithm (\ref{alg:P-EM-SO}). We conjecture that Algorithm (\ref{alg:EM-SO}) converges linearly. Finally, \citet{Chen:2018} integrate variance-reduction into the partial E-step of the EM algorithm, and integrating this into our algorithm may be a fruitful direction of future research. While we focus on an ecological case study, the inference procedures described here may unlock more complicated HMMs with larger latent spaces and bigger data sets for practitioners across a variety of disciplines.


\iffalse

Questions:

\begin{itemize}
    \item right now it looks like a constant step size is slow for eta vs theta. Why?
    \item should I use second order optimization methods?
    \item how long should I do a partial E step for? or how large should batch sizes be? Perhaps I should take the inverse of one minus the largest diagonal element of $\Gamma$ : $\max_i \frac{1}{1-\Gamma_{ii}}$
    \item I should probably use $||\sum_{n=m-m^*}^{m} \nabla P_{t_n}(z[n]) - \widehat \nabla P_{t_n} + \frac{1}{T} \sum_{t=1}^T \widehat \nabla P_t|| < \varepsilon$ as a convergence criterion to see if we should terminate, since that is my gradient estimate (Note that that expression is imprecise). In addition, we can assume that the gradient estimate is normally distributed with some unknown variance $\sigma^2$, so if the mean is less than the estimated variance then we can terminate. (use a non-central F-distribution for each dimension, add the statistics up).
    \item Use an adaptive step size of some sort with the likelihood after each epoch. Also, if the step size is too big, different components of $\eta$ show oscillatory behaviour (why?)
    \item How do I select a step size? We could use a line-search like in \citep{Schmidt:2017} to select step size, but also could we just use an adaptive $L$ for each function?  
    \item How do present my results? Calculating the true objective function at each point in kinda a lot (???) Maybe I just record the likelihood at the end of each epoch.
\end{itemize}

The advent of high-frequency biologging technology has allowed researchers to model animal behaviours in a variety of settings. However, current statistical models and inference methods can fail if these biologging data sets are sampled at exceptionally fine, coarse, or irregular scales.

In this thesis proposal, I have introduced a general, hierarchical framework that can be used to account for fine-scale, non-Markovian behaviour within the structure of an HMM. I use this framework to model the movement of killer whales, but it can be used to describe any high-frequency data set with state-switching behaviour and intricate fine-scale structure. 

I also propose a generalization of an HMM which incorporates rare or preferentially observed labels. I test this framework using a simple simulation study and propose a concrete research direction to test my method using a simulation study and a the killer whale case study.

Parameter estimation for these complex HMMs can be expensive. Therefore, I have introduced inference algorithms which update model parameters without iterating through the entire data set of observations. I detailed how I will used simulated and real-world data to compare my inference algorithms to current state-of-the-art methods.

Finally, I detail the general structure of many continuous-time animal movement models that describe animal behaviour using irregular or coarsely sampled time series. I describe computational issues with fitting these continuous-time methods, and I propose a parallel tempering algorithm for performing parameter inference. I prove a useful result that the rejection rate of my algorithm stays bounded as the time discretization goes to infinity and corroborate this theoretical result empirically using a simulated data set. I propose to test the parallel tempering algorithm using a sparsely-sampled data set of either killer whale or Gentoo penguin position as well as two simulated data sets which are known to exhibit either multi-dimensionality or multi-modality. Finally, I describe concrete evaluation metrics that I will use to compare my method with current state-of-the art techniques.

Many current models for animal movement and behaviour struggle on data sets sampled at very fine, coarse, or irregular time scales. These models either do not adequately describe the underlying movement process or are computationally expensive to fit. In this thesis proposal, I have introduced several animal movement models that can incorporate data on a variety of time scales. I have also introduced inference techniques that limit the computational burden of fitting such models. While I focus on an ecological case study using the kinematic data of a killer whale, researchers from a wide variety of fields can use the techniques I have proposed here to better understand data sets on a variety of scales.

\fi