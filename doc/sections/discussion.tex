% !TeX root = ../main.tex

The advent of high-frequency sensing technology has allowed researchers to model partially-observed stochastic processes that are exceptionally long $T > \approx 10^5$ and finely sampled ($\approx 50$ samples per second for our case study). The fine-scale structure of these processes has led to the advent of complex statistical models to deal with the complicated, multi-scale dependence structure within these data sets. Parameter estimation for complex HMMs over large data sets can be expensive. Therefore, we have introduced inference algorithms which update model parameters without iterating through the entire data set of observations.

It is intuitive that our stochastic optimization algorithms perform better for the simulation study compared to the case study. In particular, for a well-specified model, the algorithm would need fewer examples (i.e. a shorter observations sequence) to learn a reasonable model compared to a mis-specified model. Stochastic optimization is well-suited to problems in which there is a large degree of redundancy in the data. However, for real-world applications, the model is mis-specified, so there is likely less redundancy in the data. Therefore, more of the data set is needed to find a reasonable model.

We found that SVRG tended to perform better than SAGA for all simulation and case studies. While we are not sure why this is the case, it could have to do with fact that we sampled without replacement, and summing equation (\ref{eqn:SAGA_update}) over all $t_m$ will cancel out all control variate terms so long as $M$ is a multiple of $T$. In particular:
%
\begin{equation}
    \theta_{k,M} = \theta_{k,0} - \lambda_F \sum_{m=1}^{M} \left[\nabla_\theta F^{(k)}_{t_m}(\theta_{k,m})\right]
\end{equation}
%
This cancellation of control variate terms is not the case for SAGA. A more formal analysis of why SVRG performs better than SAGA is left as future work.

While we used SVRG and SAGA in our analysis, there are many more variance-reduced stochastic optimization algorithms that could be applied to this problem. SARAH recursively updates the control variate in the inner loop of the optimization algorithm \citep{Nguyen:2017}, SPIDER uses a path-integrated differential estimator of the gradient \citep{Fang:2018}, and LiSSA is a second-order variance-reduced stochastic optimization scheme \citep{Agarwal:2017}. We primarily focus on applying variance-reduced stochastic optimization in general in this work, so we focus on SAGA and SVRG as canonical optimization algorithms. However, future work can be done to examine the performance of all of these methods on learning HMM parameters.

While we focus on an ecological case study, complex HMMs are used to model time series data in a variety of fields including (but not limited to) speech recognition \citep{Gales:2008}, geology \citep{Bebbington:2007}, and ecology \citep{McClintock:2020}. Our inference method allows practitioners to use variance-reduced stochastic optimization techniques within the M-step of an EM algorithm. This allows the EM algorithm to be used when no closed-form solution of the M- step exists with minimal addition computational burden compared to the EM algorithm when the M-step does have a closed form solution. In addition, if a partial E-step is implemented we see that our algorithm empirically converges faster than even gradient-based methods such as BFGS.

This algorithm can unlock more complicated hidden Markov models with larger latent spaces and bigger data sets for practitioners who have been unable to use them previously due to computational constraints.

\iffalse

Questions:

\begin{itemize}
    \item right now it looks like a constant step size is slow for eta vs theta. Why?
    \item should I use second order optimization methods?
    \item how long should I do a partial E step for? or how large should batch sizes be? Perhaps I should take the inverse of one minus the largest diagonal element of $\Gamma$ : $\max_i \frac{1}{1-\Gamma_{ii}}$
    \item I should probably use $||\sum_{n=m-m^*}^{m} \nabla P_{t_n}(z[n]) - \widehat \nabla P_{t_n} + \frac{1}{T} \sum_{t=1}^T \widehat \nabla P_t|| < \varepsilon$ as a convergence criterion to see if we should terminate, since that is my gradient estimate (Note that that expression is imprecise). In addition, we can assume that the gradient estimate is normally distributed with some unknown variance $\sigma^2$, so if the mean is less than the estimated variance then we can terminate. (use a non-central F-distribution for each dimension, add the statistics up).
    \item Use an adaptive step size of some sort with the likelihood after each epoch. Also, if the step size is too big, different components of $\eta$ show oscillatory behaviour (why?)
    \item How do I select a step size? We could use a line-search like in \citep{Schmidt:2017} to select step size, but also could we just use an adaptive $L$ for each function?  
    \item How do present my results? Calculating the true objective function at each point in kinda a lot (???) Maybe I just record the likelihood at the end of each epoch.
\end{itemize}

The advent of high-frequency biologging technology has allowed researchers to model animal behaviours in a variety of settings. However, current statistical models and inference methods can fail if these biologging data sets are sampled at exceptionally fine, coarse, or irregular scales.

In this thesis proposal, I have introduced a general, hierarchical framework that can be used to account for fine-scale, non-Markovian behaviour within the structure of an HMM. I use this framework to model the movement of killer whales, but it can be used to describe any high-frequency data set with state-switching behaviour and intricate fine-scale structure. 

I also propose a generalization of an HMM which incorporates rare or preferentially observed labels. I test this framework using a simple simulation study and propose a concrete research direction to test my method using a simulation study and a the killer whale case study.

Parameter estimation for these complex HMMs can be expensive. Therefore, I have introduced inference algorithms which update model parameters without iterating through the entire data set of observations. I detailed how I will used simulated and real-world data to compare my inference algorithms to current state-of-the-art methods.

Finally, I detail the general structure of many continuous-time animal movement models that describe animal behaviour using irregular or coarsely sampled time series. I describe computational issues with fitting these continuous-time methods, and I propose a parallel tempering algorithm for performing parameter inference. I prove a useful result that the rejection rate of my algorithm stays bounded as the time discretization goes to infinity and corroborate this theoretical result empirically using a simulated data set. I propose to test the parallel tempering algorithm using a sparsely-sampled data set of either killer whale or Gentoo penguin position as well as two simulated data sets which are known to exhibit either multi-dimensionality or multi-modality. Finally, I describe concrete evaluation metrics that I will use to compare my method with current state-of-the art techniques.

Many current models for animal movement and behaviour struggle on data sets sampled at very fine, coarse, or irregular time scales. These models either do not adequately describe the underlying movement process or are computationally expensive to fit. In this thesis proposal, I have introduced several animal movement models that can incorporate data on a variety of time scales. I have also introduced inference techniques that limit the computational burden of fitting such models. While I focus on an ecological case study using the kinematic data of a killer whale, researchers from a wide variety of fields can use the techniques I have proposed here to better understand data sets on a variety of scales.

\fi