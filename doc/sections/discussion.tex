The advent of high-frequency sensing technology has allowed researchers to model exceptionally long, high-frequency stochastic processes with increasingly complex HMMs \citep{Patterson:2017}. However, these complex models can be computationally expensive to fit \citep{Glennie:2023}. We introduce an inference algorithm that speeds up maximum likelihood estimation for HMMs compared to existing batch-gradient methods. We do so without approximating the likelihood, which is required for many existing stochastic inference methods \citep{Gotoh:1998,Ye:2017}.

Our method does not require a closed-form solution for the M step, which enables quick inference for a diverse range of HMM models. Such a method is useful in practice because many HMMs lack closed-form solutions for the M step. In finance, \citet{Langrock:2018} modeled the relationship between the price of energy and price of the oil in Spain using non-parametric HMMs. In ecology, \citet{Lawler:2019} used autoregression in the state-dependent distributions of an HMM to model the movement of grey seals (\textit{Halichoerus grypus}). Likewise, \citet{Pirotta:2018} used covariates in the transition probability matrix of an HMM to determine the effect of fishing boats on the behavior of Northern Fulmars (\textit{Fulmarus glacialis}). Our method will be especially relevant as advances in biologging and tracking technology allow practitioners to collect increasingly large and high-frequency data sets \citep{Patterson:2017}. 

Our new algorithm was particularly effective when performing inference over large data sets. For example, when applied to simulations with $T=10^{5}$ observations, our algorithm converged in approximately half as many epochs and tended to converge to regions of higher likelihood compared to existing baselines. Our case study of killer whale kinematic data showed similar improvements, demonstrating how our optimization procedure makes complex hierarchical HMMs less computationally expensive to fit on large biologging data sets.

The partial E step variant of our algorithm (i.e. with $P = \texttt{True}$) outperforms baselines particularly well early in the optimization procedure (in the first $\approx$ 5 epochs). As such, using a partial E step may be particularly advantageous when researchers are modeling large data sets with relatively low convergence thresholds.

One method that is particularly aligned with our algorithm is that of \citet{Zhu:2017}, who implement variance-reduced stochastic optimization to perform the M step of the EM algorithm on high-dimensional latent-variable models. Their method obtains a sub-linear computational complexity in the length of the observation sequence as well as a linear convergence rate. However, they focus primarily on mixture models rather than HMMs, and they do not combine the variance-reduced stochastic M step with a partial E step, which is an extension that we implement here. Further, their theoretical results assume independence between observations, which we do not rely on here.

Future work can explore the performance of our new algorithm when applied to increasingly complex HMMs. Parameter inference for these complicated models may add difficulties beyond those presented in our simulation and case studies. For example, HMMs with covariates in the transition matrix are less stable than time-homogeneous HMMs, and optimization algorithms can easily get stuck in local optima of the likelihood surface. Our optimization algorithm was less prone to getting stuck in local optima than the baseline methods in this work, so the same may be true for more complicated HMMs.

While we use SVRG and SAGA in our analysis, there are other variance-reduced stochastic optimization algorithms that could be applied within our framework. For example, SARAH recursively updates the control variate in the inner loop of the optimization algorithm \citep{Nguyen:2017}, SPIDER uses a path-integrated differential estimator of the gradient \citep{Fang:2018}, and LiSSA is a second-order variance-reduced stochastic optimization scheme \citep{Agarwal:2017}. Future work can integrate these algorithms within our framework and evaluate the resulting performance. While we focus on an ecological case study here, the inference procedures we developed can unlock more complicated HMMs with larger latent spaces and bigger data sets for practitioners across a variety of disciplines.