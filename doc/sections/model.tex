Both the E step and the M step of the Baum-Welch algorithm are expensive when the length of the observation sequence ($T$) is large. The E step is expensive because $\bfgamma_t(\bfphi_{k})$ and $\bfxi_t(\bfphi_{k})$ must be calculated for $t = 1,\ldots,T$ to define $Q(\bfphi \mid \bfphi_k)$. If closed-form solutions to (\ref{eqn:BW_update}) are not readily available, then the M step is also expensive because evaluating full gradients of $Q(\bfphi \mid \bfphi_{k})$ takes $\calO(T)$ time. In this section, we introduce an original algorithm that speeds up both the expensive M step as well as the expensive E step of the Baum-Welch algorithm.

\subsection{Variance-Reduced Stochastic M Step}
\label{subsec:stoch_M}

To speed up the expensive M step, we notice from Equation (\ref{eqn:Q_sum}) that $Q$ is a large sum and thus implement variance-reduced stochastic optimization. It is straightforward to re-frame the M step of iteration $k$ of the Baum Welch algorithm from Equation (\ref{eqn:BW_update}) so it looks like the minimization problem from Equation (\ref{eqn:stoch_opt}). To do so, we define $\bfxi_1 = \emptyset$ and the loss function $F(\cdot \mid \bfgamma,\bfxi)$ as follows:

\begin{gather}
    F(\bfphi \mid \bfgamma, \bfxi) = \frac{1}{T}\sum_{t=1}^T F_t(\bfphi \mid \bfgamma_t , \bfxi_t), \qquad \text{where} \label{eqn:F}\\
    %
    F_1(\bfphi \mid \bfgamma_1,\bfxi_1) = - \sum_{i=1}^N \gamma^{(i)}_1 \log f^{(i)}(y_t;\theta^{(i)}) - \sum_{i=1}^N \gamma^{(i)}_1 \log \delta^{(i)}(\bfnu), \label{eqn:F_1} \\
    %
    F_t(\bfphi \mid \bfgamma_t , \bfxi_t) = - \sum_{i=1}^N \gamma^{(i)}_t \log f^{(i)}(y_t;\theta^{(i)}) - \sum_{i=1}^N \sum_{j=1}^N \xi^{(i,j)}_t \log \Gamma^{(i,j)}(\bfeta), \quad t = 2, \ldots, T. \label{eqn:F_t}
\end{gather}
%
The two functions $F$ and $Q$ are closely related to one another, as $F(\bfphi \mid \bfgamma(\bfphi_k), \bfxi(\bfphi_k)) = - Q(\bfphi \mid \bfphi_k) / T$. However, we make a distinction between the two to bridge the gap between existing EM literature (which uses $Q$) and stochastic optimization literature (which uses $F$). At any iteration $k$ of the EM algorithm, the loss function $F(\cdot \mid \bfgamma(\bfphi_k),\bfxi(\bfphi_k))$ can be minimized using Algorithm \ref{alg:VRSO}.

There are additional reasons to use SAGA and SVRG within the Baum-Welch algorithm beyond the standard benefits of variance-reduced stochastic optimization. Traditionally, SAGA is more memory intensive than SVRG because the gradient at every index must be stored. However, the Baum-Welch algorithm involves storing weights for each time index $t$ to define $F(\cdot \mid \bfgamma(\bfphi_k), \bfxi(\bfphi_k))$, so storing each gradient for SAGA is not considerably more memory intensive than the Baum-Welch algorithm itself. Alternatively, SVRG can be more computationally expensive than SAGA partially because it requires periodically re-calculating the full gradient approximation $\widehat \nabla F^{(0)}$, and this involves a full pass of the underlying data set. However, the E step of the Baum-Welch algorithm also involves a full pass of the data set, so using SVRG is not considerably more computationally expensive than the Baum-Welch algorithm itself. In this way, using either SAGA or SVRG in the M step adds minimal computational and memory complexity to the Baum-Welch algorithm.

\subsection{Partial E Step within the M step}
\label{subsec:stoch_E}

Variance-reduced stochastic optimization reduces the computational cost of the M step, but the E step itself still has a time complexity of $\calO(T)$, which can be prohibitive for large $T$. To decrease this computational burden, \citet{Neal:1998} justify a partial E step within the EM algorithm for general latent variable models. However, they assume that the optimization of the M step has a closed-form solution. We use their method as inspiration and add a partial E step to the stochastic M step of the Baum-Welch algorithm. 

Consider running one iteration of our version of the Baum-Welch algorithm with an initial parameter estimate $\bfphi^{(0)}$. The E step involves calculating the conditional probabilities $\bfgamma(\bfphi^{(0)})$ and $\bfxi(\bfphi^{(0)})$, and the M step involves running variance-reduced stochastic optimization with loss function $F(\cdot \mid \bfgamma(\bfphi^{(0)}),\bfxi(\bfphi^{(0)}))$ and initial parameter value $\bfphi^{(0)}$. Now, suppose $\bfphi^{(m)}$ is to be updated using a gradient estimate using a random observation index $t_m$. The function $F_{t_m}(\cdot \mid \bfgamma_{t_m}(\bfphi^{(0)}),\bfxi_{t_m}(\bfphi^{(0)}))$ depends on $\bfgamma_{t_m}(\bfphi^{(0)})$ and $\bfxi_{t_m}(\bfphi^{(0)})$, each of which are vectors of conditional probabilities given $\bfphi^{(0)}$. However, $\bfphi^{(0)}$ is an out-of-date parameter estimate since the current parameter estimate is $\bfphi^{(m)}$. Therefore, it is natural to update $\bfgamma_{t_m}$ and $\bfxi_{t_m}$ and redefine $F_{t_m}(\cdot \mid \bfgamma_{t_m}, \bfxi_{t_m})$ before calculating $\bfphi^{(m+1)}$. 

A naive method would be to calculate the new conditional probabilities $\bfgamma_{t_m}(\bfphi^{(m)})$ and $\bfxi_{t_m}(\bfphi^{(m)})$ and then update $F_{t_m}$ as $F_{t_m}(\cdot \mid \bfgamma_{t_m}(\bfphi^{(m)}), \bfxi_{t_m}(\bfphi^{(m)}))$. This would ensure that $\bfgamma_{t_m}$ and $\bfxi_{t_m}$ are completely up-to-date, but evaluating $\bfgamma_{t_m}(\bfphi^{(m)})$ and $\bfxi_{t_m}(\bfphi^{(m)})$ takes $\calO(TN^2)$ time and requires a full E step. Instead, our goal is to update $\bfgamma_{t_m}$ and $\bfxi_{t_m}$ in a way that does not scale with $T$.
%

To this end, we define the mappings $\widetilde \bfalpha_t$, $\widetilde \bfbeta_t$, $\widetilde \bfgamma_t$, and $\widetilde \bfxi_t$ for $t = 1,\ldots,T$ similarly to Equations (\ref{eqn:alpha}--\ref{eqn:xi}). If $\mathbf{a} \in \bbR^N$ and $\mathbf{b} \in \bbR^N$ are generic row vectors, then:

\begin{gather}
    \widetilde \bfalpha_1(\mathbf{a},\bfphi) = \bfdelta ~ P(y_1;\bftheta), \qquad \widetilde \bfalpha_t(\mathbf{a},\bfphi) = \mathbf{a} ~ \bfGamma ~ P(y_t;\bftheta), \quad t = 2,\ldots,T, \label{eqn:tilde_alpha} \\
    %
    \widetilde \bfbeta^\top_T(\mathbf{b},\bfphi) = \mathbf{1}_N^\top, \qquad \widetilde \bfbeta^\top_t(\mathbf{b},\bfphi) = \bfGamma ~ P(y_{t+1};\bftheta) ~ \mathbf{b}^\top, \quad t = 1,\ldots,T-1, \label{eqn:tilde_beta} \\ \nonumber \\
    %
    \widetilde \bfgamma_t(\mathbf{a},\mathbf{b}) = \frac{\mathbf{a} ~ \text{diag}(\mathbf{b})}{\mathbf{a} ~ \mathbf{b}^T}, \quad t = 1,\ldots,T, \label{eqn:tilde_gamma} \\ \nonumber \\
    %
    \widetilde \bfxi_{t}(\mathbf{a},\mathbf{b},\bfphi) = \frac{\text{diag}(\mathbf{a}) ~ \bfGamma ~ P(y_t;\bftheta) ~ \text{diag}(\mathbf{b})}{\mathbf{a} ~ \bfGamma ~ P(y_{t};\bftheta) ~ \mathbf{b}^\top}, \quad t = 2,\ldots,T \label{eqn:tilde_xi},
\end{gather}
%
all of which take $\calO(N^2)$ time to compute. We have designed these mappings so that $\widetilde \bfalpha_1(\mathbf{a},\bfphi) = \bfalpha_{1}(\bfphi)~$, $~\widetilde \bfalpha_t(\bfalpha_{t-1}(\bfphi),\bfphi) = \bfalpha_{t}(\bfphi)~$, $~\widetilde \bfbeta^\top_T(\mathbf{b},\bfphi) = \bfbeta^\top_{T}(\bfphi)~$, and $~\widetilde \bfbeta^\top_t(\bfbeta_{t+1}(\bfphi),\bfphi) = \bfbeta^\top_{t}(\bfphi)$.

At the beginning of the M step we define conditional probability \textit{approximations} $\widehat \bfalpha_{t}^{(0)} = \bfalpha_t(\bfphi^{(0)})$, $\widehat \bfbeta_{t}^{(0)} = \bfbeta_t(\bfphi^{(0)})$, $\widehat \bfgamma_{t}^{(0)} = \bfgamma_t(\bfphi^{(0)})$, and $\widehat \bfxi_{t}^{(0)} = \bfxi_t(\bfphi^{(0)})$ for $t = 1,\ldots,T$. This is simply the E step of the Baum-Welch algorithm and takes $\calO(TN^2)$ time to compute. Then, at any given step $m$ of the stochastic M step, we update $F_{t_m}$ by first updating $\widehat \bfalpha_{t_m}^{(m+1)} = \widetilde \bfalpha_{t_m}\left(\widehat \bfalpha_{t_m-1}^{(m)} ~,~ \bfphi^{(m)}\right)$ and $\widehat \bfbeta_{t_m}^{(m+1)} = \widetilde \bfbeta_{t_m}\left(\widehat \bfbeta_{t_m+1}^{(m)} ~,~ \bfphi^{(m)}\right)$, followed by $\widehat \bfgamma_{t_m}^{(m+1)} = \widetilde \bfgamma_{t_m}\left(\widehat \bfalpha_{t_m}^{(m+1)} ~,~ \widehat \bfbeta_{t_m}^{(m+1)}\right)$ and $\widehat \bfxi_{t_m}^{(m+1)} = \widetilde \bfxi_{t_m}\left(\widehat \bfalpha_{t_m-1}^{(m+1)} ~,~ \widehat \bfbeta_{t_m}^{(m+1)} ~,~ \bfphi^{(m)}\right)$. Finally, the loss function at index $t_m$ can be defined as $F_{t_m}\left(\cdot ~ \Big | ~ \widehat \bfgamma_{t_m}^{(m+1)},\widehat \bfxi_{t_m}^{(m+1)}\right)$. Updating $F_{t_m}$ in this way take a total of $\calO(N^2)$ time, which accomplishes a parameter update step that does not scale with $T$. Algorithm \ref{alg:VRSO-PE} outlines the M step of the Baum-Welch algorithm with a partial E step integrated in. 

The partial E step detailed above is closely related to belief propagation, an algorithm that calculates conditional probabilities of variables within graphical models \citep{Pearl:1982}. In fact, the E step of the Baum-Welch algorithm is a specific instance of belief propagation, where Equations (\ref{eqn:tilde_alpha} -- \ref{eqn:tilde_xi}) above correspond to ``passing messages" within the graphical model. Belief propagation can only perform exact inference on acyclic graphical models (including HMMs), but a generalization called loopy belief propagation can perform approximate inference on general graphical models \citep{Pearl:1988}. Practitioners balance approximation error and computational complexity to decide how long to run loopy belief propagation. Likewise, we consider computational complexity when running belief propagation and evaluate Equations (\ref{eqn:tilde_alpha} -- \ref{eqn:tilde_xi}) only once to approximate $\bfgamma_{t_m}(\bfphi^{(m)})$ and $\bfxi_{t_m}(\bfphi^{(m)})$. This forms the basis of the partial E step within the stochastic M step of our modified Baum-Welch algorithm.

\begin{algorithm}
\caption{\texttt{VRSO-PE}$(\{\widehat \bfalpha_t^{(0)}, \widehat \bfbeta_t^{(0)}, \widehat \bfgamma_t^{(0)}, \widehat \bfxi_t^{(0)}\}_{t=1}^T,\bfphi^{(0)},\lambda,A,P,M)$}\label{alg:VRSO-PE}
\begin{algorithmic}[1]
\Require Initial conditional probability approximations $\{\widehat \bfalpha^{(0)}, \widehat \bfbeta^{(0)}, \widehat \bfgamma^{(0)}, \widehat \bfxi^{(0)}\}$, initial parameters $\bfphi^{(0)}$, step size $\lambda$, algorithm $A \in \{\text{SVRG, SAGA}\}$, whether to do a partial-E step $P \in \{\texttt{True,False}\}$, and number of iterations $M$.
%
\For{$t=1,\ldots,T$} \Comment{initialize gradients}
    \State $\widehat \nabla F^{(0)}_{t} = \nabla F_t\left(\bfphi^{(0)} ~ \Big | ~ \widehat \bfgamma^{(0)}_{t}, \widehat \bfxi^{(0)}_{t}\right)$ 
\EndFor
\State $\widehat \nabla F^{(0)} = (1/T) \sum_{t=1}^T \widehat \nabla F^{(0)}_{t}$
%
\For{$m = 0,\ldots,M-1$}:
    %
    \State Pick $t_m \in \{1,\ldots,T\}$ uniformly at random.
    %
    \State $\left\{\widehat \bfalpha^{(m+1)}_t, \widehat \bfbeta^{(m+1)}_t, \widehat \bfgamma^{(m+1)}_t, \widehat \bfxi^{(m+1)}_t\right\} = \left\{\widehat \bfalpha^{(m)}_{t}, \widehat \bfbeta^{(m)}_{t}, \widehat \bfgamma^{(m)}_{t}, \widehat \bfxi^{(m)}_{t}\right\} \enspace$ for $t = 1,\ldots,T$.
    %
    \If{$P = \texttt{True}$} \Comment{partial E step}
    \State $\widehat \bfalpha_{t_m}^{(m+1)} = \widetilde \bfalpha_{t_m}\left(\widehat \bfalpha_{t_m-1}^{(m)},\bfphi^{(m)}\right), \quad \widehat \bfbeta_{t_m}^{(m+1)} = \widetilde \bfbeta_{t_m}\left(\widehat \bfbeta_{t_m+1}^{(m)},\bfphi^{(m)}\right)$ 
    %
    \State $\widehat \bfgamma_{t_m}^{(m+1)} = \widetilde \bfgamma_{t_m}\left(\widehat \bfalpha_{t_m}^{(m+1)},\widehat \bfbeta_{t_m}^{(m+1)}\right), 
    \quad \widehat \bfxi_{t_m}^{(m+1)} = \widetilde \bfxi_{t_m}\left(\widehat \bfalpha_{t_m-1}^{(m+1)},\widehat \bfbeta_{t_m}^{(m+1)},\bfphi^{(m)}\right)$
    \EndIf
    %
    \State \Comment{update parameters}
    \begin{gather}
        \bfphi^{(m+1)} = \bfphi^{(m)} - \lambda \left[\nabla F_{t_m}\left(\bfphi^{(m)} ~ \Big | ~ \widehat \bfgamma_{t_m}^{(m+1)}, \widehat \bfxi_{t_m}^{(m+1)}\right) - \widehat \nabla F^{(m)}_{t_m} + \widehat \nabla F^{(m)} \right]
    \end{gather}
    %
    \State $\widehat \nabla F_{t}^{(m+1)} = \widehat \nabla F_{t}^{(m)} \enspace$ for $t = 1,\ldots,T$ \Comment{update gradients}
    %
    \If{$A$ = SAGA}:
        \begin{gather}
            \widehat \nabla F_{t_m}^{(m+1)} = \nabla F_{t_m}\left(\bfphi^{(m)} ~ \Big | ~ \widehat \bfgamma_{t_m}^{(m+1)}, \widehat \bfxi_{t_m}^{(m+1)}\right), \\
            %
            \widehat \nabla F^{(m+1)} = \widehat \nabla F^{(m)} + \frac{1}{T} \left( \widehat \nabla F_{t_m}^{(m+1)} - \widehat \nabla F_{t_m}^{(m)}\right).
        \end{gather}
    \EndIf
\EndFor
\State \Return $\bfphi^{(M)}$
\end{algorithmic}
\end{algorithm}

\subsection{Full Algorithm}

In principal, it is possible to run Algorithm \ref{alg:VRSO-PE} alone without ever performing a full E step. However, if no partial E step is used (i.e. $P = \texttt{False}$) or if SVRG is used as the optimization algorithm, then either the conditional probability approximations $\left\{\widehat \bfalpha_t^{(m)}, \widehat \bfbeta_t^{(m)}, \widehat \bfgamma_t^{(m)}, \widehat \bfxi_t^{(m)} \right\}_{t=1}^T$ or the gradient approximations $\left\{\widehat \nabla F_{t}^{(m)} \right\}_{t=1}^T$ will not be updated and become out-of-date. To avoid this issue, Algorithm \ref{alg:EM-VRSO} combines the M step defined in Algorithm \ref{alg:VRSO-PE} with a full E step to complete our new Baum-Welch algorithm for HMMs.

\begin{algorithm}
\caption{\texttt{EM-VRSO}$(\bfphi_0,\lambda, A, P, M, K)$ (Version 1)}\label{alg:EM-VRSO}
\begin{algorithmic}[1]
\Require Initial parameters ($\bfphi_{0}$), step size ($\lambda$), algorithm $A \in \{\text{SVRG, SAGA}\}$, whether to do a partial E step $P \in \{\texttt{True,False}\}$, iterations per update ($M$), and number of updates ($K$).
%
\For{$k = 0,\ldots,K-1$}
% 
\State $\{\bfalpha_{k,t}, \bfbeta_{k,t}, \bfgamma_{k,t}, \bfxi_{k,t}\}_{t=1}^T = \texttt{E-step}(\bfphi_{k})$ \Comment{E step}
%
\State $\ell \gets 0$ \Comment{M step}
%
\While{$\ell = 0$ or $\log p(\bfy; \bfphi_{k,\ell}) < \log p(\bfy;\bfphi_{k})$} 
\State $\ell \gets \ell+1$
\State $\bfphi_{k,\ell} = \texttt{VRSO-PE}(\{\bfalpha_{k,t}, \bfbeta_{k,t}, \bfgamma_{k,t}, \bfxi_{k,t}\}_{t=1}^T,\bfphi_k,\lambda,A,P,M)$
%
\EndWhile
\State $\bfphi_{k+1} = \bfphi_{k,\ell}$
\EndFor
\State \Return $\bfphi_K$
\end{algorithmic}
\end{algorithm}

There are two versions of \texttt{EM-VRSO}. Version 1 (Algorithm \ref{alg:EM-VRSO}), requires the likelihood to not decrease (i.e. $\log p(\bfy;\bfphi_{k,\ell}) \geq \log p(\bfy;\bfphi_{k})$) in order to exit the while loop of the M step. Version 2 (Algorithm 6) requires the likelihood to \textit{strictly} increase by some threshold to exit the while loop of the M step. We use version 2 to prove theoretical results, but the strict threshold relies on values that are usually not known in practice. Therefore, we use version 1 in our simulation and case studies and defer version 2 to the online appendix. Our simulation and case studies show that version 1 of \texttt{EM-VRSO} converges to local maxima of the log-likelihood function in practice. 

At first, it seems troubling to require $\log p(\bfy;\bfphi_{k,\ell}) \geq \log p(\bfy;\bfphi_{k})$ to exit the while loop of \texttt{EM-VRSO}, since this requirement may cause an infinite loop if it cannot be met. In practice, we found that the log-likelihood increased after a pass through the M step in most cases unless the step size $\lambda$ was very large. As such, we detail how to adaptively select $\lambda$ in section \ref{sec:prac}. For a theoretical justification that the while loop terminates, denote $\ell^*(k)$ as the (random) number of runs through the inner loop of \texttt{EM-VRSO} for iteration $k$ (i.e. the maximum value obtained by $\ell$ for a given value of $k$). We prove in Theorem 1 below that $\bbP(\ell^*(k) < \infty) = 1$. One final concern is whether the sequence $\{\bfphi_{k}\}_{k=0}^\infty$ generated by \texttt{EM-VRSO} converges to a local maximum of the likelihood as $K \to \infty$. Theorem 1 below guarantees such convergence under standard regularity conditions. We prove Theorem 1 in the online appendix.
    
\begin{theorem}

    Suppose the following conditions are met in Algorithm 6 with $P = \texttt{False}$ and $A = \text{SVRG}$:
    
    \begin{enumerate}
        \item The parameters $\bfphi$ lie in $\bfPhi = \bbR^r$ for some dimension $r$.
        %
        \item $\bfPhi_{\bfphi_0} = \{\bfphi \in \bfPhi: \log p(\bfy;\bfphi) \geq \log p(\bfy;\bfphi_0)\}$ is compact for all $\bfphi_0$ if $\log p(\bfy;\bfphi_0) > -\infty$.
        %
        \item $\log p(\bfy;\bfphi)$ is differentiable in $\bfphi$ for all $\bfphi \in \bfPhi$.
        %
        \item $F_t(\bfphi \mid \bfgamma_t(\bfphi'), \bfxi_t(\bfphi'))$ is convex with respect to $\bfphi$ and $F(\bfphi \mid \bfgamma(\bfphi'), \bfxi(\bfphi'))$ is strongly convex with respect to $\bfphi$ for all $\bfphi'$ with constant $C > 0$. Namely, for all $\bfphi$, $\bfphi_0$ and $\bfphi'$:
        %
        \small
        \begin{equation}
            F_t(\bfphi \mid \bfgamma_t(\bfphi'), \bfxi_t(\bfphi')) \geq F_t(\bfphi_0 \mid \bfgamma_t(\bfphi'), \bfxi_t(\bfphi')) + \nabla F_t(\bfphi_0 \mid \bfgamma_t(\bfphi'), \bfxi_t(\bfphi'))^T(\bfphi-\bfphi_0),
        \end{equation}
        %
        \begin{equation}
            F(\bfphi \mid \bfgamma(\bfphi'), \bfxi(\bfphi')) \geq F(\bfphi_0 \mid \bfgamma(\bfphi'), \bfxi(\bfphi')) + \nabla F(\bfphi_0 \mid \bfgamma(\bfphi'), \bfxi(\bfphi'))^T(\bfphi-\bfphi_0) + \frac{C}{2} \|\bfphi - \bfphi_0\|_2^2.
        \end{equation}
        \normalsize
        %
        \item $F_t(\bfphi \mid \bfgamma_t(\bfphi'), \bfxi_t(\bfphi'))$ is uniformly Lipschitz-smooth with respect to $\bfphi$ for all $t$ and $\bfphi'$ with constant $L \geq C > 0$. Namely, for all $t$, $\bfphi$, $\bfphi_0$ and $\bfphi'$:
        %
        \small
        \begin{equation}
            F_t(\bfphi \mid \bfgamma_t(\bfphi'), \bfxi_t(\bfphi')) \leq F_t(\bfphi_0 \mid \bfgamma_t(\bfphi'), \bfxi_t(\bfphi')) + \nabla F_t(\bfphi_0 \mid \bfgamma_t(\bfphi'), \bfxi_t(\bfphi'))^T(\bfphi - \bfphi_0) + \frac{L}{2} \| \bfphi - \bfphi_0 \|_2^2.
        \end{equation}
        \normalsize
        %
        \item The step size $\lambda$ is sufficiently small and $M$ is sufficiently large such that
        %
        \begin{equation}
            \zeta = \frac{1}{C \lambda(1-2L\lambda)M} + \frac{2L\lambda}{1-2L\lambda} < 1.
        \end{equation}
        %
        \item $\nabla F_t(\bfphi \mid \bfgamma_t(\bfphi'), \bfxi_t(\bfphi'))$ is uniformly continuous in $(\bfphi,\bfphi')$ for all $t$.
    \end{enumerate}
    
    Further, let $\calS = \{\ell^*(k) < \infty \text{ for all } k \}$. Then, $\bbP\{\calS\} = 1$, and the following holds on $\calS$: all limit points of $\{\bfphi_{k}\}_{k=0}^\infty$  are stationary points of $\log p(\bfy;\cdot)$, and $\log p(\bfy;\bfphi_{k})$ converges monotonically to $\log p^* = \log p(\bfy;\bfphi^*)$ for some stationary point of $\log p(\bfy;\cdot)$, $\bfphi^*$.
\end{theorem}

Conditions 1--3 are from \citet{Wu:1983} and are standard assumptions needed to prove the convergence of the EM algorithm. Likewise, Conditions 4--6 are from \citet{Johnson:2013} and are standard assumptions used to prove common properties of stochastic optimization algorithms. Condition 7 is needed to prove that SVRG is a continuous mapping.

Condition 2 from \citet{Wu:1983} and condition 5 of \citet{Johnson:2013} can be restrictive and are often violated in common settings. For example, both are violated when estimating the variance of normal state-dependent distributions within an HMM. This issue is well-known for maximum likelihood estimation in mixture models \citep{Chen:2009,Liu:2015b}. It can be avoided by setting lower bounds on the variance components \citep{Zucchini:2016}. 

Condition 4 seems concerning at first because the log-likelihood of a hidden Markov model is usually multi-modal and non-convex. However, the convexity condition applies to $F_t$ rather than the log-likelihood itself. In addition, $F_t$ is a linear combination of probability densities, which are often convex in $\bftheta$ \citep{Boyd:2004}, and log-sum-exp functions, which are convex in $\bfnu$ and $\bfeta$. Even if $F_t$ is not convex, stochastic gradient methods can escape local optima within the M step more effectively than standard gradient descent \citep{Kleinberg:2018}.

Theorem 1 guarantees convergence only to a local optimum of the likelihood for version 2 of \texttt{EM-VRSO} with $P = \texttt{False}$ and $A = \text{SVRG}$. However, some studies suggest that EM algorithms like \texttt{EM-VRSO} may escape local optima of the likelihood faster than direct numerical optimization of the log-likelihood \citep{Zhang:2020}.
%
Unfortunately, the convergence analysis for \texttt{EM-VRSO} becomes more complex when $P = \texttt{True}$, as the E and M steps are intertwined. Nonetheless, Theorem 2 below demonstrates that stationary points of the log-likelihood function serve as fixed points of Algorithm \ref{alg:EM-VRSO} for all values of $P$ and $A$. The proof of Theorem 2 is provided in the online appendix.

\begin{theorem}
    If $\nabla \log p(\bfy;\bfphi_0) = 0$, then for all $\lambda \in \bbR$, $A \in \{\text{SAGA}, \text{SVRG}\}$, $P \in \{\texttt{True},\texttt{False}\}$, $M \in \bbN$, and $K \in \bbN$, $\texttt{EM-VRSO}(\bfphi_0, \lambda, A, P, M, K) = \bfphi_0$ with probability 1, where $\texttt{EM-VRSO}$ is defined in Algorithm \ref{alg:EM-VRSO}.
\end{theorem}

Theorem 2 is useful because it guarantees that Algorithm \ref{alg:EM-VRSO} does not change $\bfphi_0$ when it is a stationary point of the likelihood. However, it makes no guarantees that Algorithm \ref{alg:EM-VRSO} will converge if $\nabla \log p(\bfy;\bfphi_0) \neq 0$ and either $P = \texttt{True}$ or $A = \text{SAGA}$. Nonetheless, we see in our simulation and case studies that Algorithm \ref{alg:EM-VRSO} approaches local maxima of the log-likelihood function faster than existing full-batch baselines for all values of $P$ and $A$. For theoretical guarantees on convergence, practitioners can set $P = \texttt{True}$ or $A = \text{SAGA}$ for a predetermined number of iterations, followed by switching to $P = \texttt{False}$ and $A = \text{SVRG}$, or a full-gradient method such as BFGS \citep{Fletcher:2000}.