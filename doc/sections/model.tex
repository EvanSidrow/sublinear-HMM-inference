
If the length of the observations sequence $T$ is large, both the E- step and the M- step of the EM algorithm are expensive. The E-step is expensive because $\gamma_t$ and $\xi_t$ must be calculated for $t = 1,\ldots,T$ to define $F^{(k)}$ and $G^{(k)}$. The M-step is also expensive if closed-form solutions to (\ref{eqn:EM_update_theta}) and (\ref{eqn:EM_update_eta}) are not readily available. If closed-form solutions are not available, practitioners must maximize equations (\ref{eqn:EM_update_theta}) and (\ref{eqn:EM_update_eta}) numerically, which requires evaluating gradients of $T$ terms. We introduce an algorithm to address the expensive M- step in Section \ref{subsec:stoch_M}, and we extend this algorithm to address the expensive E- step in Section \ref{subsec:stoch_E}.

\subsection{(Variance-Reduced) Stochastic M Step}
\label{subsec:stoch_M}

To alleviate the expensive M-step, we propose using a variance-reduced stochastic optimization technique. To this end, we introduce Algorithm (\ref{alg:EM-SO}), which is a specific instance of the generalized EM algorithm \citep{Dempster:1977}. Under standard regularity conditions, Theorem 1 shows that version 2 of Algorithm (\ref{alg:EM-SO}) converges to a stationary point of the log-likelihood.

Although the M-step of Algorithm (\ref{alg:EM-SO}) updates parameters in sub-linear time, note that the end of algorithm (\ref{alg:EM-SO}) requires evaluating $\log p(\bfy;\theta_{k,M},\eta_{k,M})$, which takes $\calO(T)$ time. Nonetheless, if $\theta_{k,M} = \theta_{k+1}$ and $\eta_{k,M} = \eta_{k+1}$, then it is free to evaluate $\log p(\bfy;\theta_{k,M},\eta_{k,M})$ while initializing $F^{(k+1)}$ and $G^{(k+1)}$ since $\log p(\bfy;\theta_{k,M},\eta_{k,M}) = \sum_{i=1}^N \alpha_T^{(i)}(\theta_{k,M},\eta_{k,M})$.

This algorithm has two versions, and the second version involves the quantity $\zeta$, which is defined in condition (6) of Theorem 1. Version 1 is used in practice, but version 2 makes convergence analysis much easier and it is used to prove Theorem 1. Unlike version 1, version 2 of the algorithm requires that the likelihood \textit{strictly} increases by some threshold after running through each iteration of SVRG. This strict threshold relies on values that are usually not known in practice (e.g. $\zeta$ and $Q^*(\theta_k,\eta_k)$). Luckily, using a strict threshold to update the parameters is intuitively less desirable than simply updating the parameters whenever the $Q-$ function increases, which is what we do in practice.

\begin{algorithm}
\caption{EM algorithm with variance-reduced stochastic M- step}\label{alg:EM-SO}
\begin{algorithmic}[1]
\Require Initial parameters ($\theta_{0}$, $\eta_{0}$), step sizes ($\lambda_F$ and $\lambda_G$), algorithm (SVRG or SAGA), and iterations per update ($M$)
%
\State $k \gets 0$

%
\State Initialize $F^{(k)} = \frac{1}{T} \sum_t F_t^{(k)}$ and $G^{(k)} = \frac{1}{T} \sum_t G_t^{(k)}$ using (\ref{eqn:F}) and (\ref{eqn:G})
\Comment{E-step of EM algorithm}
%
\vspace{10pt}
%
\State Initialize gradients $\widehat \nabla_\theta F_t^{(k)} \gets \nabla_\theta F_t^{(k)} (\theta_k)$ and $\widehat \nabla_\eta G_t^{(k)} \gets \nabla_\eta G_t^{(k)} (\eta_k)$ for all $t$
%
\State $\theta_{k,0} \gets \theta_k$ and $\eta_{k,0} \gets \eta_k$
%
\For{$m = 0,1,\ldots,M-1$}:
    %
    \State Pick $t_{k,m} \in \{1,\ldots,T\}$ uniformly at random.
    \State
    \Comment{update parameters}
    \begin{gather}
        \theta_{k,m+1} = \theta_{k,m} - \lambda_F \left[\nabla_\theta F_{t_{k,m}}^{(k)}(\theta_{k,m}) - \widehat \nabla_\theta F_{t_{k,m}}^{(k)} + \frac{1}{T} \sum_{t=1}^T \widehat \nabla_\theta F^{(k)}_{t} \right] \\
        %
        \eta_{k,m+1} = \eta_{k,m} - \lambda_G \left[\nabla_\eta G_{t_{k,m}}^{(k)}(\eta_{k,m}) - \widehat \nabla_\eta G_{t_{k,m}}^{(k)} + \frac{1}{T} \sum_{t=1}^T \widehat \nabla_\eta G^{(k)}_{t} \right]
    \end{gather}
    %
    \If{using SAGA}:
        \Comment{update table at index $t_{k,m}$}
        \begin{gather}
            \widehat \nabla_\theta F_{t_{k,m}}^{(k)} \gets \nabla_\theta F_{t_{k,m}}^{(k)}(\theta_{k,m}) \\
            \widehat \nabla_\eta G_{t_{k,m}}^{(k)} \gets \nabla_\eta G_{t_{k,m}}^{(k)}(\eta_{k,m})
        \end{gather}
    \EndIf
    %
\EndFor
%
\vspace{10pt}
%
\If{using version 1 and $\log p(\bfy;\theta_{k,M},\eta_{k,M}) > \log p(\bfy;\theta_{k,0},\eta_{k,0})$}:
    \Comment{move to next iteration}
    \State $\theta_{k+1} \gets \theta_{k,M}, \quad \eta_{k+1} \gets \eta_{k,M}, \quad \text{and} \quad k \gets k+1$
\ElsIf{using version 2 and $Q^*(\theta_k,\eta_k) - Q\big(\theta_{k,M},\eta_{k,M} ~ \big| ~ \theta_k, \eta_k\big) > \frac{\zeta + 1}{2} \Big(Q^*(\theta_k,\eta_k) - Q \big(\theta_k,\eta_k ~ \big| ~ \theta_k, \eta_k\big) \Big)$}
    \State $\theta_{k+1} \gets \theta_{k,M}, \quad \eta_{k+1} \gets \eta_{k,M}, \quad \text{and} \quad k \gets k+1$
\EndIf
%
\vspace{10pt}
%
\State return to step 2
\end{algorithmic}
\end{algorithm}

%%%%%
    
\begin{theorem}

    Suppose that the conditions of Theorem 1 of \citet{Johnson:2013} hold for both $F^{(k)}$ and $G^{(k)}$. In particular:
    
    \begin{enumerate}
        \item For fixed $\theta$ and $\eta$, both $F(\theta,\theta',\eta')$ and $G(\eta,\theta',\eta')$ are continuous in $\theta'$ and $\eta'.$
        %
        \item $F_t(\theta,\theta',\eta')$ is uniformly Lipschitz-smooth with respect to $\theta$ for all $t$, $\theta'$ and $\eta'$ with constant $L_F > 0$. Namely, for all $t$, $\theta$, $\theta_0$, $\theta'$ and $\eta'$:
        %
        $$F_t(\theta, \theta', \eta') \leq F_t(\theta_0,\theta',  \eta') + \nabla_\theta F_t(\theta_0, \theta',  \eta')^T(\theta-\theta_0) + \frac{L_F}{2} ||\theta - \theta_0||^2.$$ 
        %
        \item $G_t(\eta,\theta',\eta')$ is uniformly Lipschitz-smooth with respect to $\eta$ for all $t$, $\theta'$ and $\eta'$ with constant $L_G > 0$. Namely, for all $t$, $\eta$, $\eta_0$, $\theta'$ and $\eta'$:
        %
        $$G_t(\eta, \theta',  \eta') \leq G_t(\eta_0,\theta',  \eta') + \nabla_\eta G_t(\eta_0,\theta',  \eta')^T(\eta-\eta_0) + \frac{L_G}{2} ||\eta - \eta_0||^2.$$
        %
        \item $F(\theta,\theta',\eta')$ is convex with respect to $\theta$ and $F(\theta,\theta',\eta')$ is strongly convex with respect to $\theta$ for all $\theta'$ and $\eta'$ with constant $C_F > 0$. Namely, for all $\theta$, $\theta_0$, $\theta'$ and $\eta'$:
        %
        $$F(\theta, \theta', \eta') \geq F(\theta_0,\theta',  \eta') + \nabla_\theta F(\theta_0, \theta',  \eta')^T(\theta-\theta_0) + \frac{C_F}{2} ||\theta - \theta_0||^2.$$ 
        %
        \item $G(\eta,\theta',\eta')$ is convex with respect to $\eta$ and $G(\eta,\theta',\eta')$ is strongly convex with respect to $\eta$ for all $\theta'$ and $\eta'$ with constant $C_G > 0$. Namely, for all $\eta$, $\eta_0$, $\theta'$ and $\eta'$:
        %
        $$G(\eta, \theta', \eta') \geq G(\theta_0,\theta',\eta') + \nabla_\theta G(\theta_0, \theta', \eta')^T (\eta-\eta_0) + \frac{C_G}{2} ||\eta - \eta_0||^2.$$ 
        %
        \item The step sizes $\lambda_F$ and $\lambda_G$ are sufficiently small and $M$ is sufficiently large such that 
        $$\zeta \equiv \max \left\{\frac{1}{C_F \lambda_F(1-2L_F\lambda_F)M} + \frac{2L_F\lambda_F}{1-(2L_F\lambda_F)}, \frac{1}{C_G \lambda_G(1-2L_G\lambda_G)M} + \frac{2L_G\lambda_G}{1-(2L_G\lambda_G)}\right\} < 1.$$
    \end{enumerate}

    In addition, suppose that assumptions of \citet{Wu:1983} hold:

    \begin{enumerate}
        \item The parameter space $\Theta$ (i.e. $\{\theta,\eta\} \in \Theta$) is a subset of $r$-dimensional Euclidean space $\bbR^r$ for some $r$.
        \item $\Theta_{\theta_0,\eta_0} = \{\{\theta,\eta\} \in \Theta: \log p(\bfy;\theta,\eta) \geq \log p(\bfy;\theta_0,\eta_0)\}$ is compact for any $\log p(\bfy;\theta_0,\eta_0) > -\infty$.
        \item $\log p(\bfy;\theta_0,\eta_0)$ is continuous in $\Theta$ and differentiable in the interior of $\Theta$.
    \end{enumerate}
    
    Then, all limit points of $\{(\theta_k,\eta_k)\}_{k=0}^\infty$ generated from version 2 of Algorithm (\ref{alg:EM-SO}) using SVRG are stationary points of $\log p(\bfy;\theta,\eta)$ and $\log p(\bfy;\theta_k,\eta_k)$ converges monotonically to $\log p^* = \log p(\bfy;\theta^*,\eta^*)$ for some stationary point of $\log p$, $(\theta^*,\eta^*)$.
\end{theorem}
%

%Each of these algorithms have advantages and disadvantages. SAG is the most intuitive of the three algorithms and corresponds to randomly updating one component of the gradient from the sums in Equations (\ref{eqn:F}) and (\ref{eqn:G}) before taking a gradient step. However, the gradient estimates are biased. The proof of convergence for SAG is also complicated.

%SVRG is convenient because it produces unbiased estimates of the gradient. In addition, it also does not rely on any values of $\phi_t$ or $\zeta_t$, so SVRG has a significantly lower storage cost compared to SAG and SAGA. In addition, formal analysis of SVRG is much easier than SAG due to the fact the gradients are unbiased and the table average does not change at every parameter update. However, SVRG involves two gradients evaluations at every parameter update rather than only one as in SAG and SAGA. In addition, it requires the entire gradient to be calculated each epoch.

%Finally, SAGA has the best theoretical guarantees of convergence rate of the three algorithms. Like SVRG, it also has unbiased gradient estimates. However, its advantages over SVRG are modest and it requires gradients to be stored for all $t = 1,\ldots,T$.

Conditions (1--6) from \citet{Johnson:2013} are standard assumptions used to prove common properties of stochastic optimization algorithms. Likewise, Conditions (1--3) from \citet{Wu:1983} are standard assumptions needed to prove the convergence of EM in general. 

Unfortunately, Condition (2) from \citet{Wu:1983} can be restrictive, and is often violated when estimating variance components in a mixture model. Namely, the likelihood of an HMM with Gaussian emissions is unbounded when estimating variance components. This violates condition (2) of \citet{Johnson:2013} as well. Unbounded likelihoods when estimating variance components is a well-known issue for maximum likelihood estimation in mixture models \citep{Chen:2009,Liu:2015b}. This issue can be avoided by either specifying an appropriate prior over the parameters $\theta$ and $\eta$, setting bounds on the variance components, or by jittering the parameters $\theta$ if it appears that the likelihood is ``blowing up" to infinity.

Theorem 1 is a convergence result for version 2 of Algorithm (\ref{alg:EM-SO}), which requires knowledge of the true optimum $Q^*(\theta_{k},\eta_{k})$ at each iteration $k$. However, it is intuitively clear that version 1 should be preferred in practice over version 2 since it updates the parameters $\{\theta,\eta\}$ \textit{whenever} those parameters increase the log-likelihood of the HMM.

%We briefly consider when each condition is satisfied.

%Condition (1) is satisfied so long as the emission densities $f(y_t;\theta^{(i)})$ and probability transition matrices $\Gamma(\eta)$ are continuous with respect to $\theta$ and $\eta$, respectively. The functions $F(\theta,\theta',\eta')$ and $G(\eta,\theta',\eta')$ are simply weighted sums of $\gamma(\theta',\eta')$ and $\xi(\theta',\eta')$ for fixed $\theta$ and $\eta$, and $\gamma$ and $\xi$ are calculated using repeated evaluation of $f(y_t;\theta^{(i)})$ and $\Gamma(\eta)$ (see Equations (\ref{eqn:gamma}) and (\ref{eqn:xi})).

%Condition (2) is satisfied if $\log f(y_t ; \theta^{(i)})$ is uniformly Lipschitz-smooth with respect to $\theta^{(i)}$ for all $y_t$, since $F_t$ is a weighted sum of $\log f(y_t ; \theta^{(i)})$ for $i = 1,\ldots,N$. Note that the log-density of a normal distribution is
%
%\begin{equation}
%    \log f_{norm}\left(y_t;\mu,\log(\sigma^2)\right) = -\frac{1}{2}(y_t-\mu)^2 e^{-\log(\sigma^2)} - \frac{1}{2} \log(\sigma^2),
%    \label{eqn:norm_log_like}
%\end{equation}
%
%which is Lipschitz smooth with respect to $\mu$ and $\log(\sigma^2)$ as long as $\log(\sigma^2)$ remains bounded from below. Unfortunately, estimating the variance of an HMM with normal emission distributions violates condition (2) since the second derivative of $\log f_{norm}$ with respect to $\log(\sigma^2)$ is unbounded as $\log(\sigma^2) \to -\infty$. However, in our case study and simulation study $\log(\sigma^2)$ remains bounded in practice.

%Condition (3) is usually satisfied because element $(i,j)$ of the log-transition probability matrix can be written as
%\begin{equation}
%    \log \Gamma^{(i,j)} = \eta^{(i,j)} - \log\left(\sum_{k=1}^N\exp\left(\eta^{(i,k)}\right)\right),
%\end{equation}
%which is Lipschitz-smooth. Further, $G_t$ is a weighted sum of the elements of $\log \Gamma (\eta)$, so it too must be Lipschitz-smooth. Similarly, $G_t$ is Lipschitz-smooth if $\log \Gamma (\eta)$ is parameterized using time-dependent covariates.

%Condition (4) is satisfied for Gaussian emission distributions where $\theta = \{\mu,\log(\sigma^2)\}$ so long as a strongly convex prior is placed on $\log(\sigma^2)$. This is because the log-likelihood of the normal distribution (see Equation (\ref{eqn:norm_log_like}) is convex (but not strongly convex) with respect to $\theta = \{\mu,\log(\sigma^2)\}$, and the function $F$ is a weighted sum of these log-densities. Adding a strongly-convex prior over $\theta$ ensures strong-convexity in the function $F$.

%Condition (5) is satisfied so long as a strongly convex prior is placed over $\eta$. This is because the negative log-sum-exp function is convex, but not strongly convex. This also holds if $\log \Gamma (\eta)$ is parameterized using time-dependent covariates, since the composition of two convex functions is again convex.

%Finally, Condition (6) can be satisfied by tuning the step size and iterations per M-step appropriately. See section (\label{sec:prac}) for more details about step-size selection.

Many of the drawbacks inherent to SVRG and SAGA are not as burdensome in the context of the EM algorithm. In particular, SVRG occasionally requires a full gradient evaluation, which is not desirable for a large data set. However, the E-step of an EM algorithm requires a full pass of the data set \textit{anyway}, so the additional burden of calculating a full gradient after each E-step is minimal. Likewise, SAGA involves storing gradient estimates at each data point $t$, which is storage-intensive. However, the EM algorithm also requires storing the weights $\gamma_t^{(i)}(\theta, \eta)$ and $\xi_t^{(i,j)}(\theta, \eta)$ as a part of the E-step, so storing gradient estimates in addition to these weights represents a minimal additional storage cost (depending upon the number of parameters in the model).

% The algorithm above applies even if the state-space of $\bfx$ is not discrete as long as it is possible to sample from $p(\bfx | \bfy ; \theta, \Gamma)$. \citet{Gu:1998} extend the algorithm above to apply even if it is not possible to sample from $p(\bfx | \bfy ; \theta, \Gamma)$ by drawing $\bfx$ from a Markov Chain with $p(\bfx | \bfy ; \theta, \Gamma)$ as its stationary distribution. \citet{Gu:1998} also extend this algorithm to general incomplete data models and prove that such an algorithm converges almost surely (under certain regularity conditions).

%\subsection{Expanded view of EM}

\subsection{Stochastic E Step}
\label{subsec:stoch_E}

While algorithm (\ref{alg:EM-SO}) above helps with the expensive M-step of the Baum-Welch algorithm, the E-step still has a time complexity of $\calO(T)$, which can again be computationally burdensome for large $T$. 
To this end, \citet{Neal:1998} justify a \textit{partial} E-step within the EM algorithm. For HMMs, a partial E-step involves updating only a subset of the weights $\{\gamma_t,\xi_t\}_{t=1}^T$ as defined in Equations (\ref{eqn:gamma}) and (\ref{eqn:xi}) at each iteration of the EM algorithm. 
Using this intuition, we propose to combine the partial E-step of \citet{Neal:1998} with the partial M-step from the previous section. For example, if the parameters $\{\theta, \eta\}$ are updated using a gradient estimate based on a random time index $t_{k,m}$, it is natural to update $G_{t_{k,m}}^{(k)}$ and $F_{t_{k,m}}^{(k)}$ via $\xi_{t_{k,m}}$ and $\gamma_{t_{k,m}}$ at the same time. As such, we introduce the notation $F^{(k,m)}$ and $G^{(k,m)}$ to denote the current objective function at iteration $k$ through the EM algorithm and step $m$ of the inner-loop of the M step. We describe this full procedure in Algorithm (\ref{alg:P-EM-SO}).

%show that the EM algorithm can be thought of as maximizing some auxiliary function $H$ with respect to both the parameters $\{\eta,\theta\}$ as well as some auxiliary distribution $\tilde p (\bf X; \gamma; \xi)$ with respect to the parameters $\gamma$ and $\xi$. In this context, $\gamma$ and $\xi$ are not functions of the parameters $\{\eta,\theta\}$, but instead parameters that define the auxiliary distribution $\tilde p$. However, note that in order for $\tilde p$ to be a valid probability distribution, $\gamma_t$ and $\xi_t$ must be consistent with one another. Therefore, if $\gamma$ and $\xi$ are allowed to vary independently from one another, $\tilde p$ will not be valid. Nonetheless, we can use the intuition from \citet{Neal:1998} to mix the E- and the M- steps of the EM algorithm. 

\begin{algorithm}
\caption{EM algorithm with variance-reduced stochastic M- step and stochastic E- step}\label{alg:P-EM-SO}
\begin{algorithmic}[1]
\Require Initial parameters ($\theta_{0}$, $\eta_{0}$), step sizes ($\lambda_F$ and $\lambda_G$), algorithm (SVRG or SAGA), and iterations per update ($M$)
%
\State $k \gets 0$

%
\State Initialize $F^{(k,0)} = \frac{1}{T} \sum_t F_t^{(k,0)}$ and $G^{(k,0)} = \frac{1}{T} \sum_t G_t^{(k,0)}$ using (\ref{eqn:F}) and (\ref{eqn:G})
\Comment{E-step of EM algorithm}
%
\vspace{10pt}
%
\State $\theta_{k,0} \gets \theta_k$ and $\eta_{k,0} \gets \eta_k$
%
\State Initialize gradient estimates $\widehat \nabla_\theta F_t^{(k)} \gets \nabla_\theta F_t^{(k,0)} (\theta_{k,0})$ and $\widehat \nabla_\eta G_t^{(k)} \gets \nabla_\eta G_t^{(k,0)} (\eta_{k,0})$ for all $t$
%
\For{$m = 0,1,\ldots,M-1$}:
    %
    \State Pick $t_{k,m} \in \{1,\ldots,T\}$ uniformly at random.
    %
    \State Update $\gamma_{t_{k,m}}$ and $\xi_{t_{k,m}}$ via $\alpha_{t_{k,m}}$ and $\beta_{t_{k,m}}$ using Equations (\ref{eqn:alpha}) -- (\ref{eqn:xi}).
    %
    \State Define $F^{(k,m+1)}$ and $G^{(k,m+1)}$ using Equations (\ref{eqn:F}) and (\ref{eqn:G}) with $\xi_{t_{k,m}}$ and $\gamma_{t_{k,m}}$.
    %
    \State \Comment{update parameters}
    %
    \begin{gather}
        \theta_{k,m+1} \gets \theta_{k,m} - \lambda_F \left[\nabla_\theta F_{t_{k,m}}^{(k,m+1)}(\theta_{k,m}) - \widehat \nabla_\theta F_{t_{k,m}}^{(k)} + \frac{1}{T} \sum_{t=1}^T \widehat \nabla_\theta F^{(k)}_{t} \right] \\
        %
        \eta_{k,m+1} \gets \eta_{k,m} - \lambda_G \left[\nabla_\eta G_{t_{k,m}}^{(k,m+1)}(\eta_{k,m}) - \widehat \nabla_\eta G_{t_{k,m}}^{(k)} + \frac{1}{T} \sum_{t=1}^T \widehat \nabla_\eta G^{(k)}_{t} \right]
    \end{gather}
    %
    \If{using SAGA}:
        \Comment{update table at index $t_{k,m}$}
        \begin{gather}
            \widehat \nabla_\theta F_{t_{k,m}}^{(k)} \gets \nabla_\theta F_{t_{k,m}}^{(k,m+1)}(\theta_{k,m}) \\
            \widehat \nabla_\eta G_{t_{k,m}}^{(k)} \gets \nabla_\eta G_{t_{k,m}}^{(k,m+1)}(\eta_{k,m})
        \end{gather}
    \EndIf
    %
\EndFor
%
\vspace{10pt}
%
\If{$\log p(\bfy;\theta_{k,M},\eta_{k,M}) > \log p(\bfy;\theta_{k,0},\eta_{k,0})$}:
    \Comment{move to next iteration}
    \State $\theta_{k+1} \gets \theta_{k,M}, \quad \eta_{k+1} \gets \eta_{k,M}, \quad \text{and} \quad k \gets k+1$
\EndIf
%
\vspace{10pt}
%
\State return to step 2
\end{algorithmic}
\end{algorithm}

Algorithm (\ref{alg:P-EM-SO}) still requires $\calO(T)$ operations to perform a \textit{full} E-step, but a \textit{partial} E-step is taken within each parameter update of the M-step of Algorithm (\ref{alg:P-EM-SO}). As a result, each parameter update and partial E-step takes no more than $\calO(1)$ time. This can be especially useful for early iterations of the EM algorithm, when the weights $\{\gamma_t,\xi_t\}_{t=1}^T$ are poor approximations of the conditional probabilities that they represent.

%After completing the E- and the M- step, algorithm (\ref{alg:P-EM-SO}) requires evaluating $\log p(\bfy;\theta_{k,M},\eta_{k,M})$, which has a time complexity of $\calO(T)$. However, $F_t^{(k+1,0)}$ depends upon $\alpha_T(\theta_{k+1},\eta_{k+1})$, and $p(\bfy;\theta_{k+1},\eta_{k+1}) = \sum_{i=1}^N \alpha_T^{(i)}(\theta_{k+1},\eta_{k+1})$. Therefore, if in fact $\theta_{k,M},\eta_{k,M} = \theta_{k+1},\eta_{k+1}$, then evaluating $\log p(\bfy;\theta_{k,M},\eta_{k,M})$ is trivial after initializing $F_t^{(k+1,0)}$ and $G_t^{(k+1,0)}$. 

Unfortunately, convergence analysis for algorithm (\ref{alg:P-EM-SO}) is much more complicated than for version 2 of algorithm (\ref{alg:EM-SO}) since the E- and M- steps of the EM algorithm are mixed together. If practitioners desire theoretic guarantees for convergence, we recommend running algorithm (\ref{alg:P-EM-SO}) for a predetermined number of iterations, and then switching to either algorithm (\ref{alg:EM-SO}) or a full-gradient method such as BFGS \citep{Fletcher:2000}.

%One option for convergence analysis involves showing that this is the limiting case of an SMC algorithm as the number of particles goes to infinity. SVRG and SAGA both produce unbiased gradient estimates conditioned on these particles. This is similar to the proof of \citet{Naesseth:2020} for Markovian score climbing.

%Note that if the observed data is independent, then it is straightforward to apply variance-reduced stochastic gradient descent to the log-likelihood, since the log-likelihood of each data point contributes one term to a sum that makes up the log-likelihood. However, the log-likelihood of an HMM cannot be written as a tractable sum, so stochastic gradient descent is not feasible for the raw likelihood. 

%The algorithm above is equivalent to standard variance-reduced stochastic gradient descent algorithms for independent data. This is because updating $\xi_{t_{k,m}}$ and $\gamma_{t_{k,m}}$, followed by $\nabla_{\theta} F_{t_{k,m}}(\theta;\xi_{t_{k,m}},\gamma_{t_{k,m}})$ and $\nabla_{\eta} G_{t_{k,m}}(\eta;\xi_{t_{k,m}},\gamma_{t_{k,m}})$ before taking a gradient step is equivalent to simply evaluating the gradient at data point $t_{k,m}$ for independent data by the Fisher identity for the gradient.

If practitioners use SVRG within algorithm (\ref{alg:P-EM-SO}), then they must save both an outdated set of weights $\big\{\gamma_t(\theta_{k,0},\eta_{k,0}),\xi_t(\theta_{k,0},\eta_{k,0})\big\}_{t=1}^T$ as well as a current set of weights $\big\{ \gamma_t,\xi_t \big\}_{t=1}^T$ . This is because each gradient estimate depends upon $\widehat \nabla_\theta F_{t_{k,m}}^{(k,0)}$ and $\widehat \nabla_\eta G_{t_{k,m}}^{(k,0)}$, each of which in turn depend upon the outdated weights $\{\gamma_t(\theta_{k,0},\eta_{k,0}),\xi_t(\theta_{k,0},\eta_{k,0})\}_{t=1}^T$. Likewise, each gradient estimate also depends upon $\widehat \nabla_\theta F_{t_{k,m}}^{(k,m+1)}$ and $\widehat \nabla_\eta G_{t_{k,m}}^{(k,m+1)}$, each of which depend upon the current weights $\big\{ \gamma_t,\xi_t \big\}_{t=1}^T$.

Also, when SAGA within algorithm (\ref{alg:P-EM-SO}), one may set $M = \infty$ and never fully refresh the gradient. In this case it is difficult to determine when the algorithm has converged since the full log-likelihood $\log p(\bfy;\theta_{k,M},\eta_{k,M})$ is never explicitly evaluated. As such, setting $M \approx 10T$ adds minimal computational burden compared to $M = \infty$ but periodically evaluates the full likelihood. We use this strategy and set $M = 10T$ for one of our experimental studies.

%Note that there is a problem for SVRG when changing the weights $\gamma$ and $\xi$ as we go. In particular, note that we have to re-evaluate the gradients at the old parameters to get unbiased estimates of the gradient. However, if the weights are changing as we do the M step, then we have to re-evaluate the old weights to do SVRG. BUT, notice that calculating those weights requires that we either store them or iterate through the whole data set :(. We could update the table average as we update the weights, but then we would have to know the OLD value of those weights to update the full gradient effectively. The only real saving grace we have here is that if we have the new weights, then saving the old weights is not as bad a saving the old gradients, which we would have to do for SAGA.