\subsection{Stochastic Optimization in the EM algorithm}

We now have the background necessary to construct a variance-reduced optimization technique for HMMs. The M-step of the Baum-Welch algorithm can be written as a minimization problem, where the objective function is a sum of $T$ terms. In particular, $$\theta[k+1] = \argmin_{\theta} F^{(k)}(\theta), \qquad \eta[k+1] = \argmin_{\eta} G^{(k)}(\eta),$$ where:

\begin{align}
    F^{(k)}(\theta) &= \sum_{t=1}^T F_t^{(k)}(\theta), \qquad F_t^{(k)}(\theta) = - \sum_{i=1}^N \gamma^{(i)}_t(\theta[k], \eta[k]) \log f^{(i)}(y_t;\theta) \label{eqn:F} \\ \nonumber \\
    %
    G^{(k)}(\eta) &= \sum_{t=1}^{T} G_t^{(k)}(\eta), 
    \qquad G_t^{(k)}(\eta) = 
    \begin{cases}
        - \sum_{i=1}^N \gamma^{(i)}_t(\theta[k],\eta[k]) \log \delta^{(i)}(\eta), & t = 1, \\\\
        - \sum_{i=1}^N \sum_{j=1}^N \xi^{(i,j)}_t(\theta[k], \eta[k]) \log \Gamma^{(i,j)}(\eta), & t \geq 2
    \end{cases}
    \label{eqn:G}
\end{align}

%The Robbins-Monro algorithm above can be seen as adding randomness to the E- step of the EM algorithm, and then taking one gradient step in the M-step using the noisy Q-function. This algorithm is useful if the size of the state-space $N$ is very large (or infinite), since it is infeasible to exactly calculate $\gamma$ and $\xi$ (i.e. perform the E- step of the EM algorithm) in those cases. 

Note that $F_t^{(k)}$ and $G_t^{(k)}$ are defined such that the M-step of the EM algorithm is a minimization problem instead of a maximization problem. 

If the length of the observations sequence $T$ is very large, %it is expensive to draw samples from $\bfx \sim p(\bfx | \bfy ; \hat \theta_k, \hat \Gamma_k)$ \textit{and} evaluate the gradient $\nabla_{\theta,\eta} \log p(\bfy,\bfx;\theta,\eta)$, even if $\bfx$ is drawn using an MCMC technique. In other words, 
both the E- step and the M- step of the EM algorithm are expensive. The E-step is expensive because $\gamma_t$ must be calculated for $t = 1,\ldots,T$, while the M-step is expensive because numerical maximization of equations (\ref{eqn:EM_update_theta}) and (\ref{eqn:EM_update_Gamma}) requires taking a gradient of $T$ terms.

To help with the expensive E-step, \citet{Neal:1998} describe a partial E-step in the EM algorithm, where $\gamma_t$ and $\xi_t$ are updated only for a subset of $t \in \{1,\ldots,T\}$ before updating the parameters $\eta$ and $\theta$ at each step. To help with the expensive M-step, we can apply variance-reduced stochastic optimization techniques. To this end, we introduce the following EM-algorithm with a variance-reduced stochastic M-step:

\begin{enumerate}
    \item Initialize $\theta[0]$, $\eta[0]$, and $k \leftarrow 0$.
    \item Define $F_t^{(k)}$ and $G_t^{(k)}$ for $t = 1, \ldots, T$ according to the equations on the right in (\ref{eqn:F}) and (\ref{eqn:G}). This is equivalent to performing the E-step of the EM algorithm.
    %
    \item Initialize a table of gradient estimates $\widehat \nabla_\theta F_t^{(k)}$ and $\widehat \nabla_\eta G_t^{(k)}$ for each $t = 1,\ldots,T$:
    \begin{itemize}
        \item $\widehat \nabla_\theta F_{t}^{(k)} \leftarrow \nabla_\theta F_{t}^{(k)}(\theta[k])$ for $t = 1,\ldots,T$,
        %
        \item $\widehat \nabla_\eta G_{t}^{(k)} \leftarrow \nabla_\eta G_{t}^{(k)}(\theta[k])$ for $t = 1,\ldots,T$.
    \end{itemize}
    %
    \item Initialize $\theta[k,0] \leftarrow \theta[k]$ and $\eta[k,0] \leftarrow \eta[k]$.
    %
    \item For $m = 0,\ldots,M-1$:
    \begin{enumerate}
        \item Pick $t_m \in \{1,\ldots,T\}$ uniformly at random.
        %
        \item Calculate $\theta[k,m+1]$ and $\eta[k,m+1]$ depending upon the algorithm:
        \begin{enumerate}
            \item If using SAG:
            \begin{gather}
                \theta[k,m+1] = \theta[k,m] - \lambda^{\theta} \left[\frac{\nabla_\theta F_{t_m}^{(k)}(\theta[k,m]) - \widehat \nabla_\theta F_{t_m}^{(k)}}{T} + \frac{1}{T} \sum_{t=1}^T \widehat \nabla_\theta F^{(k)}_{t} \right] \\
                %
                \eta[k,m+1] = \eta[k,m] - \lambda^{\eta} \left[\frac{\nabla_\eta G_{t_m}^{(k)}(\eta[k,m]) - \widehat \nabla_\eta G_{t_m}^{(k)}}{T} + \frac{1}{T} \sum_{t=1}^T \widehat \nabla_\eta G^{(k)}_{t} \right]
            \end{gather}
            \item If using SVRG or SAGA:
            \begin{gather}
                \theta[k,m+1] = \theta[k,m] - \lambda^{\theta} \left[\nabla_\theta F_{t_m}^{(k)}(\theta[k,m]) - \widehat \nabla_\theta F_{t_m}^{(k)} + \frac{1}{T} \sum_{t=1}^T \widehat \nabla_\theta F^{(k)}_{t} \right] \\
                %
                \eta[k,m+1] = \eta[k,m] - \lambda^{\eta} \left[\nabla_\eta G_{t_m}^{(k)}(\eta[k,m]) - \widehat \nabla_\eta G_{t_m}^{(k)} + \frac{1}{T} \sum_{t=1}^T \widehat \nabla_\eta G^{(k)}_{t} \right]
            \end{gather}
        \end{enumerate}
        %
        \item If using SAG or SAGA, update the gradients at location $t_m$ in the table:
        \begin{itemize}
            \item $\widehat \nabla_\theta F_{t_m}^{(k)} \leftarrow \nabla_\theta F_{t_m}^{(k)}(\theta[k,m])$,
            %
            \item $\widehat \nabla_\eta G_{t_m}^{(k)} \leftarrow \nabla_\eta G_{t_m}^{(k)}(\eta[k,m])$.
        \end{itemize}
        This updates table average results to create a better update rule in step 5(b).
    \end{enumerate}
    %
    \item If $F^{(k)}(\theta[k,M]) < F^{(k)}(\theta[k])$, set $\theta[k+1] = \theta[k,M]$. Otherwise, set $\theta[k+1] = \theta[k]$.
    %
    \item If $G^{(k)}(\eta[k,M]) < G^{(k)}(\eta[k])$, set $\eta[k+1] = \eta[k,M]$. Otherwise, set $\eta[k+1] = \eta[k]$
    %
    \item Set $k \leftarrow k+1$ and return to step 2.
\end{enumerate}

The algorithm above is a specific instance of the Generalized EM algorithm \citep{Dempster:1977}. More formally:

\begin{lemma}
    Suppose that the conditions of Theorem 1 of \citet{Johnson:2013} hold for both $F^{(k)}$ and $G^{(k)}$. In particular:
    \begin{enumerate}
        \item $F_t^{(k)}$ and $G_t^{(k)}$ are Lipschitz-smooth with constants $L_\theta > 0$ or $L_\eta > 0$, respectively: $$F_t^{(k)}(\theta) \leq F_t^{(k)}(\theta') + \nabla_\theta F_t^{(k)}(\theta')^T(\theta-\theta') + \frac{L_\theta}{2} ||\theta - \theta'||,$$ $$G_t^{(k)}(\eta) \leq G_t^{(k)}(\eta') + \nabla_\eta G_t^{(k)}(\eta')^T(\eta-\eta') + \frac{L_\eta}{2} ||\eta - \eta'||,$$
        \item $F_t^{(k)}$ and $G_t^{(k)}$ are $\gamma_\theta$- or $\gamma_\eta$- strongly convex:
        $$F_t^{(k)}(\theta) \geq F_t^{(k)}(\theta') + \nabla_\theta F_t^{(k)}(\theta')^T(\theta-\theta') + \frac{\gamma_\theta}{2} ||\theta - \theta'||^2,$$ $$G_t^{(k)}(\eta) \geq G_t^{(k)}(\eta') + \nabla_\eta G_t^{(k)}(\eta')^T(\eta-\eta') + \frac{\gamma_\eta}{2} ||\eta - \eta'||^2,$$
        \item The step sizes $\lambda^{\theta}$ and $\lambda^{\eta}$ are set such that $$\alpha^{\theta} = \frac{1}{\gamma^{\theta} \lambda^{\theta}(1-2L^{\theta}\lambda^{\theta})M} + \frac{2L^{\theta}\lambda^{\theta}}{1-(2L^{\theta}\lambda^{\theta})} < 1,$$
        $$\alpha^{\eta} = \frac{1}{\gamma^{\eta} \lambda^{\eta}(1-2L^{\eta}\lambda^{\eta})M} + \frac{2L^{\eta}\lambda^{\eta}}{1-(2L^{\eta}\lambda^{\eta})} < 1.$$
    \end{enumerate}
    Then, algorithm (1) is a specific instance of a Generalized EM algorithm and the likelihood $\log p(\bfy ; \theta[k+1],\eta[k+1])$ convergences to some $\log p^*$.
\end{lemma}
%
\begin{proof}
First, note that iterating through steps (2) - (7) of algorithm (1) above is equivalent to repeatedly performing \textit{independent} iterations of SVRG until either $F^{(k)}(\theta[k,M]) < F^{(k)}(\theta[k])$ or $G^{(k)}(\eta[k,M]) < G^{(k)}(\eta[k])$. This implies that algorithm (1) is a generalized EM algorithm, so $\log p(\bfy ; \theta[k],\eta[k])$ convergences to some $\log p^*$ as $k \to \infty$ by Theorem 1 of \citep{Wu:1983}. All that is left to prove is that either $F$ or $G$ will decrease in a finite number of iterations through steps (2) - (8).

To this end, let $\theta^*[k+1] = \argmin_\theta F^{(k)}(\theta)$ and $\eta^*[k+1] = \argmin_\eta G^{(k)}(\eta)$. Further, treat the value of $\theta[k]$ and $\eta[k]$ as fixed. If conditions (1) - (3) above hold for both $F^{(k)}$ and $G^{(k)}$ for all $k$, then SVRG attains geometric convergence by Theorem 1 of \citet{Johnson:2013}. In particular, after one iteration through steps (2) - (8): 
%
\begin{align*}
    \bbE\left[F^{(k)}(\theta[k,M]) - F^{(k)}(\theta^*[k+1]) ~\Big\vert~ \theta[k]\right] \leq \alpha^{\theta} \left(F^{(k)}(\theta[k]) - F^{(k)}(\theta^*[k+1])\right), \\
    %
    \bbE\left[G^{(k)}(\eta[k,M]) - G^{(k)}(\eta^*[k+1]) ~\Big\vert~ \eta[k] \right] \leq \alpha^{\eta} \left(G^{(k)}(\eta[k]) - G^{(k)}(\eta^*[k+1])\right)
\end{align*}
%
for some $\alpha^{\theta}, \alpha^{\eta} < 1$ (see Theorem 1 in \citet{Johnson:2013} for more details). Using Markov's inequality, we have:
\begin{align*}
    \bbP\left[F^{(k)}(\theta[k,M]) < F^{(k)}(\theta[k]) ~\Big\vert~ \theta[k]\right] \geq 1-\alpha^{\theta}, \\
    %
    \bbP\left[G^{(k)}(\eta[k,M]) < G^{(k)}(\eta[k]) ~\Big\vert~ \eta[k]\right] \geq 1-\alpha^{\eta}.
\end{align*}
Further, note that each iteration through steps (2) - (8) of algorithm (1) is independent of the previous iteration if both start from the same value of $(\theta[k],\eta[k])$. Therefore, with probability 1, the M-step of the algorithm above will terminate in finite time with parameters that decrease either $F^{(k)}$ or $G^{(k)}$ (or both) and therefore increase the log-likelihood. This indicates that algorithm (1) is a generalized EM algorithm, and that each M- step of the EM algorithm terminates in a finite number of iterations through steps (2)-(8). This completes the proof.
\end{proof}

%Each of these algorithms have advantages and disadvantages. SAG is the most intuitive of the three algorithms and corresponds to randomly updating one component of the gradient from the sums in Equations (\ref{eqn:F}) and (\ref{eqn:G}) before taking a gradient step. However, the gradient estimates are biased. The proof of convergence for SAG is also complicated.

%SVRG is convenient because it produces unbiased estimates of the gradient. In addition, it also does not rely on any values of $\phi_t$ or $\zeta_t$, so SVRG has a significantly lower storage cost compared to SAG and SAGA. In addition, formal analysis of SVRG is much easier than SAG due to the fact the gradients are unbiased and the table average does not change at every parameter update. However, SVRG involves two gradients evaluations at every parameter update rather than only one as in SAG and SAGA. In addition, it requires the entire gradient to be calculated each epoch.

%Finally, SAGA has the best theoretical guarantees of convergence rate of the three algorithms. Like SVRG, it also has unbiased gradient estimates. However, its advantages over SVRG are modest and it requires gradients to be stored for all $t = 1,\ldots,T$.

Unfortunately, conditions (1)-(3) are often not satisfied when estimating the variance of the emission distribution of a hidden Markov model with normal emission densities. This is because the Lipschitz constant $L_\theta$ goes to infinity as the variance $\sigma^2$ goes to zero. In addition, the likelihood $\log p(\bfy ; \theta, \eta)$ is unbounded. This is a well-known issue for maximum likelihood estimation in mixture models \citep{Chen:2009,Liu:2015b}.

Interestingly, many drawbacks of SVRG and SAGA disappear when used within an EM algorithm. In particular, SVRG occasionally requires a full gradient evaluation, which is not desirable for a large data set. However, the E-step of an EM algorithm requires a full pass of the data set \textit{anyway}, so the additional burden of calculating a full gradient after each E-step is minimal. Likewise, SAGA involves storing gradient estimates at each data point $t$, which is storage-intensive. However, the EM algorithm also requires storing the weights $\gamma_t^{(i)}(\theta, \eta)$ and $\xi_t^{(i,j)}(\theta, \eta)$ as a part of the E-step, so storing gradient estimates in addition to these weights can be a minimal additional burden (depending upon the number of parameters).

%The algorithm above applies even if the state-space of $\bfx$ is not discrete as long as it is possible to sample from $p(\bfx | \bfy ; \theta, \Gamma)$. \citet{Gu:1998} extend the algorithm above to apply even if it is not possible to sample from $p(\bfx | \bfy ; \theta, \Gamma)$ by drawing $\bfx$ from a Markov Chain with $p(\bfx | \bfy ; \theta, \Gamma)$ as its stationary distribution. \citet{Gu:1998} also extend this algorithm to general incomplete data models and prove that such an algorithm converges almost surely (under certain regularity conditions).

%\subsection{Expanded view of EM}

\subsection{Mixing E- and M- steps}

While algorithm (1) above is useful to help with the expensive M-step of the EM algorithm, the E-step of the EM (step (2)) still takes $\calO(T)$ time to complete, which can again be computationally burdensome for large $T$. To this end, \citet{Neal:1998} justify a \textit{partial} E-step within the EM algorithm. For HMMs, a partial E-step involves updating only a subset of the weights $\{\gamma_t,\xi_t\}_{t=1}^T$ as defined in Equations (\ref{eqn:F}) and (\ref{eqn:G}) before performing the M-step of the EM-algorithm. \citet{Neal:1998} assume that the maximization problem of the M-step has a closed form solution, but we consider problems with more difficult maximization problems within the M-step. 
%The practicality of this algorithm relies on the fact that the M-step of the EM algorithm is computationally negligible compared to the E-step. However, in our case we assume that the M-step involves repeated iterations of some variance-reduced stochastic gradient descent algorithm such as SVRG, SAGA or SAG. 
Therefore, it may be advantageous to \textit{combine} the partial E-step of \citet{Neal:1998} with the partial M-step in which some small number of gradient steps are performed. For example, if the parameters $\{\theta, \eta\}$ are updated using a gradient estimate based on a random time index $t_m$, it is natural to update $G_{t_m}^{(k)}$ and $F_{t_m}^{(k)}$ via $\xi_{t_m}$ and $\gamma_{t_m}$ at the same time. Such an algorithm looks as follows:

%show that the EM algorithm can be thought of as maximizing some auxiliary function $H$ with respect to both the parameters $\{\eta,\theta\}$ as well as some auxiliary distribution $\tilde p (\bf X; \gamma; \xi)$ with respect to the parameters $\gamma$ and $\xi$. In this context, $\gamma$ and $\xi$ are not functions of the parameters $\{\eta,\theta\}$, but instead parameters that define the auxiliary distribution $\tilde p$. However, note that in order for $\tilde p$ to be a valid probability distribution, $\gamma_t$ and $\xi_t$ must be consistent with one another. Therefore, if $\gamma$ and $\xi$ are allowed to vary independently from one another, $\tilde p$ will not be valid. Nonetheless, we can use the intuition from \citet{Neal:1998} to mix the E- and the M- steps of the EM algorithm. 

\begin{enumerate}
    \item Initialize $k \leftarrow 0$ and $\theta[k,0]$, $\eta[k,0]$.
    \item Define $F_t^{(k,0)}$ and $G_t^{(k,0)}$ for $t = 1, \ldots, T$ according to equations (\ref{eqn:F}) and (\ref{eqn:G}).
    %
    \item Initialize a table of gradient estimates $\widehat \nabla_\theta F_t^{(k,0)}$ and $\widehat \nabla_\eta G_t^{(k,0)}$ for each $t = 1,\ldots,T$:
    \begin{itemize}
        \item $\widehat \nabla_\theta F_{t_m}^{(k,0)} \leftarrow \nabla_\theta F_{t}^{(k,0)}(\theta[k,0])$ for $t = 1,\ldots,T$,
        %
        \item $\widehat \nabla_\eta G_{t_m}^{(k,0)} \leftarrow \nabla_\eta G_{t}^{(k,0)}(\theta[k,0])$ for $t = 1,\ldots,T$.
    \end{itemize}
    %
    \item For $m = 0,\ldots,M-1$:
    \begin{enumerate}
        \item Pick $t_m \in \{1,\ldots,T\}$ uniformly at random.
        %
        \item Calculate $F^{(k,m+1)}$ and $G^{(k,m+1)}$ by updating $\gamma_{t_m}$ and $\xi_{t_m}$ at index $t_m$:
        \begin{itemize}
            \item $\alpha^{(i)}_{t_m} \leftarrow \sum_j \alpha_{t_m-1}^{(j)} \Gamma^{(j,i)}[k,m] f^{(i)}(y_t;\theta[k,m])$
            \item $\beta^{(i)}_{t_m} \leftarrow \sum_j \Gamma^{(i,j)}[k,m] f^{(j)}(y_{t+1};\theta[k,m]) \beta_{t_m+1}^{(j)}$
            \item Update $\gamma_{t_m}$ according to Equation (\ref{eqn:gamma}).
            \item Update $\xi_{t_m}$ according to Equation (\ref{eqn:xi}).
            \item Define $F^{(k,m+1)}$ and $G^{(k,m+1)}$ using Equations (\ref{eqn:F}) and (\ref{eqn:G}) and the new values of $\xi_{t_m}$ and $\gamma_{t_m}$.
        \end{itemize}
        %
        \item Calculate $\theta[k,m+1]$ and $\eta[k,m+1]$ depending upon the algorithm:
        \begin{enumerate}
            \item If using SAG:
            \begin{gather}
                \theta[k,m+1] = \theta[k,m] - \lambda^{\theta} \left[\frac{\nabla_\theta F_{t_m}^{(k,m+1)}(\theta[k,m]) - \widehat \nabla_\theta F_{t_m}^{(k,m)}}{T} + \frac{1}{T} \sum_{t=1}^T \widehat \nabla_\theta F^{(k,m)}_{t} \right] \\
                %
                \eta[k,m+1] = \eta[k,m] - \lambda^{\eta} \left[\frac{\nabla_\eta G_{t_m}^{(k,m+1)}(\eta[k,m]) - \widehat \nabla_\eta G_{t_m}^{(k,m)}}{T} + \frac{1}{T} \sum_{t=1}^T \widehat \nabla_\eta G^{(k,m)}_{t} \right]
            \end{gather}
            \item If using SVRG or SAGA:
            \begin{gather}
                \theta[k,m+1] = \theta[k,m] - \lambda^{\theta} \left[\nabla_\theta F_{t_m}^{(k,m+1)}(\theta[k,m]) - \widehat \nabla_\theta F_{t_m}^{(k,m)} + \frac{1}{T} \sum_{t=1}^T \widehat \nabla_\theta F^{(k,m)}_{t} \right] \\
                %
                \eta[k,m+1] = \eta[k,m] - \lambda^{\eta} \left[\nabla_\eta G_{t_m}^{(k,m+1)}(\eta[k,m]) - \widehat \nabla_\eta G_{t_m}^{(k,m)} + \frac{1}{T} \sum_{t=1}^T \widehat \nabla_\eta G^{(k,m)}_{t} \right]
            \end{gather}
        \end{enumerate}
        %
        \item If using SAG or SAGA, update the gradients at location $t_m$ in the table:
        \begin{itemize}
            \item $\widehat \nabla_\theta F_{t_m}^{(k,m+1)} \leftarrow \nabla_\theta F_{t_m}^{(k,m+1)}(\theta[k,m])$,
            %
            \item $\widehat \nabla_\eta G_{t_m}^{(k,m+1)} \leftarrow \nabla_\eta G_{t_m}^{(k,m+1)}(\eta[k,m])$.
        \end{itemize}
        This updates table average results to create a better update rule in step 4(c).
        \item Leave all other gradient estimates unchanged, so for all $t = 1,\ldots,T$:
        \begin{itemize}
            \item $\widehat \nabla_\theta F_{t}^{(k,m+1)} \leftarrow \widehat \nabla_\theta F_{t}^{(k,m)}$ for all $t \neq t_m$ (or all $t$ if using SVRG),
            %
            \item $\widehat \nabla_\eta G_{t}^{(k,m+1)} \leftarrow \widehat \nabla_\eta G_{t}^{(k,m)}$ for all $t \neq t_m$ (or all $t$ if using SVRG).
        \end{itemize}
    \end{enumerate}
    \item Set $k \leftarrow k+1$ and return to step 2.
\end{enumerate}

While this extension is natural, it greatly complicates the convergence analysis since the E- and M- steps of the EM algorithm are no longer neatly separated. As a result, this algorithm cannot be seen as a specific instance of a generalized EM algorithm. Preliminary results indicate that this approach is feasible in practice, but future work involves convergence analysis. 

One option for convergence analysis involves showing that this is the limiting case of an SMC algorithm as the number of particles goes to infinity. SVRG and SAGA both produce unbiased gradient estimates conditioned on these particles. This is similar to the proof of \citet{Naesseth:2020} for markovian score climbing.

%Note that if the observed data is independent, then it is straightforward to apply variance-reduced stochastic gradient descent to the log-likelihood, since the log-likelihood of each data point contributes one term to a sum that makes up the log-likelihood. However, the log-likelihood of an HMM cannot be written as a tractable sum, so stochastic gradient descent is not feasible for the raw likelihood. 

The algorithm above is equivalent to standard variance-reduced stochastic gradient descent algorithms for independent data. This is because updating $\xi_{t_m}$ and $\gamma_{t_m}$, followed by $\nabla_{\theta} F_{t_m}(\theta;\xi_{t_m},\gamma_{t_m})$ and $\nabla_{\eta} G_{t_m}(\eta;\xi_{t_m},\gamma_{t_m})$ before taking a gradient step is equivalent to simply evaluating the gradient at data point $t_m$ for independent data by the Fisher identity for the gradient.

Note that we have to save both the old set of weights and the new set of weights for SVRG if we use a mixed E- and M-step. Also, if we are using SAG or SAGA, we can simply set $M = \infty$ and never fully refresh the gradient.
%Note that there is a problem for SVRG when changing the weights $\gamma$ and $\xi$ as we go. In particular, note that we have to re-evaluate the gradients at the old parameters to get unbiased estimates of the gradient. However, if the weights are changing as we do the M step, then we have to re-evaluate the old weights to do SVRG. BUT, notice that calculating those weights requires that we either store them or iterate through the whole data set :(. We could update the table average as we update the weights, but then we would have to know the OLD value of those weights to update the full gradient effectively. The only real saving grace we have here is that if we have the new weights, then saving the old weights is not as bad a saving the old gradients, which we would have to do for SAGA.