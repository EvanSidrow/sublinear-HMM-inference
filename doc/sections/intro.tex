%\section{Introduction}

Traditionally, the parameters of an HMM are estimated either via maximum likelihood estimation or Bayesian inference. As the name suggests, maximum likelihood estimation involves maximizing Equation (\ref{eqn:HMM_like}) with respect to the parameters $\theta$ and $\Gamma$. On the other hand, Bayesian inference usually involves sampling parameters from a posterior distribution using MCMC techniques, although recent work has approximated the posterior distribution using variational inference \citep{Foti:2014}.

The likelihood of an HMM is maximized either directly using the gradient of Equation (\ref{eqn:HMM_like}), or by using the \textit{Baum-Welch} algorithm \citep{Baum:1970}, which is a specific instance of the EM algorithm for HMMs. The Baum-Welch algorithm has several appealing theoretical guarantees compared to gradient descent, but it is less flexible and non-trivial to adapt to more complicated models. In addition, it requires solving an optimization problem at each ``M" step, and it converges linearly to a local maximum compared to quadratic convergence for second-order gradient descent methods \citep{Khreich:2012}. Ultimately, the decision between using the Baum-Welch algorithm versus gradient descent depends upon the researcher.

Likelihood maximization techniques return point estimates of parameter values and rely on the Fisher information to estimate standard errors. This approach fails to capture any multi-modality in the sampling distribution of the parameter estimates. In addition, likelihood maximization tends to estimate the probability of each hidden state $X_t$ conditioned on the parameter estimates $\hat \theta$, rather than estimating the parameters and hidden states jointly. Bayesian models solve both of these issues: they define a posterior distribution that models uncertainty in parameter estimates and jointly estimate the hidden states $X_t$ along with the parameters $\theta$. 

Unfortunately, most Bayesian models require sampling from a posterior distribution using MCMC techniques that are seen as more computationally expensive compared to likelihood maximization. However, the advent of Hamiltonian Monte Carlo \citep{Neal:2012}, the no-U-turn sampler \citep{Homan:2014}, non-reversible parallel tempering \citep{Syed:2019,Sacchi:2021}, and their implementation in statistical software such as \textit{stan} \citep{Stan:2021} and \textit{blang} \citep{Bouchard:2021} has considerably reduced the computational barrier of MCMC.

Nonetheless, scalability is a concern for all of these methods because each optimization or MCMC step requires an evaluation of the full data likelihood or its gradient- both of which have $\mathcal{O}(T)$ time complexity in the data set length $T$. While linear time complexity in $T$ is relatively efficient, bio-logging data sets are becoming increasingly large \citep{Patterson:2017}, and require increasingly complex HMMs to model \citep{Adam:2019}. In addition, common model validation techniques such as cross-validation can be computationally prohibitive even for relatively simple HMM models \citep{Pohle:2017}.

%To speed up likelihood evaluation, \citet{Nielsen:2011} evaluate the matrix multiplications from Equation (\ref{eqn:HMM_like}) in parallel. However, this algorithm actually \textit{increases} the time complexity of the algorithm to $\mathcal{O}(N^3 T)$ and can be too slow for large $N$ or when parallel computational power is limited.

For many statistical models, there do exist inference techniques that do not require iterating through the entire data set to update parameters. I will henceforth refer to these techniques as sub-linear methods because the time-complexity of one parameter update is faster than linear in the $T$. Unfortunately, most sub-linear inference methods rely on some independence assumptions that are violated by data generated from an HMM. 
%require dividing the data set into subsets that are assumed to be independent of one another. Unfortunately, HMMs explicitly model serial dependence between observations, so it is difficult to divide a data set into independent subsets. 
I will nonetheless review a selection of sub-linear inference methods for direct likelihood maximization, expectation maximization, and Bayesian inference to provide background and inspiration for my proposed method.

In the case of direct likelihood maximization, stochastic gradient descent is a ubiquitous technique from the machine learning literature \citep{Robbins:1951, Johnson:2013, Defazio:2014, Kingma:2014}. Stochastic gradient algorithms work by randomly sampling a sub-set of data, evaluating the gradient over that sub-set's likelihood, and taking a gradient step to update the parameters of the model. Stochastic gradient descent is known to converge to a local minimum of the negative log-likelihood under mild regularity conditions if each randomly sampled sub-set is \textit{independent} under the generating model \citep{Robbins:1951}.

In the case of expectation maximization, computation can be sped up by performing both the E- step and the M- step in parallel \citep{Lee:2021}. Another common sub-linear method for the EM algorithm is the \textit{incremental} EM algorithm, which involves updating only a subset of hidden variables in a mixture model \citep{Neal:1998, Thiesson:2001, Karimi:2019}. %Convergence properties of various stochastic gradient descent techniques and stochastic-EM algorithms have been derived by \citet{Chen:2018} and \citet{Karimi:2019}, respectively.

For Bayesian inference, stochastic gradient descent MCMC is a sub-linear sampling technique \citep{Ma:2015} that has been implemented in R \citep{Baker:2019}. \citet{Nemeth:2021} show that stochastic gradient descent can out-perform Hamiltonian Monte Carlo in some settings. In addition, stochastic variational inference (SVI) is a variational inference technique that updates an approximate posterior distribution without iterating thought the entire data  \citep{Hoffman:2013}.

All of the sub-linear methods mentioned here rely on the assumption that a data set easily can be divided into independent subsets, but this assumption is violated for HMMs. Nonetheless, some work has been done to apply sub-linear inference methods to HMMs.
%In the context of correlated data and HMMs, however, many of these methods either break down or require significant adjustments. In the case of direct likelihood maximization, %\citet{Saaudi:2019} implement stochastic gradient descent for HMMs, but their gradient estimates are biased and they do not provide any theoretical guarantees regarding approximation error or convergence properties. 
For example, \citet{Ye:2017} use the exponential memory decay of hidden Markov models to define sufficiently large subsets of data that are approximately independent of one another. The authors then use these approximately independent subsets of data to perform stochastic gradient descent for HMM parameter inference. However, this approach is approximate and requires that users tune subset sizes while running the algorithm. Tuning subset size requires estimating the Lyapunov exponent of a dynamical system defined by the probability transition matrix of an HMM.
More examples of sub-linear inference techniques are given by \citet{Khreich:2012}, who review incremental EM methods for HMM inference. One method of particular interest is that of \citet{Gotoh:1998}, which divides the observation sequence into sub-sequences, assumes that each sub-sequence is independent, and uses the incremental EM algorithm on the resulting data set. This method is approximate and requires sufficiently large sub-sequences to minimize approximation error. 
%\citet{Hsu:2012} introduce spectral methods for estimating HMMs, but this method avoids inferring the probability transition matrix and hidden states directly, even though these hidden states may be of interest to practitioners.

In a Bayesian setting, \citet{Foti:2014} implements stochastic variational inference for HMMs while \citet{Ma:2017} develops stochastic gradient MCMC for HMM. Both of these methods rely on the same basic structure as \citet{Ye:2017} and therefore suffer from the same issues. Namely, both methods have approximation error and requires that users calculate a sufficiently large subset size at each step during parameter inference.

In summary, it is difficult to account for the dependence structure of an HMM when developing efficient inference methods. As such, I will develop methods to split a data set generated from a PHMM into independent segments using the conditional independence property of PHMMs. Then, methods that require independent data such as stochastic gradient descent, incremental EM, SGMCMC, and variational inference can be used in a plug-and-play manner. I will also generalize these methods to perform sub-linear parameter updates for traditional HMMs (in addition to PHMMs).

%Independence between HMM sub-sequences has several applications beyond stochastic gradient and EM methods. These include, but are not limited to, tempering by subsampling \citep{vandeMeent:2014}, annealing by increasing resampling (AIR) \citep{Higuchi:2020}, and Bayesian or Hilbert coreset construction \citep{Campbell:2019}. It is my goal to perform short case and/or simulation studies to implement these inference methods and compare them with others mentioned earlier in this proposal. Luckily, many theoretical guarantees have already been derived in the references above, so by introducing independence into the HMM model, I get these theoretical guarantees for free.

This chapter is organized as follows: First, I set up notation and write the likelihood of a PHMM in terms of subsets of observations that are \textit{independent} conditioned on the set of labels $\{L_t\}_{t=1}^T = \{\ell_t\}_{t=1}^T$. Next, I generalize this technique so that it can be applied to a standard HMM (i.e., when no labels are known). After that, I introduce a memoization algorithm for HMM inference that uses stored likelihood estimates and avoids the need to condition on labels at all. Finally, I propose experiments and metrics that I will use to evaluate and compare these methods.