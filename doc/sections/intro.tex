Hidden Markov models (HMMs) are statistical models for sequential data that are widely used to model time series in fields such as speech recognition \citep{Gales:2008}, geology \citep{Bebbington:2007}, neuroscience \citep{Kottaram:2019}, finance \citep{Mamon:2007}, and ecology \citep{McClintock:2020}. Such models are often used to predict a latent process of interest (e.g. a spoken phrase or an animal's behavioral state) from an observed time series (e.g. raw audio or time-depth data). Many practitioners estimate the parameters of an HMM by maximizing the likelihood function using either gradient-based numerical maximization or the EM algorithm \citep{Baum:1970,Dempster:1977}.

One serious concern for both numerical maximization and the EM algorithm is that every parameter update requires iterating though the full set of observations to calculate either the likelihood or its gradient. This concern is likely to only worsen in the future, as time-series data sets are increasingly collected at high frequencies and contain large numbers of observations \citep{Patterson:2017,Li:2020}. These data sets require progressively complex HMMs which can be computationally expensive to fit \citep{Adam:2019,Sidrow:2021}. In addition, many model validation techniques such as cross-validation require repeated parameter estimation which can be prohibitive even for relatively simple HMMs \citep{Pohle:2017}.

Many inference techniques for independent data sets do not require iterating through the entire data set to update a modelâ€™s parameters. We henceforth refer to these techniques as \textit{sub-linear} methods. One example of a sub-linear method is stochastic gradient descent (SGD), which estimates the gradient of the full likelihood using a random subset of the data \citep{Robbins:1951}. It is ubiquitous in the optimization literature and has inspired several extensions. For example, \citet{Johnson:2013}, \citet{Defazio:2014} and \citet{Kingma:2014} reduce the variance of the gradient estimates compared to standard SGD, while \citet{Zinkevich:2010} incorporate parallelization into SGD.
%
Similarly, the incremental EM algorithm is a generalization of the EM algorithm that updates only a subset of hidden variables at each E step \citep{Neal:1998, Thiesson:2001, Karimi:2019}. 
%
Both stochastic gradient descent and incremental EM assume that the underlying data set is comprised of independent subsets. Such an assumption is sometimes reasonable for HMMs. For example, \citet{Gales:2008} infer spoken words from many relatively short audio files, and they assume that each audio file is independent from one another. However, the assumption of independence is generally violated for HMMs designed for long, sequentially-dependent time series. Long time series are increasingly common in practice and are the focus of this paper. 

Some work has been done to apply sub-linear inference methods to HMMs for long time series. For example, \citet{Gotoh:1998} divide a sequence of observations into segments and use the incremental EM algorithm to perform inference. This approach assumes that segments are independent of one another, which is not true in general. Alternatively, \citet{Ye:2017} define a sufficiently large ``buffer" before and after segments of data to minimize the effect of serial dependence. However, the appropriate size of the buffer can be difficult to calculate. More examples of sub-linear inference techniques are given by \citet{Khreich:2012}, who review on-line and incremental methods for HMM inference. However, most of these methods assume either that the M step of the EM algorithm is tractable (see section 3.2.1 of \citet{Khreich:2012}), or that the emissions of the HMM are discrete \citep{Baldi:1993}. 

In this paper, we introduce a new inference method for HMMs based on variance-reduced stochastic optimization. This inference method updates the HMM parameters without iterating through the entire data set. Critically, it does not require a closed-form solution for its M step, does not require any buffer tuning, and does not introduce error into the HMM likelihood.

We begin with a formal definition of HMMs, a brief review of standard inference techniques for HMMs, and a description of stochastic optimization algorithms before introducing our algorithm. We then prove that our algorithm converges to a local maximum of the likelihood (under standard regularity assumptions) and note several practical considerations regarding its implementation. Finally, we compare the efficiency of our new algorithm to that of standard optimization techniques using several simulation studies and a kinematic case study of eight northern resident killer whales ({\em{Orcinus orca}}) off the western coast of Canada. 