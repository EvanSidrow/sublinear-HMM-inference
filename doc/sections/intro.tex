%\section{Introduction}

%Recent advances in tracking technology allows ecologists to collect an unprecedented amount of movement and kinematic data for a wide variety of animals \citep{Patterson:2017}. These data sets allow researchers to detect hunting behavior \citep{Heerah:2017}, understand habitat selection \citep{Michelot:2019b}, and develop activity budgets \citep{Dot:2016} for these animals. Understanding an animal's movement and behavior can assist in their conservation \citep{Sutherland:1998}. One of the most prevalent models used to describe animal movement is the hidden Markov model, or HMM \citep{McClintock:2020}.

Hidden Markov models (HMMs) are statistical models for sequential data that are widely used to model time series in fields such as speech recognition \citep{Gales:2008}, geology \citep{Bebbington:2007}, neuroscience \citep{Kottaram:2019}, finance \citep{Mamon:2007}, and ecology \citep{McClintock:2020}. Such models are often used to predict a latent process of interest (e.g. a spoken phrase or an animal's behavioral state) from an observed time series (e.g. raw audio or time-depth data). %Hidden Markov models can be used in both supervised and unsupervised settings.
%Observations from an HMM are generated from a distribution which depends upon the state of an unobserved (or ``hidden") Markov chain. 
Many practitioners estimate the parameters of an HMM by maximizing the likelihood function using either gradient-based numerical maximization or the Baum-Welch algorithm \citep{Baum:1970}. The latter is a special case of the expectation-maximization (EM) algorithm. 

One serious concern for both numerical maximization and the Baum-Welch algorithm is that every parameter update requires iterating though the full set of observations to calculate either the likelihood or its gradient. This concern is likely to only worsen in the future, as time-series data sets are increasingly collected at high frequencies and contain large numbers of observations \citep{Patterson:2017,Li:2020}. These data sets require increasingly complex HMMs which can be computationally expensive to fit \citep{Adam:2019,Sidrow:2021}. In addition, many model validation techniques such as cross-validation require repeated parameter estimation which can be prohibitive even for relatively simple HMMs \citep{Pohle:2017}.

%On the other hand, Bayesian inference usually involves sampling parameters from a posterior distribution using MCMC techniques, although recent work has approximated the posterior distribution using variational inference \citep{Foti:2014}.
%First, the likelihood can be maximized directly using standard numerical maximization techniques such as gradient descent. Alternatively, practitioners can use the \textit{Baum-Welch} algorithm , which is a specific instance of the EM algorithm adapted for HMMs. 

%The Baum-Welch algorithm has several appealing theoretical guarantees compared to gradient descent, but it is less flexible and non-trivial to adapt to more complicated models. In addition, it requires solving an optimization problem at each step, even when the optimization problem does not have a closed-form solution. 
%Finally, the Baum-Welch algorithm converges linearly to a local maximum of the likelihood function, while second-order gradient descent methods \citep{Khreich:2012} can converge quadratically. Ultimately, the decision between using the Baum-Welch algorithm versus gradient descent depends upon the researcher.

%Likelihood maximization techniques return point estimates of parameter values and rely on the Fisher information to estimate standard errors. This approach fails to capture any multi-modality in the sampling distribution of the parameter estimates. In addition, likelihood maximization tends to estimate the probability of each hidden state $X_t$ conditioned on the parameter estimates $\hat \theta$, rather than estimating the parameters and hidden states jointly. Bayesian models solve both of these issues: they define a posterior distribution that models uncertainty in parameter estimates and jointly estimate the hidden states $X_t$ along with the parameters $\theta$. 

%Unfortunately, most Bayesian models require sampling from a posterior distribution using MCMC techniques that are seen as more computationally expensive compared to likelihood maximization. However, the advent of Hamiltonian Monte Carlo \citep{Neal:2012}, the no-U-turn sampler \citep{Homan:2014}, non-reversible parallel tempering \citep{Syed:2019,Sacchi:2021}, and their implementation in statistical software such as \textit{stan} \citep{Stan:2021} and \textit{blang} \citep{Bouchard:2021} has considerably reduced the computational barrier of MCMC.

%To speed up likelihood evaluation, \citet{Nielsen:2011} evaluate the matrix multiplications from Equation (\ref{eqn:HMM_like}) in parallel. However, this algorithm actually \textit{increases} the time complexity of the algorithm to $\mathcal{O}(N^3 T)$ and can be too slow for large $N$ or when parallel computational power is limited.

Many inference techniques for independent data sets do not require iterating through the entire data set to update a model's parameters. We henceforth refer to these techniques as \textit{sub-linear} methods. %because the time complexity of one parameter update is faster than linear in the length of the observation sequence. 
%but most %sub-linear inference methods 
%rely on independence assumptions that are violated in applications involving sequential data. 
% require dividing the data set into subsets that are assumed to be independent of one another. Unfortunately, HMMs explicitly model serial dependence between observations, so it is difficult to divide a data set into independent subsets. 
% I will nonetheless review a selection of sub-linear inference methods for direct likelihood maximization, expectation maximization, and Bayesian inference to provide background and inspiration for my proposed method.
%
One example of a sub-linear method is stochastic gradient descent, which %relies on an unbiased estimate of the full gradient by
evaluates the gradient of the likelihood over a random subset of the data \citep{Robbins:1951}. It is ubiquitous in the optimization literature and has inspired several extensions \citep{Johnson:2013, Defazio:2014, Kingma:2014}. %However, most theoretical guarantees rely on the assumption that all observations are independent of one another. 
%Stochastic gradient algorithms work by randomly sampling a sub-set of data, evaluating the gradient over that sub-set's likelihood, and taking a gradient step to update the parameters of the model. Stochastic gradient descent is known to converge to a local minimum of the negative log-likelihood under mild regularity conditions if each randomly sampled sub-set is independent under the generating model \citep{Robbins:1951}. %However, this independence assumption is violated for time-series data since observations are correlated in time.
%
Similarly, the incremental EM algorithm is a generalization of the EM algorithm %often used to fit mixture model 
that updates only a subset of hidden variables at each E step \citep{Neal:1998, Thiesson:2001, Karimi:2019}. %Convergence properties of various stochastic gradient descent techniques and stochastic-EM algorithms have been derived by \citet{Chen:2018} and \citet{Karimi:2019}, respectively.
%
%For Bayesian inference, stochastic gradient descent MCMC is a sub-linear sampling technique \citep{Ma:2015} that has been implemented in R \citep{Baker:2019}. \citet{Nemeth:2021} show that stochastic gradient descent can out-perform Hamiltonian Monte Carlo in some settings. In addition, stochastic variational inference (SVI) is a variational inference technique that updates an approximate posterior distribution without iterating thought the entire data  \citep{Hoffman:2013}.
%
However, these methods assume that the underlying data set is comprised of independent subsets. This assumption is violated for HMMs since the underlying data set exhibits sequential dependence. 

Some work has been done to apply sub-linear inference methods to HMMs.
%In the context of correlated data and HMMs, however, many of these methods either break down or require significant adjustments. In the case of direct likelihood maximization, %\citet{Saaudi:2019} implement stochastic gradient descent for HMMs, but their gradient estimates are biased and they do not provide any theoretical guarantees regarding approximation error or convergence properties. 
For example, \citet{Gotoh:1998} divide the sequence of observations into subsequences
%, assumes that each sub-sequence is independent, 
and use the incremental EM algorithm to perform inference. This approach assumes that subsequences are independent of one another, which is not true in general. Alternatively, \citet{Ye:2017} define a sufficiently large ``buffer" before and after subsequences of data to minimize the effect of serial dependence. %The authors then use these approximately independent subsets of data to perform stochastic gradient descent for HMM parameter inference. 
However, the appropriate size of the buffer can be difficult to calculate. %since it depends upon the Lyapunov exponent of a dynamical system defined by the probability transition matrix of an HMM. 
More examples of sub-linear inference techniques are given by \citet{Khreich:2012}, who review on-line and incremental methods for HMM inference. However, most of these methods assume either that the M step of the EM algorithm is tractable (see section 3.2.1 of \citet{Khreich:2012}), or that the emissions of the HMM are discrete \citep{Baldi:1993}. 

In this paper, we introduce a new inference method for HMMs based on variance-reduced stochastic optimization. This inference method updates the HMM parameters without iterating through the entire data set. Critically, it does not require a closed-form solution for its M step, does not require any buffer tuning, and does not introduce error into the HMM likelihood.

We begin with a formal definition of HMMs, a brief review of standard inference techniques for HMMs, and a description of stochastic optimization algorithms before introducing our algorithm. We then prove that our algorithm converges to a local maximum of the likelihood (under standard regularity assumptions) and note several practical considerations regarding its implementation. Finally, %to demonstrate its advantages and applicability to large data sets and complicated models, 
we compare the efficiency of our new algorithm to that of standard optimization techniques using several simulation studies and a kinematic case study of eight killer whales ({\em{Orcinus orca}}) off the western coast of Canada.

%\citep{Cappe:1998} argue for a quasi-Newton algorithm for quadratic convergence, but our simulation studies show that an out-of-the-box implementation of BFGS (a quasi-Newton optimization algorithm) for HMMs performs very poorly if the original parameter estimates are not close to the optimum parameters. 
%\citet{Hsu:2012} introduce spectral methods for estimating HMMs, but this method avoids inferring the probability transition matrix and hidden states directly, even though these hidden states may be of interest to practitioners.

%In a Bayesian setting, \citet{Foti:2014} implements stochastic variational inference for HMMs while \citet{Ma:2017} develops stochastic gradient MCMC for HMM. Both of these methods rely on the same basic structure as \citet{Ye:2017} and therefore suffer from the same issues. Namely, both methods have approximation error and requires that users calculate a sufficiently large subset size at each step during parameter inference.

%In summary, it is difficult to account for the dependence structure of an HMM when developing efficient inference methods. As such, I will develop methods to split a data set generated from a PHMM into independent segments using the conditional independence property of PHMMs. Then, methods that require independent data such as stochastic gradient descent, incremental EM, SGMCMC, and variational inference can be used in a plug-and-play manner. I will also generalize these methods to perform sub-linear parameter updates for traditional HMMs (in addition to PHMMs).

%Independence between HMM sub-sequences has several applications beyond stochastic gradient and EM methods. These include, but are not limited to, tempering by subsampling \citep{vandeMeent:2014}, annealing by increasing resampling (AIR) \citep{Higuchi:2020}, and Bayesian or Hilbert coreset construction \citep{Campbell:2019}. It is my goal to perform short case and/or simulation studies to implement these inference methods and compare them with others mentioned earlier in this proposal. Luckily, many theoretical guarantees have already been derived in the references above, so by introducing independence into the HMM model, I get these theoretical guarantees for free.

%This chapter is organized as follows: First, I set up notation and write the likelihood of a PHMM in terms of subsets of observations that are \textit{independent} conditioned on the set of labels $\{L_t\}_{t=1}^T = \{\ell_t\}_{t=1}^T$. Next, I generalize this technique so that it can be applied to a standard HMM (i.e., when no labels are known). After that, I introduce a memoization algorithm for HMM inference that uses stored likelihood estimates and avoids the need to condition on labels at all. Finally, I propose experiments and metrics that I will use to evaluate and compare these methods.