\documentclass[11pt]{article}
\pdfoutput=1

\input{header}
\input{defs}

\pdfminorversion=4
% NOTE: To produce blinded version, replace "0" with "1" below.
\newcommand{\blind}{1}
\linenumbers

% DON'T change margins - should be 1 inch all around.
\addtolength{\oddsidemargin}{-.5in}%
\addtolength{\evensidemargin}{-1in}%
\addtolength{\textwidth}{1in}%
\addtolength{\textheight}{1.7in}%
\addtolength{\topmargin}{-1in}%

\date{}
\begin{document}

%\bibliographystyle{natbib}

\def\spacingset#1{\renewcommand{\baselinestretch}%
{#1}\small\normalsize} \spacingset{1}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\if0\blind
{
    \title{Variance-Reduced Stochastic Optimization for Efficient Inference of Hidden Markov Models : Response to Reviewer 1}

    \author{
      \textbf{Evan Sidrow} \\
      Department of Statistics \\
      University of British Columbia\\
      Vancouver, Canada \\
      \texttt{evan.sidrow@stat.ubc.ca} \\
      %
      \and
      %
      \textbf{Nancy Heckman} \\
      Department of Statistics \\
      University of British Columbia \\
      Vancouver, Canada \\
      %
      \and
      %
      \textbf{Alexandre Bouchard-C\^ot\'e} \\
      Department of Statistics \\
      University of British Columbia \\
      Vancouver, Canada \\
      \and
      %
      \textbf{Sarah M. E. Fortune} \\
      Department of Oceanography \\
      Dalhousie University \\
      Halifax, Canada \\
      %
      \and
      %
      \textbf{Andrew W. Trites} \\
      Department of Zoology \\
      Institute for the Oceans and Fisheries \\
      University of British Columbia \\
      Vancouver, Canada \\
      %
      \and
      %
      \textbf{Marie Auger-M\'eth\'e} \\
      Department of Statistics \\
      Institute for the Oceans and Fisheries \\
      University of British Columbia \\
      Vancouver, Canada \\
    }
    \maketitle
} \fi

\if1\blind
{
  \bigskip
  \bigskip
  \bigskip
  \begin{center}
    {\LARGE\bf Variance-Reduced Stochastic Optimization for Efficient Inference of Hidden Markov Models: Response to Reviewer 1}
  \end{center}
  \medskip
} \fi

I really like this paper, I have mostly minor comments. The paper is very well written. For example, after reading and writing basics of HMMs multiple times, I really appreciate the clear and concise description of HMM in section 2.1!

\begin{itemize}
    \item \textit{Thank you very much for the kind words. Your comments are insightful and have helped improve the manuscript. We hope that our responses and edits to the manuscript address all of the comments given. We have uploaded two versions of the paper: one with all black font, and one with red font where significant changes have been made.}
\end{itemize}

In your simulation study as well as case study, you consider Gaussian observational densities. Consider a case where the observations are categorical, so that for example you only observe some category $j$ when $t < T/2$. When using only partial data based on $F_{t_m}$, could such thing cause problems in the M loop? My gut feeling is that this could be problematic, but that might very well be incorrect. If there is a potential issue here, then I'd like to see an example on how the method performs in categorical data case, or at least a disclaimer that method assumes something about the observational density.

\begin{itemize}
    \item \textit{Thank you for this comment, as it brings up a good point about stochastic optimization in general. Below we address some issues that we believe that the reviewer may have in mind, but please let us know if we are missing anything.}
    
    \textit{First, note that a categorical distribution is convex and Lipschitz smooth with respect to the parameters of the distribution. Namely, if the emission distribution is categorical with a total of $J$ categories, then one may define the parameters as $\theta^{(i)} = \begin{pmatrix} \theta^{(i,1)} & \theta^{(i,2)} & \cdots & \theta^{(i,J)} \end{pmatrix}$ so that the log-density of the emission distribution is}
    
    $$\log f^{(i)}(y_t = j \mid X_t = i ; \theta^{(i)}) = \theta^{(i,j)} - {\log \left(\sum_{j' = 1}^{J} \exp(\theta^{(i,j')})\right)},$$ 
    %
    \textit{where $\theta^{(i,1)} = 0$ for identifiability. Then, $-\log f^{(i)}(y_t = j \mid X_t = i ; \theta^{(i)})$ is convex and uniformly Lipschitz smooth with respect to $\theta^{(i)}$, so theorem 1 from the text still applies (as long as the other conditions from the theorem are satisfied).}
    
    \textit{Next, if category $j$ is extremely rare, then the gradient estimate $\widehat \nabla F_t$ may become out of date with respect to the parameter $\theta^{(i,j)}$ for all $i = 1,\ldots,N$. While this may be true, note that in our algorithm, the full gradient is refreshed every E step, and each M step is approximately as expensive to compute as one gradient step of gradient-based algorithms. As such, the gradient is no more out-of-date than existing gradient-based algorithms. It is also an issue if category $j$ appears less than $N$ times altogether, in which case it would make sense that perhaps $\hat \theta^{(i,j)} \to -\infty$ for some hidden state $i$. This can be dealt with by restricting the parameter space or by introducing a prior over the parameters. However, this is largely an issue with estimating rare events and beyond the scope of our paper.}
    
    \textit{Further, if category $j$ is only observed when $t < T/2$, then the sequential dependence within the HMM might be extremely strong. This may be a problem if the M step proceeded by iterating though $t = 1,2,\ldots,T$ sequentially. However, we select $t_m$ randomly at each iteration of the M step, which addresses issues caused by sequential dependence. We also sample without replacement and have $M \geq T$ for all of our experiments. This guarantees that we will select all $t = 1,\ldots,T$ within each iteration of the M step of our algorithm.}
    
    \textit{Finally, this comment brings up a good point regarding an issue with stochastic optimization in general. In particular, if the gradients of each term ($\nabla F_t$) are very different from one another, then the variance of the gradient estimates can be very large, which will negatively affect the optimization. This issue is still an area of active research in stochastic optimization, and is the reason why we employ variance-reduced techniques (e.g. SVRG and SAGA) instead of standard SGD. Even in settings with high variance between gradient terms, stochastic optimization is often preferred over standard gradient descent when the loss function is very expensive to compute.}
\end{itemize}

The paper focuses on speeding up the estimation in cases with large $T$. A brief discussion on problems with large number of time series would be nice, although this is often less of a problem as it is possible to parallelize many computations with respect to the number of series.

\begin{itemize}
    \item \textit{We have added to the discussion of this topic in our introduction. Specifically, in the paragraph on sub-linear inference algorithms for datasets comprised of independent subsets, we now  clarify this point and address the case when the number of time series is large (lines 46-50).}
\end{itemize}

Regarding the requirement of increasing log-likelihood: You prove that this will not require infinite loop, but wouldn't it still be possible that the loop takes infeasibly long to complete? While perhaps theoretically problematic, can you say anything about practical consequences of adding some kind of upper limit to the number of iterations of the M step loop in order to avoid overly long loops? 

\begin{itemize}
    \item \textit{We agree that this could present a significant issue when running the algorithm. However, in our case and simulation studies, we found that the log-likelihood increased after a single pass through the M step in almost all situations. The only exception was when the step size was very large, so the parameters were ``thrown off" into very weird regions of parameter space. However, in this case we halved the step size after each unsuccessful M step, and the algorithm corrected itself. To this end, if practitioners find that the log-likelihood never increases after each M step, then we would suggest re-selecting the initial step-size, increasing the number of iterations within the M step, or doing other forms of trouble-shooting, as this behavior is very unexpected in practice. We have added information about this in the main text after the presentation of the algorithm (lines 267-270).}
\end{itemize}

Related to this, one of the assumptions of Theorem 1 is that $F_t$ is convex. Isn't this assumption violated for typical HMMs which exhibit multi-modality (not only due to label-switching)? I think implicitly you are considering Theorem 1 in the neighbourhood of some local optimum (as is hinted in line 317)? 

\begin{itemize}
    \item \textit{It is true that the log-likelihood of a typical HMM is multi-modal and non-convex. However, our convexity requirement is for the surrogate function $F_t$ rather than the full log-likelihood $\log p$. In addition, $F_t$ is a linear combination of probability densities which are often convex. Further, we only prove convergence to a local (not global) optimum. We have clarified these points on lines 300-309.}
\end{itemize}

Continuing on multimodality, can you say anything about how the stochasticity of the proposed algorithm affects to optimization in terms of escaping local modes? This is likely not specific to the proposed algorithm though, so maybe it's enough to say something general about stochastic optimization and mixture models.

\begin{itemize}
    \item \textit{Relating to the case when $P = \texttt{False}$, we have added a reference to some work suggesting that the EM algorithms (like \texttt{EM-VRSO} with $P = \texttt{False}$) can escape local optima of the likelihood faster than direct numerical optimization.} 

    \textit{If $P = \texttt{True}$, then we do not have any formal theoretical results about escaping local optima. However, Figures 4 and 7 in the main text show scatter plots of the likelihood at termination for each algorithm and parameter initialization. Those figures suggest that our algorithm (specifically with $A = \texttt{SVRG}$, $P = \texttt{True}$, and $M=T$) tends to escape local optima better than the baseline methods.}
    
    \textit{If the M step of the EM algorithm is not convex, then some previous work has shown that stochastic gradient methods can escape local optima more effectively than standard gradient descent. We have added some information about this after the statement of Theorem 1 (lines 300-309).}
\end{itemize}

On 5.1, you state that $\mu^{(i)}$ is $N(0, I)$, i.e. it does not depend on state? Same for the covariance matrix. This probably leads to simulated data where the states are very similar. If you would simulate states for example as $N(i, I)$, they would likely be more easily separable, could you say something about how the results of the simulation might change in such, perhaps more realistic setting? My guess that all methods would perform even better but would be more sensitive to initialization?

\begin{itemize}
    \item \textit{The covariance matrix indeed does not depend upon state, and $\mu^{(i)}$ is drawn from a standard normal distribution for all $i = 1,\ldots,N$. However, the distance between the means will grow with the dimension $d$, as the Euclidean distance between any two of these means is $\sqrt{2}$ times a chi random variable with $d$ degrees of freedom:}

    $$||\mu^{(i)} - \mu^{(j)}|| = \sqrt{2} * \sqrt{\sum_{k = 1}^d \left(\frac{\mu_k^{(i)} - \mu_k^{(j)}}{\sqrt{2}}\right)^2}, \qquad \frac{\mu_k^{(i)} - \mu_k^{(j)}}{\sqrt{2}} \sim \calN(0,1).$$

    \textit{Therefore, for $d = 3$, the expected value of Euclidean distance between the means is approximately $2.26$ with a standard deviation of approximately $0.95$, and for $d = 6$ the expected value is approximately $3.32$ with a standard deviation of approximately $0.98$. In our case, when $d = 3$ and $N = 3$, the Euclidean distance between any pair of means within a data set ranged from 1.0 to 4.0, and when $d = 6$ and $N = 3$, the Euclidean distance between any pair of means within a data set ranged from 2.0 to 4.7. The standard deviation of the observation distribution was $1/e \approx 0.37$, so the means in our simulation study were relatively well separated. We have added to section 5.1 to discuss this (lines 386-389).}
\end{itemize}

Line 384, why not compare to standard Baum-Welch as well?

\begin{itemize}
    \item \textit{We did not compare our algorithm to the standard Baum-Welch because we are primarily concerned with situations in which closed-form solutions to the M-step of the EM algorithm are not available. If such solutions are available, then we would suggest using the standard Baum-Welch or a partial E-step within the standard Baum-Welch. We only chose a model with closed-form solutions to keep the example clear and easily reproducible. We now clarify why we did not use the Baum-Welch algorithm on lines 413-417.}
\end{itemize}

Section 5.3, if the interest is in modeling very large data sets (in terms of $T$?), why not show those results in the main text and $T=10^3$ in the supplement?

\begin{itemize}
    \item \textit{Apologies for the confusion: we did show the results for $T=10^5$ in the main text and deferred the results for $T = 10^3$ in the supplement. We have clarified this in Section 5.3 (lines 447-449).}
\end{itemize}

At the beginning of the case study, I started to wonder what these dives and depth readings are, for which I found an answer later in 6.1, maybe it would be good to mention already at the beginning that definitions will follow.

\begin{itemize}
    \item \textit{Thank you for pointing this out. We introduced the data at the beginning of the section 6 to highlight that the underlying data set was large, but we agree that introducing the data so early is confusing. We have deleted the bit about the data set in the first paragraph of the section, and simply introduced the underlying data in section 6.1 (lines 480-495).}
\end{itemize}

The description of the transition matrix in the case study is bit hard to follow. Equation (36) defines $\Gamma_t$, but then (37) redefines it, should (36) be about $\Gamma_c$? Maybe it would be good to show the full transition matrix 37 as expanded as well? Why does $\Gamma_t$ depend on time? At first I though you were using the observed sequences to define the start/end of dives, but as you are also modeling $E_t$ it appears not. 

\begin{itemize}
    \item \textit{We agree that the presentation of the transition matrix was not clear in the case study. It is in fact the case that the transition matrix at time $t$ depends upon the observation $E_{t-1}$, and this fact was skimmed over. Then, technically the model in the case study is actually a generalized version of an HMM (Li et al. refer to it as an HMMSDO). We have addressed this in the main text and added citations from papers that show that allowing the transition matrix to depend upon the previous observation does not affect the inference procedure (lines 517-524).}
    
    \textit{We experimented with expanding the transition matrix in (37), but unfortunately the 9-by-9 matrix was very large to write out in the main text. However, we have added an expanded version in Supplement B.}
\end{itemize}

%\bibliography{References}

\end{document}
