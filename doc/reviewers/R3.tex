\documentclass[11pt]{article}
\pdfoutput=1

\input{header}
\input{defs}

\pdfminorversion=4
% NOTE: To produce blinded version, replace "0" with "1" below.
\newcommand{\blind}{1}
\linenumbers

% DON'T change margins - should be 1 inch all around.
\addtolength{\oddsidemargin}{-.5in}%
\addtolength{\evensidemargin}{-1in}%
\addtolength{\textwidth}{1in}%
\addtolength{\textheight}{1.7in}%
\addtolength{\topmargin}{-1in}%

\date{}
\begin{document}

%\bibliographystyle{natbib}

\def\spacingset#1{\renewcommand{\baselinestretch}%
{#1}\small\normalsize} \spacingset{1}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\if0\blind
{
    \title{Variance-Reduced Stochastic Optimization for Efficient Inference of Hidden Markov Models : Response to Reviewer 1}

    \author{
      \textbf{Evan Sidrow} \\
      Department of Statistics \\
      University of British Columbia\\
      Vancouver, Canada \\
      \texttt{evan.sidrow@stat.ubc.ca} \\
      %
      \and
      %
      \textbf{Nancy Heckman} \\
      Department of Statistics \\
      University of British Columbia \\
      Vancouver, Canada \\
      %
      \and
      %
      \textbf{Alexandre Bouchard-C\^ot\'e} \\
      Department of Statistics \\
      University of British Columbia \\
      Vancouver, Canada \\
      \and
      %
      \textbf{Sarah M. E. Fortune} \\
      Department of Oceanography \\
      Dalhousie University \\
      Halifax, Canada \\
      %
      \and
      %
      \textbf{Andrew W. Trites} \\
      Department of Zoology \\
      Institute for the Oceans and Fisheries \\
      University of British Columbia \\
      Vancouver, Canada \\
      %
      \and
      %
      \textbf{Marie Auger-M\'eth\'e} \\
      Department of Statistics \\
      Institute for the Oceans and Fisheries \\
      University of British Columbia \\
      Vancouver, Canada \\
    }
    \maketitle
} \fi

\if1\blind
{
  \bigskip
  \bigskip
  \bigskip
  \begin{center}
    {\LARGE\bf Variance-Reduced Stochastic Optimization for Efficient Inference of Hidden Markov Models: Response to Reviewer 3}
  \end{center}
  \medskip
} \fi

Dear Authors,
I hope this message finds you well. I recently had the opportunity to review your paper titled “Variance-Reduced Stochastic Optimization for Efficient Inference of Hidden Markov Models”. I would like to express my appreciation for the substantial contributions you have made. Your paper is exceptionally well-written, offering a concise yet comprehensive overview of the stochastic gradient descent (SGD) literature and presenting an elegant marriage of notation from hidden Markov Models (HMM), expectation maximization (EM), and SGD. It is, without a doubt, an excellent paper that addresses the crucial issue of linear optimization speed for lengthy time series, marking a pioneering effort in the field. The relevance of your work to the increasing data resolution trend, leading to larger datasets, further highlights its importance. However, I would like to offer some constructive suggestions for further enhancement. Please find my comments below; I hope you find them helpful. My primary concern revolves around the exclusive use of epochs to quantify optimization effort. I recommend considering the inclusion of time until convergence, in addition to epochs, as a metric for measuring complexity. Sole reliance on epochs may slightly favor your method, as one full numerical gradient evaluation, equating to the complexity of one epoch, might be faster than one epoch comprising several batch gradient evaluations. In the end, estimation time matters; thus, I believe it should also be reported in your simulation experiments and the application. Therefore, I would suggest adding time to convergence to the results. 

Second, while your simulation scenarios and the real data application paint a sufficiently detailed picture of the method’s performance in a proof-of-concept manner, what I missed is the performance results when fitting inhomogeneous HMMs with covariates affecting the state process. Typically, as these models are less stable than homogeneous models, they are more difficult to fit and often tend to converge to local minima when choosing suboptimal starting values. As such, it would be intriguing to see if a stochastic optimization approach performs worse or indeed better than established methods as it could be less prone to converge to local minima. I don’t expect you to extend your simulations to such models in this contribution, but I would propose to comment on this in the discussion, as you already refer to more complex models. Besides that, I only have some minor comments: 

page 12: Please provide an explanation for keeping the expected number of transitions constant, as this seems unconventional.

Regarding your simulation experiments, I did not really understand if your baseline is a vanilla EM with full BFGS optimization in the M step or if this refers to direct numerical maximum likelihood estimation using BFGS. Please clarify this.

page 3, line 6: I find it confusing that you use η (i) with one and η (i,j) with two superscripts to denote the transformed parameters for δ and Γ. I propose using different letters here to subsequently summarise them in one vector η.

page 3, equation 3: You should avoid using capital letters X and Y in the equation for the likelihood (random variable notation). This is rather unusual, as xt and yt are treated as fixed realizations here.

page 6, algorithm 3, line 5: The colon after “do” is inconsistent with the remaining algorithms, in which you don’t use a colon.

page 8: It is rather distracting to the reader that the theorem is broken after point one.

page 11 line 9: Please note that the original source (Schmidt et al.) uses the squared norm in this context.

Once again, thank you for your valuable contribution to the field. I believe that addressing these suggestions would further elevate the impact of your work. I look forward to seeing its continued development.

Best wishes

\end{document}
