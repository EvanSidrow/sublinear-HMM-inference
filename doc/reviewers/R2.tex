\documentclass[11pt]{article}
\pdfoutput=1

\input{header}
\input{defs}

\pdfminorversion=4
% NOTE: To produce blinded version, replace "0" with "1" below.
\newcommand{\blind}{1}
\linenumbers

% DON'T change margins - should be 1 inch all around.
\addtolength{\oddsidemargin}{-.5in}%
\addtolength{\evensidemargin}{-1in}%
\addtolength{\textwidth}{1in}%
\addtolength{\textheight}{1.7in}%
\addtolength{\topmargin}{-1in}%

\date{}
\begin{document}

%\bibliographystyle{natbib}

\def\spacingset#1{\renewcommand{\baselinestretch}%
{#1}\small\normalsize} \spacingset{1}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\if0\blind
{
    \title{Variance-Reduced Stochastic Optimization for Efficient Inference of Hidden Markov Models : Response to Reviewer 2}

    \author{
      \textbf{Evan Sidrow} \\
      Department of Statistics \\
      University of British Columbia\\
      Vancouver, Canada \\
      \texttt{evan.sidrow@stat.ubc.ca} \\
      %
      \and
      %
      \textbf{Nancy Heckman} \\
      Department of Statistics \\
      University of British Columbia \\
      Vancouver, Canada \\
      %
      \and
      %
      \textbf{Alexandre Bouchard-C\^ot\'e} \\
      Department of Statistics \\
      University of British Columbia \\
      Vancouver, Canada \\
      \and
      %
      \textbf{Sarah M. E. Fortune} \\
      Department of Oceanography \\
      Dalhousie University \\
      Halifax, Canada \\
      %
      \and
      %
      \textbf{Andrew W. Trites} \\
      Department of Zoology \\
      Institute for the Oceans and Fisheries \\
      University of British Columbia \\
      Vancouver, Canada \\
      %
      \and
      %
      \textbf{Marie Auger-M\'eth\'e} \\
      Department of Statistics \\
      Institute for the Oceans and Fisheries \\
      University of British Columbia \\
      Vancouver, Canada \\
    }
    \maketitle
} \fi

\if1\blind
{
  \bigskip
  \bigskip
  \bigskip
  \begin{center}
    {\LARGE\bf Variance-Reduced Stochastic Optimization for Efficient Inference of Hidden Markov Models: Response to Reviewer 2}
  \end{center}
  \medskip
} \fi

The authors propose improving the computationally expensive E- and M-steps of the Baum-Welch algorithm via a partial E-step and variance-reduced stochastic optimization techniques, respectively, to infer the parameters of hidden Markov models. The proposed method is supported by two theorems guaranteeing the convergence to a local maximum, and is tested via two numerical studies, one extensive simulation study and one realistic data analysis. The manuscript is very well-written and structured with strong theoretical justification. I only have several minor comments that are listed below. Please feel free to educate me if the authors think I misunderstand the manuscript.

\begin{itemize}
    \item \textit{Thank you very much for the kind words. Your comments are insightful and have helped improve the manuscript. We hope that our responses and edits to the manuscript address all of the comments given. We have uploaded two versions of the paper: one with all black font, and one with red font where significant changes have been made.}
\end{itemize}

1. Page 2: The authors state that the Baum-Welch algorithm is a special case of EM. It would be great if the authors could briefly explain why (or in what sense) the Baum-Welch algorithm is a special case of EM.

\begin{itemize}
    \item \textit{We have removed the comparison in the introduction, but we have added the following to section 2.3 to clarify:}
    
    \textit{``[The Baum-Welch Algorithm] predates the more general expectation-maximization (EM) algorithm, but the two are equivalent when applied to HMMs." (lines 121-122)}
    
\end{itemize}

2. Page 10: What does the acronym “SAGA” stand for? The authors start using the two acronyms, SAG and SVRG, after specifying the original names, but this is not the case for SAGA.

\begin{itemize}
    \item \textit{We found that it stands for ``Stochastic Average Gradient Accelerated", and have updated the manuscript accordingly (line 162).}
\end{itemize}

3. Page 10: The authors mention each algorithm by its own algorithm number, for example, ``Algorithm (3)”, in the main text, but it is originally named by ``Algorithm 3” in the corresponding pseudocode.

\begin{itemize}
    \item \textit{Thank you for catching this. We have corrected this for both algorithms and figures throughout the text.}
\end{itemize}

4. Page 22: As a measure for the convergence speed, the authors adopt the number of iterations until the convergence (among those algorithms run within 12 hours). Summarizing overall CPU times of all of the methods being compared in a table would also be helpful to get a sense of how fast the proposed algorithms are. It may also provide new intuition or insight that is not mentioned in the text.

\begin{itemize}
    \item \textit{We have added the computation time in all of the main text figures associated with the case study (Figs. 6--7) and one of the main simulation study figure (Fig. 3). The results are similar. Due to concerns regarding manuscript length, we placed other simulation study figures that included computation time in Supplement A. Here, again, the results are similar.}
\end{itemize}

5. Page 24: The authors say, “However, Algorithm (5) tended to converge to areas of higher likelihood than the baseline for almost all experiments.” I am wondering whether the authors can provide another quality measure for this argument.

\begin{itemize}
    \item \textit{We agree that the sentence pointed out is not precise enough as written. We have clarified the argument in the main text (lines 458-465).}
\end{itemize}

When there are multiple modes, local optimizers may end up with different local maxima when the initial values are spread over the parameter space. Did the authors implement a single run (a single set of initial values) for each algorithm given each data set (among five data sets) in each experiment (among eight experiments), and argue that the proposed methods end up with higher likelihood areas? Then I may not buy this argument because it is possible that the proposed methods might have started with favorable initial values (those close to high likelihood regions) ‘by chance’. Or is the argument based on some average performance among multiple runs with initial values spread throughout the parameter space for each algorithm given the same data set in each experiment?

\begin{itemize}
    \item \textit{For each experiment, we first initialized 5 data sets, and then we used 5 random seeds (0,1,2,3,4) in Python to initialize 5 random parameter initializations. Then, we ran each algorithm on all 25 parameter initialization / data set combinations. For the case study, we used 50 random seeds to initialize 50 random parameter initializations, and then ran each algorithm on every initialization.}
    
    \textit{We reported the single random initialization from each algorithm that resulted in the highest likelihood after 12 hours in Figure 2 and Figure 6 (the trace plots of the log-likelihood). However, we reported aggregated results of all initializations in Figures 3, 4, and 7 (box plots and scatter plots). Figure 2 and Figure 6 aim to show how the algorithms perform when they are re-run with many initializations, while Figures 3, 4, and 7 aim to show how they perform on average across many initializations. We have added to the captions of each figure of the main text to clarify this point.}
\end{itemize} 

\end{document}
