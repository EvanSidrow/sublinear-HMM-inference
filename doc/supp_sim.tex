\documentclass[12pt]{article}
\pdfoutput=1

\input{header}
\input{defs}

\pdfminorversion=4
% NOTE: To produce blinded version, replace "0" with "1" below.
\newcommand{\blind}{0}
\linenumbers

% DON'T change margins - should be 1 inch all around.
\addtolength{\oddsidemargin}{-.5in}%
\addtolength{\evensidemargin}{-.5in}%
\addtolength{\textwidth}{1in}%
\addtolength{\textheight}{1.3in}%
\addtolength{\topmargin}{-.8in}%

\begin{document}

%\bibliographystyle{natbib}

\def\spacingset#1{\renewcommand{\baselinestretch}%
{#1}\small\normalsize} \spacingset{1}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\if0\blind
{
    \title{Variance-reduced stochastic optimization for efficient inference of hidden Markov models: Supplement A: Simulation Study}
    
    \author{
      \textbf{Evan Sidrow} \\
      Department of Statistics \\
      University of British Columbia\\
      Vancouver, Canada \\
      %\texttt{evan.sidrow@stat.ubc.ca} \\
      %
      \and
      %
      \textbf{Nancy Heckman} \\
      Department of Statistics \\
      University of British Columbia \\
      Vancouver, Canada \\
      %
      \and
      %
      \textbf{Alexandre Bouchard-C\^ot\'e?} \\
      Department of Statistics \\
      University of British Columbia \\
      Vancouver, Canada \\
      \and
      %
      \textbf{Sarah M. E. Fortune} \\
      Department of Oceanography \\
      Dalhousie University \\
      Halifax, Canada \\
      %
      \and
      %
      \textbf{Andrew W. Trites} \\
      Department of Zoology \\
      Institute for the Oceans and Fisheries \\
      University of British Columbia \\
      Vancouver, Canada \\
      %
      \and
      %
      \textbf{Marie Auger-M\'eth\'e} \\
      Department of Statistics \\
      Institute for the Oceans and Fisheries \\
      University of British Columbia \\
      Vancouver, Canada \\
    }
    \maketitle
} \fi

\if1\blind
{
  \bigskip
  \bigskip
  \bigskip
  \begin{center}
    {\LARGE\bf Variance-reduced stochastic optimization for efficient inference of hidden Markov models: Supplement A: Simulation Study}
  \end{center}
  \medskip
} \fi

%The figures in this supplement display the log-likelihood (divided by $T$) of the maximum log-likelihood minus the log-likelihood (divided by $T$) at each epoch of a selected run of each optimization algorithm after 12 hours or 150 epochs (whichever came first) for four data sets of all experiments. For each optimization algorithm and data set, we display the random initialization that resulted in the highest likelihood after 12 hours. The dots on each figure correspond to the likelihood at convergence. Convergence is defined as the point at which the gradient norm of the log-likelihood (divided by $T$) was less than $10^{-2}$. We selected a tolerance of $10^{-2}$ because it was the lowest tolerance that all algorithms regularly converged to within 12 hours. 

\newpage

\section{Simulation study results, $T = 10^{3}$}

Below are results for the experiments with $T=10^{3}$. They are analogous to Figure (3) and Figure (4) in the main text.

\begin{figure}[H]
    \centering
    \includegraphics[width=6.5in]{../plt/boxplots_sim_T_1000.png}
    \caption{Box plots showing epochs to converge (top) and the log-likelihood of the maximum log-likelihood minus the log-likelihood (all divided by $T$) at convergence (bottom) for each optimization algorithm. FE corresponds to $\{P = \texttt{False}, M = T\}$, PE1 corresponds to $\{P = \texttt{True}, M = T\}$, and PE2 corresponds to $\{P = \texttt{False}, M = 10T\}$. Blue corresponds to SAGA and orange corresponds to SVRG. Results are shown for all simulation studies with $T=10^{3}$, $N=3$ and $d=3$ (left), $N=3$ and $d=6$ (left-middle), $N=6$ and $d=3$ (right-middle), and $N=6$ and $d=6$ (right). One epoch represents either one full E step, $T$ iterations with the M step, or one gradient step for full-gradient algorithms. The y-axis of the bottom row is on a log-scale.}
    \label{fig:boxplots_sim}
\end{figure}
%
\begin{figure}[H]
    \centering
    \includegraphics[width=6.5in]{../plt/scatter_sim_T_1000.png}
    \caption{Log-likelihood of the maximum log-likelihood minus the log-likelihood (all divided by $T$) at convergence versus epochs to converge for all simulation studies with $T=10^{3}$, $N=3$ and $d=3$ (top left), $N=3$ and $d=6$ (top right), $N=6$ and $d=3$ (bottom left), and $N=6$ and $d=6$ (bottom left). One epoch represents either one full E step, $T$ iterations with the M step, or one gradient step for full-gradient algorithms. The y-axis is on a log-scale.}
    \label{fig:scatter_sim}
\end{figure}

%%%

\newpage 

The figures below show the optimality gap vs epoch for each data set of every experiment with $T=10^{3}$ for all nine tested algorithms. They are similar to Figure (2) from the main text. $\texttt{EM-VRSO}$ tended to converge more quickly than BFGS for experiments with $N=3$. BFGS often out-performed $\texttt{EM-VRSO}$ in the long-run (after approximately 100 epochs), but BFGS is a second-order method and this behaviour is expected in the long run on small data sets. Our method is particularly well-suited to fitting HMMs on large and complex data sets. 

\begin{figure}[H]
    \centering
    \includegraphics[width=6.5in]{../plt/log-like_v_epoch_T-1000-000.png}
    \caption{Optimally gap between the current log-likelihood and optimal log-likelihood for the first simulated data sets associated with experiments $T=10^{3}$, $N \in \{3,6\}$ and $d \in \{3,6\}$. One epoch represents either one full E step, $T$ iterations with the M step, or one gradient step for full-gradient algorithms. The y-axis is on a log-scale.}
\end{figure}
%
\begin{figure}[H]
    \centering
    \includegraphics[width=6.5in]{../plt/log-like_v_epoch_T-1000-001.png}
    \caption{Optimally gap between the current log-likelihood and optimal log-likelihood for the second simulated data sets associated with experiments $T=10^{3}$, $N \in \{3,6\}$ and $d \in \{3,6\}$. One epoch represents either one full E step, $T$ iterations with the M step, or one gradient step for full-gradient algorithms. The y-axis is on a log-scale.}
\end{figure}
%
\begin{figure}[H]
    \centering
    \includegraphics[width=6.5in]{../plt/log-like_v_epoch_T-1000-002.png}
    \caption{Optimally gap between the current log-likelihood and optimal log-likelihood for the third simulated data sets associated with experiments $T=10^{3}$, $N \in \{3,6\}$ and $d \in \{3,6\}$. One epoch represents either one full E step, $T$ iterations with the M step, or one gradient step for full-gradient algorithms. The y-axis is on a log-scale.}
\end{figure}
%
\begin{figure}[H]
    \centering
    \includegraphics[width=6.5in]{../plt/log-like_v_epoch_T-1000-003.png}
    \caption{Optimally gap between the current log-likelihood and optimal log-likelihood for the fourth simulated data sets associated with experiments $T=10^{3}$, $N \in \{3,6\}$ and $d \in \{3,6\}$. One epoch represents either one full E step, $T$ iterations with the M step, or one gradient step for full-gradient algorithms. The y-axis is on a log-scale.}
\end{figure}
%
\begin{figure}[H]
    \centering
    \includegraphics[width=6.5in]{../plt/log-like_v_epoch_T-1000-004.png}
    \caption{Optimally gap between the current log-likelihood and optimal log-likelihood for the fifth simulated data sets associated with experiments $T=10^{3}$, $N \in \{3,6\}$ and $d \in \{3,6\}$. One epoch represents either one full E step, $T$ iterations with the M step, or one gradient step for full-gradient algorithms. The y-axis is on a log-scale.}
\end{figure}

\newpage

\section{Simulation study results, $T = 10^{5}$}

The figures below are similar to Figure (2) from the simulation study in the main text. They show the optimality gap vs epoch for each experiment and each of the nine tested algorithms. However, we include all simulated data sets here.

\begin{figure}[H]
    \centering
    \includegraphics[width=6.5in]{../plt/log-like_v_epoch_T-100000-000.png}
    \caption{Optimally gap between the current log-likelihood and optimal log-likelihood for the first simulated data sets associated with experiments $T=10^{5}$, $N \in \{3,6\}$ and $d \in \{3,6\}$. One epoch represents either one full E step, $T$ iterations with the M step, or one gradient step for full-gradient algorithms. The y-axis is on a log-scale.}
\end{figure}
%
\begin{figure}[H]
    \centering
    \includegraphics[width=6.5in]{../plt/log-like_v_epoch_T-100000-001.png}
    \caption{Optimally gap between the current log-likelihood and optimal log-likelihood for the second simulated data sets associated with experiments $T=10^{5}$, $N \in \{3,6\}$ and $d \in \{3,6\}$. One epoch represents either one full E step, $T$ iterations with the M step, or one gradient step for full-gradient algorithms. The y-axis is on a log-scale.}
\end{figure}
%
\begin{figure}[H]
    \centering
    \includegraphics[width=6.5in]{../plt/log-like_v_epoch_T-100000-002.png}
    \caption{Optimally gap between the current log-likelihood and optimal log-likelihood for the third simulated data sets associated with experiments $T=10^{5}$, $N \in \{3,6\}$ and $d \in \{3,6\}$. One epoch represents either one full E step, $T$ iterations with the M step, or one gradient step for full-gradient algorithms. The y-axis is on a log-scale.}
\end{figure}
%
\begin{figure}[H]
    \centering
    \includegraphics[width=6.5in]{../plt/log-like_v_epoch_T-100000-003.png}
    \caption{Optimally gap between the current log-likelihood and optimal log-likelihood for the fourth simulated data sets associated with experiments $T=10^{5}$, $N \in \{3,6\}$ and $d \in \{3,6\}$. One epoch represents either one full E step, $T$ iterations with the M step, or one gradient step for full-gradient algorithms. The y-axis is on a log-scale.}
\end{figure}
%
\begin{figure}[H]
    \centering
    \includegraphics[width=6.5in]{../plt/log-like_v_epoch_T-100000-004.png}
    \caption{Optimally gap between the current log-likelihood and optimal log-likelihood for the fifth simulated data sets associated with experiments $T=10^{5}$, $N \in \{3,6\}$ and $d \in \{3,6\}$. One epoch represents either one full E step, $T$ iterations with the M step, or one gradient step for full-gradient algorithms. The y-axis is on a log-scale.}
\end{figure}

\end{document}