{\rtf1\ansi\ansicpg1252\cocoartf2580
\cocoatextscaling0\cocoaplatform0{\fonttbl\f0\fswiss\fcharset0 Helvetica;}
{\colortbl;\red255\green255\blue255;}
{\*\expandedcolortbl;;}
\margl1440\margr1440\vieww11520\viewh8400\viewkind0
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f0\fs24 \cf0 % \\section\{Induced Independence in the PHMM\}\
\
\\subsection\{Hidden Markov Models\}\
\
% \\section\{Memoization for HMMs\}\
\
Suppose an observation sequence $\\bfy = \\\{y_t\\\}_\{t=1\}^T$ is generated from a standard HMM with corresponding latent state $\\bfx = \\\{x_t\\\}_\{t=1\}^T$. Following \\citet\{Barajas:2017\}, we reparameterize the $N \\times N$ transition probability matrix $\\Gamma$ such that the entries of the matrix are forced to be non-negative and the rows sum to one:\
\
\\begin\{equation*\}\
    \\Gamma_\{ij\} = \\frac\{\\exp(\\eta_\{ij\})\}\{\\sum_\{k=1\}^N \\exp(\\eta_\{ik\})\}, \
\\end\{equation*\}\
%\
where $i,j = 1,\\ldots,N$ and $\\eta_\{ii\}$ is set to zero for identifiability. This formulation simplifies likelihood maximization by removing constraints in the optimization problem. \
\
The joint likelihood of the observed data $\\bfy$ and the latent states $\\bfx$ is\
%\
\\begin\{equation\}\
    p(\\bfx,\\bfy;\\theta,\\eta) = \\delta_\{x_1\} f^\{(x_1)\}(y_1; \\theta^\{(x_1)\}) \\prod_\{t=2\}^T \\Gamma_\{x_\{t-1\} x_t\}(\\eta) f^\{(x_t)\}(y_t; \\theta^\{(x_t)\}).\
    \\label\{eqn:like\}\
\\end\{equation\}\
%\
Alternatively, the marginal likelihood of the observed data $\\bfy$ alone is \
%\
\\begin\{equation\}\
    p(\\bfy;\\theta,\\eta) = \\delta P(y_1;\\theta) \\prod_\{t=2\}^T \\Gamma(\\eta) P(y_t;\\theta) \\mathbf\{1\}_N.\
    \\label\{eqn:like_marginal\}\
\\end\{equation\}\
\
\\subsection\{The Baum-Welch Algorithm\}\
\
The Baum-Welch algorithm is used to estimate the parameters $\\\{\\theta,\\eta\\\}$ and is a specific instance of the EM algorithm adapted to HMMs. \
\
\\subsection\{Direct likelihood maximization\}\
\
An alternative way to perform inference over HMMs is to directly maximize the marginalize the likelihood from Equation (\\ref\{eqn:like_marginal\}) using gradient ascent. There are several ways to calculate the gradient of Equation (\\ref\{eqn:like_marginal\}), but we use the Fisher identity of the gradient for incomplete data models:\
%\
\\begin\{equation\}\
    \\nabla_\{\\theta,\\eta\} \\log p(\\bfy;\\theta,\\eta) = \\bbE_\{p(\\bfX \\mid \\bfy;\\theta,\\eta)\}\\left[ \\nabla_\{\\theta,\\eta\} \\log p(\\bfy,\\bfX;\\theta,\\eta) \\right],\
    \\label\{eqn:fisher_id\}\
\\end\{equation\}\
%\
In what follows, assume that the initial distribution of $X_1$, $\\delta$, is fixed. Then, using Equation (\\ref\{eqn:fisher_id\}) the gradient of the marginal likelihood of an HMM with respect to $\\theta$ and $\\eta$ can be written as:\
%\
\\begin\{align*\}\
    \\nabla_\{\\theta,\\eta\} \\log p(\\bfy;\\theta,\\eta) &= \\bbE_\{p(\\bfX \\mid \\bfy;\\theta,\\eta)\}\\left[ \\nabla_\{\\theta,\\eta\} \\log p(\\bfy,\\bfX;\\theta,\\eta) \\right] \\\\\
    %\
    &= \\bbE_\{p(\\bfX \\mid \\bfy;\\theta,\\eta)\} \\left[\\nabla_\{\\eta,\\theta\} \\log \\delta_\{X_1\} + \\sum_\{t = 1\}^T \\nabla_\{\\theta,\\eta\} \\log f^\{(X_t)\}(y_t;\\theta) + \\sum_\{t=1\}^\{T-1\} \\nabla_\{\\theta,\\eta\} \\log \\Gamma_\{X_t,X_\{t+1\}\} (\\eta) \\right] \\\\\
    %\
    &= \\sum_\{t = 1\}^T \\bbE_\{p(\\bfX \\mid \\bfy;\\theta,\\eta)\} \\left[ \\nabla_\{\\theta,\\eta\} \\log f^\{(X_t)\}(y_t;\\theta)\\right] + \\sum_\{t=1\}^\{T-1\} \\bbE_\{p(\\bfX \\mid \\bfy;\\theta,\\eta)\} \\left[ \\nabla_\{\\theta,\\eta\} \\log \\Gamma_\{X_t,X_\{t+1\}\} (\\eta) \\right] \\\\\
    %\
    &= \\sum_\{t = 1\}^T \\sum_\{i=1\}^N p(X_t = i | \\bfy ; \\theta, \\eta) \\nabla_\{\\theta,\\eta\} \\log f^\{(i)\}(y_t;\\theta) \\\\\
    & \\qquad + \\sum_\{t=1\}^\{T-1\} \\sum_\{i=1\}^N \\sum_\{j=1\}^N p(X_t = i, X_\{t+1\} = j | \\bfy ; \\theta,\\eta) \\nabla_\{\\theta,\\eta\} \\log \\Gamma_\{i,j\} (\\eta).\
\\end\{align*\}\
We can split the gradient of the log-likelihood into two separate terms such that the gradient with respect to $\\theta$ and the gradient with respect to $\\eta$ each have convenient representations:\
%\
\\begin\{gather\}\
    \\nabla_\{\\theta\} \\log p(\\bfy;\\theta,\\eta) = \\sum_\{t=1\}^T \\sum_\{i=1\}^N \\gamma_i(t ~;~ \\theta,\\eta) \\nabla_\{\\theta\} \\log f^\{(i)\}(y_t; \\theta) \\label\{eqn:theta_update_gd\} \\\\\
    %\
    \\nabla_\{\\eta\} \\log p(\\bfy;\\theta,\\eta) = \\sum_\{t=1\}^\{T-1\} \\sum_\{i=1\}^N \\sum_\{j=1\}^N \\xi_\{i,j\}(t ~;~ \\theta,\\eta) \\nabla_\{\\eta\} \\log \\Gamma_\{i,j\}(\\eta), \\label\{eqn:eta_update_gd\} \\\\\
    %\
    \\gamma_i(t ~;~ \\theta,\\eta) = p(X_t = i \\mid \\bfy ~;~ \\theta,\\eta), \\qquad \\xi_\{i,j\}(t ~;~ \\theta,\\eta) = p(X_t = i, X_\{t+1\} = j \\mid \\bfy ~;~ \\theta,\\eta). \\nonumber\
\\end\{gather\}\
\
There is a clear connection between the Baum-Welch updates of Equations (\\ref\{eqn:theta_update_bw\}) and (\\ref\{eqn:Gamma_update_bw\}) and the Gradient Descent updates of Equations (\\ref\{eqn:theta_update_gd\}) and (\\ref\{eqn:eta_update_gd\}). In particular, one recovers gradient descent by performing one gradient step within the M-step of the Baum-Welch algorithm rather than solving the entire maximization problem.\
\
The connection between the Baum-Welch algorithm and Gradient descent leads to a natural question. If taking one gradient step within the M- step of EM is gradient descent, and solving the M- step entirely results in the Baum-Welch algorithm, then are there other ways to perform the M-step in the EM algorithm with desirable properties? To answer this question, we first review some stochastic optimization techniques.\
\
\\subsection\{Stochastic Optimization\}\
\
At iteration $k$, the M-step of the Baum-Welch algorithm for simple HMMs is equivalent to the following optimization problem:\
\
\\begin\{gather*\}\
    \\theta^\{(k)\} =  \\argmin_\{\\theta\} \\sum_\{t=1\}^T \\sum_\{i=1\}^N \\gamma_i (t ; \\theta^\{(k-1)\}, \\eta^\{(k-1)\}) \\log f_i(y_t;\\theta) \\\\\
    %\
    \\eta^\{(k)\} =  \\argmin_\{\\eta\} \\sum_\{t=1\}^T \\sum_\{i=1\}^N \\sum_\{j=1\}^N \\xi_\{i,j\} (t ; \\theta^\{(k-1)\}, \\eta^\{(k-1)\}) \\log \\Gamma_\{i,j\}(\\eta)\
\\end\{gather*\}\
\
optimization problems that can be written as a sum are natural candidates for stochastic optimization techniques such as SAG \\citep\{\}, SVRG \\citep\{\}, and SAGA \\citep\{\}.\
\
Interestingly, many drawbacks of well-known stochastic optimization provide no additional problems when used within an EM algorithm. For example, SVRG occasionally requries an entire pass through the data gradient to be calculated\
\
It is natural to apply stochastic optimization to HMM inference using the Fisher identity. In particular, \\citet\{Cappe:2005\} describe the following Robbins-Monro type algorithm:\
\
... \
\
The algorithm above applies even if the state-space of $\\bfx$ is not discrete as long as it is possible to sample from $p(\\bfx | \\bfy ; \\theta, \\Gamma)$. \\citet\{Gu:1998\} extend the algorithm above to apply even if it is not possible to sample from $p(\\bfx | \\bfy ; \\theta, \\Gamma)$ by drawing $\\bfx$ from a Markov Chain with $p(\\bfx | \\bfy ; \\theta, \\Gamma)$ as its stationary distribution. \\citet\{Gu:1998\} also extend this algorithm to general incomplete data models and prove that such an algorithm converges almost surely (under certain regularity conditions).\
\
%\\subsection\{Expanded view of EM\}}