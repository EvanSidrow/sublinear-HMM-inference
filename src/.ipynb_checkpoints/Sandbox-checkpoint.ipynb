{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "national-juice",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "from scipy.stats import gamma\n",
    "from scipy.stats import norm\n",
    "from scipy.stats import vonmises\n",
    "from scipy.stats import multivariate_normal\n",
    "from scipy.stats import gaussian_kde\n",
    "from scipy.stats import circstd\n",
    "from scipy.special import iv\n",
    "from scipy.special import expit\n",
    "from scipy.special import logit\n",
    "from scipy.special import logsumexp\n",
    "from scipy.optimize import minimize\n",
    "from scipy.optimize import minimize_scalar\n",
    "from scipy.optimize import LinearConstraint\n",
    "from scipy.signal import convolve\n",
    "from scipy.interpolate import interp1d\n",
    "\n",
    "from copy import deepcopy\n",
    "\n",
    "import time\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "acute-library",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0,'/Users/evsi8432/Documents/Research/CarHHMM-DFT/Repository/Code')\n",
    "import Preprocessor\n",
    "import Parameters\n",
    "import HHMM\n",
    "import Visualisor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "egyptian-address",
   "metadata": {},
   "outputs": [],
   "source": [
    "pars = Parameters.Parameters()\n",
    "pars.features = [{'Y':{'corr':False,'f':'normal'}}, # coarse-scale\n",
    "                 {}]\n",
    "pars.K = [3,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "adult-white",
   "metadata": {},
   "outputs": [],
   "source": [
    "T = 10\n",
    "\n",
    "data = [{'Y':np.random.normal(),'subdive_features':[]} for _ in range(T)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "elegant-source",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eta_2_Gamma(eta):\n",
    "    \n",
    "    # get Coarse-scale Gamma\n",
    "    Gamma_coarse = np.exp(eta[0])\n",
    "    Gamma_coarse = (Gamma_coarse.T/np.sum(Gamma_coarse,1)).T\n",
    "    \n",
    "    # get fine-scale Gammas\n",
    "    Gammas_fine = []\n",
    "    for eta_fine in eta[1]:\n",
    "        Gamma_fine = np.exp(eta_fine)\n",
    "        Gammas_fine.append((Gamma_fine.T/np.sum(Gamma_fine,1)).T)\n",
    "        \n",
    "    return [Gamma_coarse,Gammas_fine]\n",
    "\n",
    "def logdotexp(A, ptm):\n",
    "    max_A = np.max(A)\n",
    "    C = np.dot(np.exp(A - max_A), ptm)\n",
    "    np.log(C, out=C)\n",
    "    C += max_A\n",
    "    return C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "placed-surname",
   "metadata": {},
   "outputs": [],
   "source": [
    "class optimizor:\n",
    "    \n",
    "    def __init__(self,hhmm):\n",
    "        \n",
    "        '''\n",
    "        constructor for optimizor class\n",
    "        '''\n",
    "        \n",
    "        self.hhmm = hhmm\n",
    "        self.data = hhmm.data\n",
    "        \n",
    "        self.K = hhmm.pars.K\n",
    "        self.T = len(hhmm.data)\n",
    "        \n",
    "        # thetas and etas \n",
    "        self.theta = deepcopy(hhmm.theta)\n",
    "        self.eta = deepcopy(hhmm.eta)\n",
    "        self.Gamma = eta_2_Gamma(self.eta)\n",
    "        \n",
    "        self.delta = np.ones(self.K[0])/self.K[0]\n",
    "        for _ in range(100):\n",
    "            self.delta = np.dot(self.delta,self.Gamma[0])\n",
    "        \n",
    "        # log-likelihood\n",
    "        self.log_like = 0\n",
    "        \n",
    "        # alpha and beta\n",
    "        self.log_alphas = np.zeros((self.K[0],self.T))\n",
    "        self.log_betas = np.zeros((self.K[0],self.T))\n",
    "        \n",
    "        # gradients wrt theta\n",
    "        self.d_log_alpha_d_theta = [[deepcopy(hhmm.theta) for _ in range(self.K[0])] for _ in range(self.T)]\n",
    "        self.d_log_beta_d_theta =  [[deepcopy(hhmm.theta) for _ in range(self.K[0])] for _ in range(self.T)]\n",
    "        self.d_log_like_d_theta = [deepcopy(hhmm.theta) for _ in range(self.T)]\n",
    "        \n",
    "        # gradients wrt eta\n",
    "        self.d_log_alpha_d_eta = [[deepcopy(hhmm.eta) for _ in range(self.K[0])] for _ in range(self.T)]\n",
    "        self.d_log_beta_d_eta =  [[deepcopy(hhmm.eta) for _ in range(self.K[0])] for _ in range(self.T)]\n",
    "        self.d_log_like_d_eta = [deepcopy(hhmm.eta) for _ in range(self.T)]\n",
    "        \n",
    "        # initialize gradients\n",
    "        self.initialize_grads()\n",
    "        \n",
    "        return\n",
    "    \n",
    "    def initialize_grads(self):\n",
    "        \n",
    "        for t in range(self.T):\n",
    "            \n",
    "            # get log likelihood gradients\n",
    "            self.d_log_like_d_eta[t] = np.zeros((self.K[0],self.K[0]))\n",
    "            \n",
    "            for k in range(self.K[0]):\n",
    "                \n",
    "                self.d_log_alpha_d_eta[t][k] = np.zeros((self.K[0],self.K[0]))\n",
    "                self.d_log_beta_d_eta[t][k] = np.zeros((self.K[0],self.K[0]))\n",
    "                \n",
    "                for feature,dist in hhmm.pars.features[0].items():\n",
    "                    if dist['f'] == 'normal' and not dist['corr']:\n",
    "                        \n",
    "                        if k == 0:\n",
    "                            self.d_log_like_d_theta[t][0][feature]['mu'] = np.zeros(self.K[0])\n",
    "                            self.d_log_like_d_theta[t][0][feature]['sig'] = np.zeros(self.K[0])\n",
    "                            self.d_log_like_d_theta[t][0][feature]['corr'] = np.zeros(self.K[0])\n",
    "\n",
    "                        self.d_log_alpha_d_theta[t][k][0][feature]['mu'] = np.zeros(self.K[0])\n",
    "                        self.d_log_alpha_d_theta[t][k][0][feature]['sig'] = np.zeros(self.K[0])\n",
    "                        self.d_log_alpha_d_theta[t][k][0][feature]['corr'] = np.zeros(self.K[0])\n",
    "\n",
    "                        self.d_log_beta_d_theta[t][k][0][feature]['mu'] = np.zeros(self.K[0])\n",
    "                        self.d_log_beta_d_theta[t][k][0][feature]['sig'] = np.zeros(self.K[0])\n",
    "                        self.d_log_beta_d_theta[t][k][0][feature]['corr'] = np.zeros(self.K[0])\n",
    "\n",
    "                    else:\n",
    "                        raise('only independent normal distributions supported at this time')\n",
    "    \n",
    "    def log_f(self,t):\n",
    "        \n",
    "        # define the data point\n",
    "        y = self.data[t]\n",
    "        \n",
    "        # initialize the log-likelihood\n",
    "        log_f = np.zeros(self.K[0])\n",
    "        \n",
    "        # initialize the gradient of the log-likelihood\n",
    "        grad_log_f = [{},{}] # one for coarse scale, one for fine-scale\n",
    "        \n",
    "        # go through each coarse-scale feature and add to log-likelihood\n",
    "        for feature,value in y.items():\n",
    "            \n",
    "            # initialize gradient for this feature\n",
    "            if feature == 'subdive_features':\n",
    "                continue\n",
    "            \n",
    "            if feature not in self.hhmm.pars.features[0]:\n",
    "                print(\"unidentified feature in y: %s\" % feature)\n",
    "                return\n",
    "            \n",
    "            dist = self.hhmm.pars.features[0][feature]['f']\n",
    "            \n",
    "            if dist == 'normal':\n",
    "                \n",
    "                mu = self.theta[0][feature]['mu']\n",
    "                sig = self.theta[0][feature]['sig']\n",
    "                \n",
    "                log_f += norm.logpdf(y[feature],\n",
    "                                     loc=mu,\n",
    "                                     scale=sig)\n",
    "                \n",
    "                grad_log_f[0][feature] = {\"mu\": ((y[feature]-mu)/sig)/sig,\n",
    "                                          \"sig\": (((y[feature]-mu)/sig)**2 - 1)/sig}\n",
    "                \n",
    "            else:\n",
    "                print(\"unidentified emission distribution %s for %s\"%(dist,feature))\n",
    "                return\n",
    "                \n",
    "        # return the result\n",
    "        return log_f,grad_log_f\n",
    "    \n",
    "    def fwd_step(self,t,grad=True):\n",
    "        \n",
    "        # get log_f, grad_log_f\n",
    "        log_f,grad_log_f = self.log_f(t)\n",
    "    \n",
    "        # deal with t = 0\n",
    "        if t == 0:\n",
    "            \n",
    "            # update log_alpha\n",
    "            self.log_alphas[:,t] = np.log(self.delta) + log_f\n",
    "            \n",
    "            # update d_log_alpha_t/d_theta\n",
    "            for k in range(self.K[0]):\n",
    "                for feature in grad_log_f[0]:\n",
    "                    for param in grad_log_f[0][feature]:\n",
    "                        self.d_log_alpha_d_theta[t][k][0][feature][param][k] = grad_log_f[0][feature][param][k]\n",
    "                        \n",
    "            # update d_log_alpha_t/d_eta\n",
    "            self.d_log_alpha_d_eta[t][k] = np.zeros((self.K[0],self.K[0]))\n",
    "            \n",
    "            return\n",
    "        \n",
    "        # now deal with t != 0\n",
    "        self.log_alphas[:,t] = logdotexp(self.log_alphas[:,t-1],self.Gamma[0]) + log_f\n",
    "        \n",
    "        # create matrix of P(X_{t-1} = i | X_{t} = j , Y_{1:t+1})\n",
    "        xi = np.zeros((self.K[0],self.K[0]))\n",
    "        for i in range(self.K[0]):\n",
    "            for j in range(self.K[0]):\n",
    "                xi[i,j] =   self.log_alphas[i,t-1] + np.log(self.Gamma[0][i,j]) + log_f[j]\n",
    "                xi[i,j] += -self.log_alphas[j,t]\n",
    "        \n",
    "        xi = np.exp(xi)\n",
    "        \n",
    "        # update d_log_alpha_t/d_theta\n",
    "        for k in range(self.K[0]): # parameter k\n",
    "            for feature in grad_log_f[0]:\n",
    "                for param in grad_log_f[0][feature]:\n",
    "                    \n",
    "                    # get d_log_fj/d_theta_j\n",
    "                    d_log_fj_d_theta_k = np.zeros(self.K[0])\n",
    "                    d_log_fj_d_theta_k[k] = grad_log_f[0][feature][param][k]\n",
    "                    \n",
    "                    for j in range(self.K[0]): # alpha_j  \n",
    "                        for i in range(self.K[0]):\n",
    "                            self.d_log_alpha_d_theta[t][j][0][feature][param][k] = d_log_fj_d_theta_k[j] + \\\n",
    "                            xi[i,j]*self.d_log_alpha_d_theta[t-1][i][0][feature][param][k]\n",
    "        \n",
    "        # get d_log_Gamma/d_eta\n",
    "        d_log_Gamma_d_eta = np.zeros((self.K[0],self.K[0],self.K[0],self.K[0]))\n",
    "        for i in range(self.K[0]):\n",
    "            for j in range(self.K[0]):\n",
    "                for k in range(self.K[0]):\n",
    "                    if j == k:\n",
    "                        d_log_Gamma_d_eta[i,j,i,k] = 1.0-self.Gamma[0][i,k]\n",
    "                    else:\n",
    "                        d_log_Gamma_d_eta[i,j,i,k] = -self.Gamma[0][i,k]\n",
    "        \n",
    "        \n",
    "        # update d_log_alpha_t/d_eta\n",
    "        for i in range(self.K[0]):\n",
    "            for j in range(self.K[0]):\n",
    "                self.d_log_alpha_d_eta[t][j] += xi[i,j]*self.d_log_alpha_d_eta[t-1][i]\n",
    "                self.d_log_alpha_d_eta[t][j] += xi[i,j]*d_log_Gamma_d_eta[i,j,:,:]\n",
    "        \n",
    "        return\n",
    "    \n",
    "    def bwd_step(self,t,grad=True):\n",
    "        \n",
    "        # deal with t = T-1\n",
    "        if t == self.T-1:\n",
    "            \n",
    "            # update log_beta\n",
    "            self.log_betas[:,t] = np.ones(self.K[0])\n",
    "            \n",
    "            # update d_log_alpha_t/d_theta\n",
    "            for k in range(self.K[0]):\n",
    "                for feature in grad_log_f[0]:\n",
    "                    for param in grad_log_f[0][feature]:\n",
    "                        self.d_log_beta_d_theta[t][k][0][feature][param][k] = 0\n",
    "                        \n",
    "            # update d_log_beta_t/d_eta\n",
    "            self.d_log_beta_d_eta[t][k] = np.zeros((self.K[0],self.K[0]))\n",
    "            \n",
    "            return\n",
    "        \n",
    "        # get log_f, grad_log_f\n",
    "        log_f,grad_log_f = self.log_f(t+1)\n",
    "        \n",
    "        # get log_beta\n",
    "        self.log_betas[:,t] = logdotexp(self.log_betas[:,t+1] + log_f,\n",
    "                                        self.Gamma[0])\n",
    "        \n",
    "        # create matrix of P(X_{t+1} = j | X_{t} = i , Y_{t+1:T})\n",
    "        xi = np.zeros((self.K[0],self.K[0]))\n",
    "        for i in range(self.K[0]):\n",
    "            for j in range(self.K[0]):\n",
    "                xi[i,j] =   np.log(self.Gamma[0][i,j]) + log_f[j] + self.log_betas[j,t+1]\n",
    "                xi[i,j] += -self.log_betas[i,t]\n",
    "        \n",
    "        xi = np.exp(xi)\n",
    "        \n",
    "        # update d_log_beta_t/d_theta\n",
    "        for k in range(self.K[0]): # parameter k\n",
    "            for feature in grad_log_f[0]:\n",
    "                for param in grad_log_f[0][feature]:\n",
    "                    for i in range(self.K[0]): # beta_i\n",
    "                        \n",
    "                        self.d_log_beta_d_theta[t][i][0][feature][param][k] = 0\n",
    "                        \n",
    "                        # get d_log_fj/d_theta_k\n",
    "                        d_log_fj_d_theta_k = np.zeros(self.K[0])\n",
    "                        d_log_fj_d_theta_k[k] =  grad_log_f[0][feature][param][k]\n",
    "                            \n",
    "                        for j in range(self.K[0]):\n",
    "    \n",
    "                            self.d_log_beta_d_theta[t][i][0][feature][param][k] += \\\n",
    "                            xi[i,j]*(self.d_log_beta_d_theta[t+1][j][0][feature][param][k] + \\\n",
    "                                     d_log_fj_d_theta_k[j])\n",
    "        \n",
    "        # get d_log_Gamma_ij/d_eta_kl\n",
    "        d_log_Gamma_d_eta = np.zeros((self.K[0],self.K[0],self.K[0],self.K[0]))\n",
    "        for i in range(self.K[0]):\n",
    "            for j in range(self.K[0]):\n",
    "                for k in range(self.K[0]):\n",
    "                    if j == k:\n",
    "                        d_log_Gamma_d_eta[i,j,i,k] = 1.0-self.Gamma[0][i,k]\n",
    "                    else:\n",
    "                        d_log_Gamma_d_eta[i,j,i,k] = -self.Gamma[0][i,k]\n",
    "        \n",
    "        \n",
    "        # update d_log_beta_t/d_eta\n",
    "        for i in range(self.K[0]):\n",
    "            for j in range(self.K[0]):\n",
    "                self.d_log_beta_d_eta[t][i] += xi[i,j]*self.d_log_beta_d_eta[t+1][j]\n",
    "                self.d_log_beta_d_eta[t][i] += xi[i,j]*d_log_Gamma_d_eta[i,j,:,:]\n",
    "        \n",
    "        return\n",
    "    \n",
    "    def fwd_pass(self,data):\n",
    "        \n",
    "        for t in range(self.T):\n",
    "            self.fwd_step(t,data[t]) \n",
    "            \n",
    "        return logsumexp(self.log_alphas[:,-1])\n",
    "    \n",
    "    def log_likelihood(self,t=None):\n",
    "        \n",
    "        if t is None:\n",
    "            t = self.T - 1\n",
    "        \n",
    "        return logsumexp(self.log_alphas[:,t] + self.log_betas[:,t])\n",
    "    \n",
    "    def grad_log_likelihood(self,t):\n",
    "        \n",
    "        ll = logsumexp(self.log_alphas[:,t] + self.log_betas[:,t])\n",
    "        p_X = np.exp(self.log_alphas[:,t] + self.log_betas[:,t] - ll)\n",
    "        \n",
    "        # get derivative with respect to theta\n",
    "        for feature in self.d_log_alpha_d_theta[t][0][0]:\n",
    "            for param in self.d_log_alpha_d_theta[t][0][0][feature]:\n",
    "                for k in range(self.K[0]):\n",
    "                    self.d_log_like_d_theta[t][0][feature][param][k] = \\\n",
    "                        sum([p_X[i]*(self.d_log_alpha_d_theta[t][i][0][feature][param][k] + \\\n",
    "                                      self.d_log_beta_d_theta[t][i][0][feature][param][k]) \\\n",
    "                             for i in range(self.K[0])])\n",
    "        \n",
    "        \n",
    "        # update derivative with respect to eta\n",
    "        self.d_log_like_d_eta[t] = np.zeros((self.K[0],self.K[0]))\n",
    "        for i in range(self.K[0]):\n",
    "            self.d_log_like_d_eta[t] += p_X[i]*(self.d_log_alpha_d_eta[t][i] + self.d_log_beta_d_eta[t][i])\n",
    "        \n",
    "        return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unavailable-champagne",
   "metadata": {},
   "source": [
    "# Make sure that the likelihoods agree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "handy-label",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-13.867088667442593"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hhmm = HHMM.HHMM(pars,data)\n",
    "hhmm.likelihood(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "nominated-alexander",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-13.86705056279064"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optim = optimizor(hhmm)\n",
    "optim.fwd_pass(data)\n",
    "optim.log_likelihood()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baking-induction",
   "metadata": {},
   "source": [
    "# Test that the gradients wrt $\\theta$ agree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "minor-referral",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'Y': {'mu': array([-0.96066131, -0.22975107,  0.06686848]),\n",
       "   'sig': array([0.54006428, 0.99586178, 0.43098696]),\n",
       "   'corr': array([-1.20142619, -0.29336559, -1.77612392])}},\n",
       " [{}, {}, {}]]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optim.theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "lovely-alarm",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'Y': 1.683910937950901, 'subdive_features': []},\n",
       " {'Y': -0.5096604118427238, 'subdive_features': []},\n",
       " {'Y': -0.23982931462063034, 'subdive_features': []},\n",
       " {'Y': -0.11207951952119154, 'subdive_features': []},\n",
       " {'Y': 0.37055955874041246, 'subdive_features': []},\n",
       " {'Y': -1.3362931227784198, 'subdive_features': []},\n",
       " {'Y': 0.7875375023249983, 'subdive_features': []},\n",
       " {'Y': -0.3694807962248447, 'subdive_features': []},\n",
       " {'Y': 0.35389217118426974, 'subdive_features': []},\n",
       " {'Y': 0.09911334681855377, 'subdive_features': []}]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "yellow-seeking",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.6271086485699658\n",
      "-0.47281592849464743\n",
      "-8.394885700904522\n",
      "[-1.08102628 -1.13495311 -8.54544538]\n",
      "0.525759618689392\n"
     ]
    }
   ],
   "source": [
    "# test gradient of mu_Y for state 0 using my hand-code (this is mu2)\n",
    "optim.fwd_pass(data)\n",
    "#optim.grad_log_likelihood(optim.T-1)\n",
    "\n",
    "t = 5\n",
    "\n",
    "print(optim.d_log_alpha_d_theta[t][0][0][\"Y\"][\"mu\"][2])\n",
    "print(optim.d_log_alpha_d_theta[t][1][0][\"Y\"][\"mu\"][2])\n",
    "print(optim.d_log_alpha_d_theta[t][2][0][\"Y\"][\"mu\"][2])\n",
    "\n",
    "# test finte differences - optim\n",
    "eps = 0.0001\n",
    "\n",
    "optim.fwd_pass(data)\n",
    "y1 = np.copy(optim.log_alphas[:,t])\n",
    "\n",
    "optim.theta[0]['Y']['mu'][2] += eps\n",
    "\n",
    "optim.fwd_pass(data)\n",
    "y2 = optim.log_alphas[:,t]\n",
    "\n",
    "print((y2-y1)/eps)\n",
    "\n",
    "optim.theta[0]['Y']['mu'][2] += -eps\n",
    "\n",
    "# test finite differences - hhmm\n",
    "eps = 0.001\n",
    "y1 = hhmm.likelihood(data)\n",
    "hhmm.theta[0]['Y']['mu'][2] += eps\n",
    "y2 = hhmm.likelihood(data)\n",
    "\n",
    "print((y2-y1)/eps)\n",
    "hhmm.theta[0]['Y']['mu'][2] += -eps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "furnished-absorption",
   "metadata": {},
   "source": [
    "# Test that the gradients wrt $\\eta$ agree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "athletic-assist",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test gradient of mu_Y for state 0 using my hand-code (this is mu2)\n",
    "optim.fwd_pass(data)\n",
    "xi = np.exp(optim.log_alphas[:,-1] - optim.log_likelihood())\n",
    "print(sum([xi[k]*optim.d_log_alpha_d_theta[-1][k][0][\"Y\"][\"mu\"][2] for k in range(optim.K[0])]))\n",
    "\n",
    "# test finte differences - optim\n",
    "eps = 0.0001\n",
    "\n",
    "optim.fwd_pass(data)\n",
    "y1 = optim.log_likelihood()\n",
    "\n",
    "optim.theta[0]['Y']['mu'][2] += eps\n",
    "\n",
    "optim.fwd_pass(data)\n",
    "y2 = optim.log_likelihood()\n",
    "\n",
    "print((y2-y1)/eps)\n",
    "\n",
    "optim.theta[0]['Y']['mu'][2] += -eps\n",
    "\n",
    "# test finite differences - hhmm\n",
    "eps = 0.001\n",
    "y1 = hhmm.likelihood(data)\n",
    "hhmm.theta[0]['Y']['mu'][2] += eps\n",
    "y2 = hhmm.likelihood(data)\n",
    "\n",
    "print((y2-y1)/eps)\n",
    "hhmm.theta[0]['Y']['mu'][2] += -eps"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
